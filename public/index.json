[{"authors":["admin"],"categories":null,"content":"I recently published the revised second edition of my R book \"Analyzing Financial and Economic Data with R\". See this blog post for more details.  A terceira edição do livro \"Análise de Dados Financeiros e Econômicos com o R\" foi recém lançada. Maiores detalhes no site.  I\u0026rsquo;m an associate professor of Finance at Federal University of Rio Grande do Sul (UFRGS), south of Brazil. Details about my past and current work are available in section Publications and Code. My CV is available as pdf and Lattes link.\nIn this blog I will write about my CRAN packages and any other ideas about finance and R or research papers that I’m currently working on. The content, however, will be mostly about R.\nGot a question? Fell free to reach me at marceloperlin@gmail.com.\n","date":1613347200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1613347200,"objectID":"697e7c76ef899ab96d2f51225c50e385","permalink":"http://www.msperlin.com/blog/author/marcelo-s.-perlin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/blog/author/marcelo-s.-perlin/","section":"authors","summary":"I recently published the revised second edition of my R book \"Analyzing Financial and Economic Data with R\". See this blog post for more details.  A terceira edição do livro \"Análise de Dados Financeiros e Econômicos com o R\" foi recém lançada. Maiores detalhes no site.  I\u0026rsquo;m an associate professor of Finance at Federal University of Rio Grande do Sul (UFRGS), south of Brazil. Details about my past and current work are available in section Publications and Code. My CV is available as pdf and Lattes link.\nIn this blog I will write about my CRAN packages and any other ideas about finance and R or research papers that I’m currently working on.","tags":null,"title":"Marcelo S. Perlin","type":"authors"},{"authors":null,"categories":null,"content":"I\u0026rsquo;m the author of many R packages distributed in CRAN (R repository of packages) and Github. Most packages are related to importing and organizing financial and economic datasets from public sources. I fell that unrestricted access to organized datasets is one of the pillars of reproducible research, which is why I\u0026rsquo;ve taken so much interest in it over the years.\nIf you used one of the packages in research, please use the proper citation format with citation('pkgname').\n Data Packages  BatchGetSymbols ( Github and CRAN) Downloads and Organizes Financial Data from Yahoo Finance (daily prices/returns of stocks and market indices). DEPRECATED \u0026ndash; Please use yfR instead. GetBCBData ( Github and CRAN) Imports Datasets from BCB (Central Bank of Brazil) using Its Official API. Its a huge database with many interesting tables. GetDFPData2 ( Github and CRAN) Reading Annual Financial Reports from Bovespa\u0026rsquo;s DFP System. Includes financial reports and many more! GetHFData ( Github and CRAN) Download and Aggregate High Frequency Trading Data from Bovespa. Includes high frequency trading data from the exchange. ARCHIVED (data is no longer available) GetLattesData ( Github and CRAN) Reading Bibliometric Data from Lattes Platform. GetQuandlData ( Github and CRAN) Fast and Cached Import of Data from \u0026lsquo;Quandl\u0026rsquo; Using the \u0026lsquo;json API\u0026rsquo;. GetTDData ( Github and CRAN) Get Data for Brazilian Bonds (Tesouro Direto) simfinR ( Github and CRAN) Import Financial Data from the \u0026lsquo;SimFin\u0026rsquo; Project. DEPRECATED \u0026ndash; see simfinapi for a replacement.   Econometric Packages  fMarkovSwitching ( R-forge) Functions for estimating and simulating an univariate markov regime switching model.   Book packages  afedR ( Github only) Book Companion for Analyzing Financial and Economic Data with R, including functions for downloading datasets, exercises and code. adfeR ( Github only) Book Companion for Análise de Dados Financeiros e Econômicos com o R, including functions for downloading datasets, exercises and code.   R Utilities  PkgsFromFiles (archived in Github only) Finding and Installing Packages Used in R and RMarkdown Files. This package helps the installation of R packages in a fresh new computer or R install by looking at used packages in scripts for a given directory, including subfolders. You can use renv::dependencies for similar effect. RndTexExams (archived in CRAN) Build and Grade Multiple Choice Exams with Randomized Content from Latex files.  ","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1588982400,"objectID":"20d066abc210303bc1a4bb0654f2bd90","permalink":"http://www.msperlin.com/blog/code/r/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/blog/code/r/","section":"code","summary":"R Code","tags":null,"title":"R Code","type":"docs"},{"authors":null,"categories":null,"content":"Be aware that, since 2015, I no longer use Matlab and all code available here is not being actively maintained.   MS_Regress - Markov Regime Switching Models The package and its description are available in Github.\n ACD Models This package includes functions and scripts for the estimation and simulation of ACD (autoregressive conditional duration) models.\nThe zip file includes:\n Scripts and functions for Estimation of an ACD(q,p) model with exponential or weibull distribution Scripts and functions for Simulation of an ACD(q,p) model with exponential or weibull distribution Scripts for cool plots/movies of weibull, burr and generalized gamma distributions. Scripts and functions for visualizing the log likelihood space of the model (the figure showed above)  Required Products: Statistics, Optimization\n Link to download\n Google Trends and Matlab This Matlab code provides a simple function that access google trends for a given string and location, parses the data and return dates, frequency of data and google\u0026rsquo;s search volume to the user.\nFell free to use it or provide suggestions on how to improve it.\nINSTRUCTIONS\n  Verify your chrome.exe file path (google can help you) and copy it over at to chromePath (see script)\n  Manually open one chrome brownser and leave it open during the whole process\n  In chrome, change the download settings so that all downloads go to your default download folder without user dialog\n  Set your default download path in dlFolder (see script)\n  Run the example script\n   Link to download\n Random Portfolios Considering some input arguments, this function performs n simulations with random trades in a price matrix, saving 3 performance indicators (annualized return, annualized standard deviation and annualized sharpe) at each simulation\nFor example, suppose that you\u0026rsquo;re a trader and have earned 15% of annualized logarithm return over 248 trading days (1 year) where you traded, in average, for long positions only, 5 stocks for each day and for 50 days. This function will check if a monkey with no skill whatsoever can, in average, replicate your return after transaction costs (defined by the user). If such mamel can do it, maybe you should review your approach at trading.\nFrom the academic point of view, this is called as the bootstrap method for assessing performance. The present code is a variant of such.\nThe use of random seeds for portfolio performance is not new. The first paper to use it, as I recall is Cumby and Modest (1987). More recently, a more formal approach at the method was given in Burns (2006).\nFor a practical application of the codes published here, please check the papers of Perlin (2007a) and Perlin (2007b).\n Link to download\n Nearest Neighbor Algorithm This is the algorithm involved on the use of the non-linear forecast of a time series based on the nearest neighbour method.\nThe basic idea of the NN algorithm is that the time series copies it\u0026rsquo;s own past behavior, and such fact can be used for forecasting purposes. On the zip file there are two functions: one is the univariate version of NN (nn.m) and the other is the multivariate approach, also called simultaneous NN (snn.m).\nThe routines of the file were build according to the work of Rodriguez, Rivero and Artilles (2001).\n Link to download\n Pairs Trading This function performs the classical pairs trading framework in a given set of prices\nFrom Wikipedia, the free encyclopedia: “The pairs trade was developed in the late 1980s by quantitative analysts and pioneered by Gerald Bamberger while at Morgan Stanley. With the help of others at Morgan Stanley at the time, including Nunzio Tartaglia, Bamberger found that certain securities, often competitors in the same sector, were correlated in their day-to-day price movements. When the correlation broke down, i.e. one stock traded up while the other traded down, they would sell the outperforming stock and buy the underperforming one, betting that the \u0026ldquo;spread\u0026rdquo; between the two would eventually converge.”\nSource: http://en.wikipedia.org/wiki/Pairs_trade#Algorithmic_pairs_trading See also the great book “Demon of Our Own Design” by Richard Bookstaber, which provides an interesting background for the pairs trading strategy.\nThere are many ways you can implement the pairs trading framework. For this package, I used a very simple set of rules. Details can be found within code’s comments.\nPlease note that this package has been developed over the years and it will no longer exactly replicate the results from my 2007 paper.\nQualities of the package:\n Handles any number of assets Outputs separately the plot for the total cumulative profit from the long, short and combined positions. Outputs all trades, including traded prices and time of trades. Provides the user a large amount of input choices for the pairs trading algorithm, including:  amount of money to put in each position (long and short) value of transaction cost (in money unit) size of moving window for finding the pairs over the price data periodicity of pairs updates maximum number of days to keep any trade (without convergence). value of threshold variable     Link to download\n State Space Models This toolbox was designed to simulate and fit linear state space models.\nThe main literature I used for this particular package is Kim and Nelson (1999).\nA state space model (without non stochastic coefficients) is given by:\ny(t)=beta(t)x(t)\u0026rsquo;+e(t) beta(t)=u+Fbeta(t-1)+v(t)\ne(t)~N(0,R) v(t)~N(0,Q)\nWhere x(t) is a vector of size (1,k), beta(t), u and v(t) are vectors with size [1,k] and F, Q are matrices of size [k,k]. More details about the model can be found at the reference.\nThe main advantage of this package is that the fitting function lets you build your own state space model by changing the input option optionsSpec. Please check the pdf document and the example scripts at the zip file for instructions of how to use it.\nSome comments about the fitting code:\n•\tSo far it doesn’t handle autoregressive processes between series of the beta matrix. In other works, at the F matrix, the only estimated coefficients are in the diagonal (all non diagonal elements are zero).\n•\tThe model is estimated by Gaussian maximum likelihood with the function fminsearch. I also played around with fminunc(), but there was no improvement over fminsearch when it comes to robustness and speed.\n•\tSo far the code doesn’t handle state space models with mixture of non stochastic and stochastic coefficients, that is, when you want some variables to have stochastic coefficients and others not in the same model.\n Link to download\n Yahoo Finance and Matlab This is a simple algorithm that downloads trading data from yahoo database. It is basically a large scale application of sqq.m which was originally submitted by Michael Boldin.\nSome of the functionalities of the package:\n User defined ticker list. Choice for benchmark ticker in dates comparison Function for downloading most recent SP500 composition in ticker list. Control for bad data (e.g. a certain percentage of prices missing). Choice of frequency of data (e.g. weekly prices). Choice of starting and ending data. Function for saving the whole data in a pre-formatted excel file together with a full reports on missing data.   Link to download\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1588982400,"objectID":"f8aa3577c3e14ab88908f4f038fe2a10","permalink":"http://www.msperlin.com/blog/code/matlab/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/blog/code/matlab/","section":"code","summary":"Matlab code tutorials.","tags":null,"title":"Matlab Code","type":"docs"},{"authors":null,"categories":null,"content":"Here you can find web applications built with Shiny/R for different academic projects:\n GetDFPData2-shiny - Provides access to financial statements and corporate events from B3, the Brazilian exchange\n PIRF - Shiny app for book Poupando e Investindo em Renda Fixa.\n Panela-Tennis - This is a personal project for a dashboard of tennis matches in my club, Sogipa. All data is collect with a Google Form. Only double matches (four people in court) are included.\n bgs-shiny \u0026ndash; Provides a web interface to R package BatchGetSymbols, allowing the user to download parsed data from Yahoo Finance, along with other analytical plots.\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"a36a1d6a9eac906090e36ded6156d284","permalink":"http://www.msperlin.com/blog/code/shiny/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/blog/code/shiny/","section":"code","summary":"R based web apps","tags":null,"title":"Shiny Apps","type":"docs"},{"authors":null,"categories":["R","data"],"content":" Back in 2020 I started to compile and share financial data in dataverse. The data covers corporate finance events from the DFP and FRE systems. The available tables are the same I use for my research and teaching material, and will be updated once a year.\nToday I updated all datasets. The available data are:\n    R Package Source of Data Description Direct Link Last Update    GetTDData Tesouro Nacional Prices and yields of brazilian sovereign bonds Link 2022-04-06  GetFREData CVM Corporate dataset from FRE systems Link 2022-04-06  GetDFPData2 CVM Annual Financial Reports from DFP system Link 2022-04-06    ","date":1649289600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649289600,"objectID":"aef8d9cb3326a85d1b86b7294eec4b9a","permalink":"http://www.msperlin.com/blog/post/2022-04-07-update-on-data/","publishdate":"2022-04-07T00:00:00Z","relpermalink":"/blog/post/2022-04-07-update-on-data/","section":"post","summary":" Back in 2020 I started to compile and share financial data in dataverse. The data covers corporate finance events from the DFP and FRE systems. The available tables are the same I use for my research and teaching material, and will be updated once a year.\nToday I updated all datasets. The available data are:\n    R Package Source of Data Description Direct Link Last Update    GetTDData Tesouro Nacional Prices and yields of brazilian sovereign bonds Link 2022-04-06  GetFREData CVM Corporate dataset from FRE systems Link 2022-04-06  GetDFPData2 CVM Annual Financial Reports from DFP system Link 2022-04-06    ","tags":["R","data"],"title":"Update of compiled datasets (2022)","type":"post"},{"authors":["Marcelo S. Perlin","Denis Boreinstein","Takeyoshi Imasato"],"categories":null,"content":"","date":1649116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649116800,"objectID":"329b7cd7c9585e0a974d595514b9f6a2","permalink":"http://www.msperlin.com/blog/publication/2022_joi-inbreeding/","publishdate":"2022-04-05T00:00:00Z","relpermalink":"/blog/publication/2022_joi-inbreeding/","section":"publication","summary":"This paper presents an embracing quantitative inbreeding analyses in the Brazilian higher education system (HES). Several studies were conducted about academic inbreeding in several countries with contradictory results on its effect in research productivity, indicating how controversial this issue is. This is the first comprehensive research based on data from more than 79,000 researchers from all fields of knowledge. We find that inbreeding can be found in all fields of science in Brazil. Results from a robust statistical analysis indicate that inbreds are significantly more productive than non-inbreds in all research publications, except in books. Particularly, we find that researchers that have spent a mobility period, either during doctoral studies or a scholarly visiting position, are more productive than other types of inbreds. The overall conclusion is that there is no evidence to support the detrimental view of academic inbreeding based on scientific productivity. We then discuss possible explanations to our findings and present suggestions of future research.","tags":[],"title":"The Academic Inbreeding Controversy: Analysis and Evidence from Brazil","type":"publication"},{"authors":null,"categories":["R","yfR","BatchGetSymbols"],"content":" Package BatchGetSymbols facilitates importation of Yahoo Finance data directly into R and is one of my most popular R packages, with over 100k installations since conception (around 2500 downloads per month). However, I developed BatchGetSymbols back in 2016, with many bad structural choices from my part.\nFor years I wanted to improved the code but always restrained myself because I did not want to mess up the execution of other people’s code that was based on BatchGetSymbols. In order to implement all the breaking changes and move forward with the package, I decided to develop a new (and fresh) package called yfR.\nToday I’m releasing the first version of yfR (not yeat in CRAN). This in a major upgrade on BatchGetSymbols, with many backwards-incompatible changes.\nMotivation yfR is the second and backwards-incompatible version of BatchGetSymbols. In a nutshell, it provides access to daily stock prices from Yahoo Finance, a vast repository with financial data around the globe. Yahoo Finance cover a large number of markets and assets, being used extensively for importing price datasets used in academic research and teaching.\nPackage yfR is based on quantmod and used its main function for fetching data from Yahoo Finance. The main innovation in yfR is in the organization of the imported financial data and using local caching system and parallel computing for speeding up large scale download of datasets from Yahoo Finance.\nSee full documentation here.\nFeatures  Fetchs daily/weekly/monthly/annual stock prices/returns from yahoo finance and outputs a dataframe (tibble) in the long format (stacked data);\n A new feature called “collections” facilitates download of multiple tickers from a particular market/index. You can, for example, download data for all stocks in the SP500 index with a simple call to yf_collection_get();\n A session-persistent smart cache system is available by default. This means that the data is saved locally and only missing portions are downloaded, if needed.\n All dates are compared to a benchmark ticker such as SP500 and, whenever an individual asset does not have a sufficient number of dates, the software drops it from the output. This means you can choose to ignore tickers with high number of missing dates.\n A customized function called yf_convert_to_wide() can transform the long dataframe into a wide format (tickers as columns), much used in portfolio optimization. The output is a list where each element is a different target variable (prices, returns, volumes).\n Parallel computing with package furrr is available, speeding up the data importation process.\n   Differences from BatchGetSymbols Package BatchgetSymbols was developed back in 2016, with many bad structural choices from my part. Since then, I learned more about R and its ecosystem, resulting in better and more maintainable code. However, it is impossible to keep compatibility with the changes I wanted to make, which is why I decided to develop a new (and fresh) package.\nHere are the main differences between yfR (new) and BatchGetSymbols (old):\n All input arguments are now formatted as “snake_case” and not “dot.case”. For example, the argument for the first date of data importation in yfR::yf_get() is first_date, and not first.date as used in BatchGetSymbols::BatchGetSymbols.\n All function have been renamed for a common API notation. For example, BatchGetSymbols::BatchGetSymbols is now yfR::yf_get(). Likewise, the function for fetching collections is yfR::yf_collection_get().\n The output of yfR::yf_get() is always a tibble with the price data (and not a list as in BatchGetSymbols::BatchGetSymbols). If one wants the tibble with a summary of the importing process, it is available as an attribute of the output (see function base::attributes)\n A new feature called “collection”, which allows for easy download of a collection of tickers. For example, you can download price data for all components of the SP500 by simply calling yfR::yf_collection_get(\"SP500\").\n New and prettier status messages using package cli\n  You can find more details at its github repo:\nhttps://github.com/msperlin/yfR\n Installation # CRAN (not yet available) #install.packages(\u0026#39;yfR\u0026#39;) # Github (dev version) devtools::install_github(\u0026#39;msperlin/yfR\u0026#39;)  Examples Fetching a single stock price library(yfR) # set options for algorithm my_ticker \u0026lt;- \u0026#39;FB\u0026#39; first_date \u0026lt;- Sys.Date() - 30 last_date \u0026lt;- Sys.Date() # fetch data df_yf \u0026lt;- yf_get(tickers = my_ticker, first_date = first_date, last_date = last_date) ##  ## ── Running yfR for 1 stocks | 2022-03-01 --\u0026gt; 2022-03-31 (30 days) ── ##  ## ℹ Downloading data for benchmark ticker ^GSPC ## ℹ (1/1) Fetching data for \u0026#39;, ## \u0026#39;FB ## ! - not cached ## ✓ - cache saved successfully ## ✓ - got 22 valid rows (2022-03-01 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- Well done msperlin! ## ℹ Binding price data # output is a tibble with data head(df_yf) ## # A tibble: 6 × 10 ## ticker ref_date price_open price_high price_low price_close volume ## \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 FB 2022-03-01 210. 212. 202. 203. 27094900 ## 2 FB 2022-03-02 205. 209. 202. 208. 29452100 ## 3 FB 2022-03-03 209. 209. 201. 203. 27263500 ## 4 FB 2022-03-04 202. 206. 199. 200. 32130900 ## 5 FB 2022-03-07 201. 201. 187. 187. 38560600 ## 6 FB 2022-03-08 188. 197. 186. 190. 37508100 ## # … with 3 more variables: price_adjusted \u0026lt;dbl\u0026gt;, ret_adjusted_prices \u0026lt;dbl\u0026gt;, ## # ret_closing_prices \u0026lt;dbl\u0026gt;  Fetching many stock prices library(yfR) library(ggplot2) my_ticker \u0026lt;- c(\u0026#39;FB\u0026#39;, \u0026#39;GM\u0026#39;, \u0026#39;MMM\u0026#39;) first_date \u0026lt;- Sys.Date() - 100 last_date \u0026lt;- Sys.Date() df_yf_multiple \u0026lt;- yf_get(tickers = my_ticker, first_date = first_date, last_date = last_date) ##  ## ── Running yfR for 3 stocks | 2021-12-21 --\u0026gt; 2022-03-31 (100 days) ── ##  ## ℹ Downloading data for benchmark ticker ^GSPC ## ℹ (1/3) Fetching data for \u0026#39;, ## \u0026#39;FB ## ✓ - found cache file (2022-03-01 --\u0026gt; 2022-03-30) ## ! - need new data (cache doesnt match query) ## ✓ - got 69 valid rows (2021-12-21 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- All OK! ## ℹ (2/3) Fetching data for \u0026#39;, ## \u0026#39;GM ## ! - not cached ## ✓ - cache saved successfully ## ✓ - got 69 valid rows (2021-12-21 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- Well done msperlin! ## ℹ (3/3) Fetching data for \u0026#39;, ## \u0026#39;MMM ## ! - not cached ## ✓ - cache saved successfully ## ✓ - got 69 valid rows (2021-12-21 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- Youre doing good! ## ℹ Binding price data p \u0026lt;- ggplot(df_yf_multiple, aes(x = ref_date, y = price_adjusted, color = ticker)) + geom_line() print(p)  Fetching collections of prices Collections are just a bundle of tickers pre-organized in the package. For example, collection SP500 represents the current composition of the SP500 index.\nlibrary(yfR) df_yf \u0026lt;- yf_collection_get(\u0026quot;SP500\u0026quot;, first_date = Sys.Date() - 30, last_date = Sys.Date()) head(df_yf)  Fetching daily/weekly/monthly/yearly price data library(yfR) library(ggplot2) library(dplyr) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union my_ticker \u0026lt;- \u0026#39;GE\u0026#39; first_date \u0026lt;- \u0026#39;2010-01-01\u0026#39; last_date \u0026lt;- Sys.Date() df_dailly \u0026lt;- yf_get(tickers = my_ticker, first_date, last_date, freq_data = \u0026#39;daily\u0026#39;) |\u0026gt; mutate(freq = \u0026#39;daily\u0026#39;) ##  ## ── Running yfR for 1 stocks | 2010-01-01 --\u0026gt; 2022-03-31 (4472 days) ── ##  ## ℹ Downloading data for benchmark ticker ^GSPC ## ℹ (1/1) Fetching data for \u0026#39;, ## \u0026#39;GE ## ! - not cached ## ✓ - cache saved successfully ## ✓ - got 3082 valid rows (2010-01-04 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- Time for some tea? ## ℹ Binding price data df_weekly \u0026lt;- yf_get(tickers = my_ticker, first_date, last_date, freq_data = \u0026#39;weekly\u0026#39;) |\u0026gt; mutate(freq = \u0026#39;weekly\u0026#39;) ##  ## ── Running yfR for 1 stocks | 2010-01-01 --\u0026gt; 2022-03-31 (4472 days) ── ##  ## ℹ Downloading data for benchmark ticker ^GSPC ## ℹ (1/1) Fetching data for \u0026#39;, ## \u0026#39;GE ## ✓ - found cache file (2010-01-04 --\u0026gt; 2022-03-30) ## ✓ - got 3082 valid rows (2010-01-04 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- You got it msperlin! ## ℹ Binding price data df_monthly \u0026lt;- yf_get(tickers = my_ticker, first_date, last_date, freq_data = \u0026#39;monthly\u0026#39;) |\u0026gt; mutate(freq = \u0026#39;monthly\u0026#39;) ##  ## ── Running yfR for 1 stocks | 2010-01-01 --\u0026gt; 2022-03-31 (4472 days) ── ##  ## ℹ Downloading data for benchmark ticker ^GSPC ## ℹ (1/1) Fetching data for \u0026#39;, ## \u0026#39;GE ## ✓ - found cache file (2010-01-04 --\u0026gt; 2022-03-30) ## ✓ - got 3082 valid rows (2010-01-04 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- Good stuff! ## ℹ Binding price data df_yearly \u0026lt;- yf_get(tickers = my_ticker, first_date, last_date, freq_data = \u0026#39;yearly\u0026#39;) |\u0026gt; mutate(freq = \u0026#39;yearly\u0026#39;) ##  ## ── Running yfR for 1 stocks | 2010-01-01 --\u0026gt; 2022-03-31 (4472 days) ── ##  ## ℹ Downloading data for benchmark ticker ^GSPC ## ℹ (1/1) Fetching data for \u0026#39;, ## \u0026#39;GE ## ✓ - found cache file (2010-01-04 --\u0026gt; 2022-03-30) ## ✓ - got 3082 valid rows (2010-01-04 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- Good job msperlin! ## ℹ Binding price data df_allfreq \u0026lt;- bind_rows( list(df_dailly, df_weekly, df_monthly, df_yearly) ) |\u0026gt; mutate(freq = factor(freq, levels = c(\u0026#39;daily\u0026#39;, \u0026#39;weekly\u0026#39;, \u0026#39;monthly\u0026#39;, \u0026#39;yearly\u0026#39;))) # make sure the order in plot is right p \u0026lt;- ggplot(df_allfreq, aes(x=ref_date, y = price_adjusted)) + geom_point() + geom_line() + facet_grid(freq ~ ticker) + theme_minimal() + labs(x = \u0026#39;\u0026#39;, y = \u0026#39;Adjusted Prices\u0026#39;) print(p)  Changing format to wide library(yfR) library(ggplot2) my_ticker \u0026lt;- c(\u0026#39;FB\u0026#39;, \u0026#39;GM\u0026#39;, \u0026#39;MMM\u0026#39;) first_date \u0026lt;- Sys.Date() - 100 last_date \u0026lt;- Sys.Date() df_yf_multiple \u0026lt;- yf_get(tickers = my_ticker, first_date = first_date, last_date = last_date) ##  ## ── Running yfR for 3 stocks | 2021-12-21 --\u0026gt; 2022-03-31 (100 days) ── ##  ## ℹ Downloading data for benchmark ticker ^GSPC ## ℹ (1/3) Fetching data for \u0026#39;, ## \u0026#39;FB ## ✓ - found cache file (2021-12-21 --\u0026gt; 2022-03-30) ## ✓ - got 69 valid rows (2021-12-21 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- Good job msperlin! ## ℹ (2/3) Fetching data for \u0026#39;, ## \u0026#39;GM ## ✓ - found cache file (2021-12-21 --\u0026gt; 2022-03-30) ## ✓ - got 69 valid rows (2021-12-21 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- All OK! ## ℹ (3/3) Fetching data for \u0026#39;, ## \u0026#39;MMM ## ✓ - found cache file (2021-12-21 --\u0026gt; 2022-03-30) ## ✓ - got 69 valid rows (2021-12-21 --\u0026gt; 2022-03-30) ## ✓ - got 100% of valid prices -- Well done msperlin! ## ℹ Binding price data l_wide \u0026lt;- yf_convert_to_wide(df_yf_multiple) prices_wide \u0026lt;- l_wide$price_adjusted head(prices_wide) ## # A tibble: 6 × 4 ## ref_date FB GM MMM ## \u0026lt;date\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 2021-12-21 334. 54.8 171. ## 2 2021-12-22 330. 56.1 171. ## 3 2021-12-23 335. 56.9 173. ## 4 2021-12-27 346. 57.4 175. ## 5 2021-12-28 346. 57.1 176. ## 6 2021-12-29 343. 57.2 177.    ","date":1648684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648684800,"objectID":"b2d04b37f698f4d848ade1063c743b54","permalink":"http://www.msperlin.com/blog/post/2022-03-31-yfr/","publishdate":"2022-03-31T00:00:00Z","relpermalink":"/blog/post/2022-03-31-yfr/","section":"post","summary":"Package BatchGetSymbols facilitates importation of Yahoo Finance data directly into R and is one of my most popular R packages, with over 100k installations since conception (around 2500 downloads per month). However, I developed BatchGetSymbols back in 2016, with many bad structural choices from my part.\nFor years I wanted to improved the code but always restrained myself because I did not want to mess up the execution of other people’s code that was based on BatchGetSymbols. In order to implement all the breaking changes and move forward with the package, I decided to develop a new (and fresh) package called yfR.","tags":["R","yfR","BatchGetSymbols"],"title":"New R package yfR","type":"post"},{"authors":null,"categories":["R","BatchGetSymbols","shiny"],"content":"  Hadley Wickham recently released an online version of Mastering Shiny. The book is great! If you haven’t read it, do it fast! On a side note, it is really amazing how much of good and curated content you can get for free in R. When I started programming back in 2007, the first step was buying a brand new – and sometimes expensive – book about the language. There were blogs and other sites, but most content was very basic and not curated, meaning that the posted code most of the time did not work. The new generation probably have no idea of how easy it is to start fresh on new code these days [I fell quite old writing this sentence :)].\nWell, this was a great opportunity for me to brush up my shiny skills. I learned shiny back in 2016 and can report on the development of the technology. I’m really impressed by the current state of shiny today. It is becoming very competitive against other data based dashboard technologies such as those using Python.\nThe result of this learning sprint is bgs-shiny, a shiny interface to package BatchGetSymbols. Within this application you can visualize and download price data from Yahoo Finance. In the background, every data point is live feed, fetched from Yahoo Finance. As usual, all code is available in Github.\n","date":1621987200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1621987200,"objectID":"5a2f192b60bba7c5fb118f25c6ec0a38","permalink":"http://www.msperlin.com/blog/post/2021-05-26-bgs-shiny/","publishdate":"2021-05-26T00:00:00Z","relpermalink":"/blog/post/2021-05-26-bgs-shiny/","section":"post","summary":"Hadley Wickham recently released an online version of Mastering Shiny. The book is great! If you haven’t read it, do it fast! On a side note, it is really amazing how much of good and curated content you can get for free in R. When I started programming back in 2007, the first step was buying a brand new – and sometimes expensive – book about the language. There were blogs and other sites, but most content was very basic and not curated, meaning that the posted code most of the time did not work.","tags":["R","BatchGetSymbols","shiny"],"title":"A shiny interface to BatchGetSymbols","type":"post"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":" ","date":1618617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618617600,"objectID":"eff90dcd8dbe9e463589d57ca4afd881","permalink":"http://www.msperlin.com/blog/talk/2021-04-17-talk-about-getdfpdata/","publishdate":"2021-04-17T00:00:00Z","relpermalink":"/blog/talk/2021-04-17-talk-about-getdfpdata/","section":"talk","summary":"Talk about R packages (in portuguese).","tags":[],"title":"Interview with Prof. Henrique (PUC-RJ)","type":"talk"},{"authors":["Mauro Mastella","Daniel Vancin","Marcelo S. Perlin","Guilherme Kirch"],"categories":null,"content":"","date":1617926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617926400,"objectID":"553504a5cb319d75a9f1e7ca19735665","permalink":"http://www.msperlin.com/blog/publication/2021_gm-boards/","publishdate":"2021-04-09T00:00:00Z","relpermalink":"/blog/publication/2021_gm-boards/","section":"publication","summary":"Purpose: This study aims to intend to check if female board representation affects performance and risk and to analyse the evolution of the demographic aspects of the presence of women on boards in Brazil. Design/methodology/approach: The authors used a sample of 150 Brazilian publicly traded companies from 2010–2018, with different measures of firm performance, firm risk and women’s presence on the board. The study approach is based on a set of ordinary least squares, quantile and panel data regressions. Findings: The presence of women on the board has a positive effect on all of our accounting and market performance measures. However, the result of the impact on risk is not conclusive. The study also found that the number of females on the board has a more significant effect at the lower levels of firm performance measured by return on equity, but at the higher levels when measured by Tobin’s Q. Regarding return on assets, the more significant effect happened on the extremes of the performance distribution. The study findings point that market investors place more value in female presence on the board than in director positions. Originality/value: By estimating the impact of women’s presence on the boards of directors in firm performance and risk, this study aimed to verify this impact in different aspects of the company. In addition, the authors did so in a sample with many years, making it possible to evaluate the historical evolution of the feminine presence in the boards of administration as well as in the groups of directors, assisting Brazilian legislators with new evidence about the possible impacts of Draft Law 7179/2017.","tags":[],"title":"Board gender diversity: performance and risk of Brazilian firms","type":"publication"},{"authors":null,"categories":["R","data"],"content":"  Back in 2020 I started to compile and share financial data in dataverse. The data covers corporate finance events from the DFP and FRE systems. The available tables are the same I use for my research and teaching material, and will be updated once a year.\nToday I updated all datasets. The available data are:\n    R Package Source of Data Description Direct Link Last Update    GetTDData Tesouro Nacional Prices and yields of brazilian sovereign bonds Link 2021-04-09  GetFREData CVM Corporate dataset from FRE systems Link 2021-04-09  BatchGetSymbols Yahoo Finance Daily adjusted and unadjusted prices and trading volumes of stocks Link 2021-04-09  GetDFPData2 CVM Annual Financial Reports from DFP system Link 2021-04-09    ","date":1617926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617926400,"objectID":"3b3702fb717b4d445fce806f35a179e3","permalink":"http://www.msperlin.com/blog/post/2021-04-09-update-on-data/","publishdate":"2021-04-09T00:00:00Z","relpermalink":"/blog/post/2021-04-09-update-on-data/","section":"post","summary":"  Back in 2020 I started to compile and share financial data in dataverse. The data covers corporate finance events from the DFP and FRE systems. The available tables are the same I use for my research and teaching material, and will be updated once a year.\nToday I updated all datasets. The available data are:\n    R Package Source of Data Description Direct Link Last Update    GetTDData Tesouro Nacional Prices and yields of brazilian sovereign bonds Link 2021-04-09  GetFREData CVM Corporate dataset from FRE systems Link 2021-04-09  BatchGetSymbols Yahoo Finance Daily adjusted and unadjusted prices and trading volumes of stocks Link 2021-04-09  GetDFPData2 CVM Annual Financial Reports from DFP system Link 2021-04-09    ","tags":["R","data"],"title":"Update of compiled datasets","type":"post"},{"authors":null,"categories":["R","GetFREData"],"content":"  I’m happy to report that package GetFREData is now available in CRAN. This R package serves as an interface to all corporate datasets available in the FRE system, a vast and official repository of information about many different corporate events. All companies listed at B3 – Brazilian stock exchange – must report to FRE any significant change in their corporate structure. You can find more details about what is available in FRE in its web interface.\nThe R package fetches data from the CVM ftp, downloads and parses the xml files, and output several tables as a list. The corporate data includes (since 2010):\n List of stockholders All capital issues Stock value over years Compensation of boards and directors Composition of boards and committees Family relations within the company List of companies related to family members Stock details Intangible details Auditing details Dividends details  Historical parsed data between 2010 and 2019 is available for download in my personal site.\nInstallation # CRAN (stable) install.packages(\u0026#39;GetFREData\u0026#39;) # github (development) if (!require(devtools)) install.packages(\u0026#39;devtools\u0026#39;) if (!require(GetFREData)) devtools::install_github(\u0026#39;msperlin/GetFREData\u0026#39;)   Example of usage library(GetFREData) library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4 ## ✓ tibble 3.1.0 ✓ dplyr 1.0.5 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() search_company(\u0026#39;grendene\u0026#39;, cache_folder = tempdir()) ## Fetching info on B3 companies ## Dowloading file from CVM ## Reading file from CVM ## Saving cache data ## Got 2331 lines for 2290 companies [Actives = 648 Inactives = 1653] ## Found 1 companies: ## GRENDENE SA | situation = ATIVO | sector = Têxtil e Vestuário | CD_CVM = 19615 ## # A tibble: 1 x 44 ## CD_CVM DENOM_SOCIAL DENOM_COMERC SETOR_ATIV PF_PJ CNPJ DT_REG DT_CONST ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 19615 GRENDENE SA GRENDENE SA Têxtil e Ves… PJ 8985034… 26/10/… 25/02/1… ## # … with 36 more variables: DT_CANCEL \u0026lt;chr\u0026gt;, MOTIVO_CANCEL \u0026lt;chr\u0026gt;, ## # SIT_REG \u0026lt;chr\u0026gt;, DT_INI_SIT \u0026lt;chr\u0026gt;, SIT_EMISSOR \u0026lt;chr\u0026gt;, ## # DT_INI_SIT_EMISSOR \u0026lt;chr\u0026gt;, CATEG_REG \u0026lt;chr\u0026gt;, DT_INI_CATEG \u0026lt;chr\u0026gt;, ## # AUDITOR \u0026lt;chr\u0026gt;, CNPJ_AUDITOR \u0026lt;dbl\u0026gt;, TP_ENDER \u0026lt;chr\u0026gt;, LOGRADOURO \u0026lt;chr\u0026gt;, ## # COMPL \u0026lt;chr\u0026gt;, BAIRRO \u0026lt;chr\u0026gt;, CIDADE \u0026lt;chr\u0026gt;, UF \u0026lt;chr\u0026gt;, PAIS \u0026lt;chr\u0026gt;, ## # CD_POSTAL \u0026lt;lgl\u0026gt;, TEL \u0026lt;chr\u0026gt;, FAX \u0026lt;chr\u0026gt;, EMAIL \u0026lt;chr\u0026gt;, TP_RESP \u0026lt;chr\u0026gt;, ## # RESP \u0026lt;chr\u0026gt;, DT_INI_RESP \u0026lt;chr\u0026gt;, LOGRADOURO_RESP \u0026lt;chr\u0026gt;, COMPL_RESP \u0026lt;chr\u0026gt;, ## # BAIRRO_RESP \u0026lt;chr\u0026gt;, CIDADE_RESP \u0026lt;chr\u0026gt;, UF_RESP \u0026lt;chr\u0026gt;, PAIS_RESP \u0026lt;chr\u0026gt;, ## # CEP_RESP \u0026lt;dbl\u0026gt;, TEL_RESP \u0026lt;chr\u0026gt;, FAX_RESP \u0026lt;chr\u0026gt;, EMAIL_RESP \u0026lt;chr\u0026gt;, ## # TP_MERC \u0026lt;chr\u0026gt;, cnpj_number \u0026lt;dbl\u0026gt; l_fre \u0026lt;- get_fre_data(companies_cvm_codes = 19615, fre_to_read = \u0026#39;last\u0026#39;, first_year = 2020, last_year = 2020, cache_folder = tempdir()) ## Fetching ftp contents ## * Reading fre_cia_aberta_2020.zip ## ## Found 1 FRE docs to read ## Starting Downloads: ## -\u0026gt; Company 19615 | fre file 100932 (ver 9) | 2020-01-01 | reading and saving cache glimpse(l_fre) ## List of 21 ## $ df_stockholders :\u0026#39;data.frame\u0026#39;: 10 obs. of 18 variables: ## ..$ CNPJ_CIA : chr [1:10] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:10] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:10], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:10] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:10] 100932 100932 100932 100932 100932 ... ## ..$ VERSAO : num [1:10] 9 9 9 9 9 9 9 9 9 9 ## ..$ type.register : chr [1:10] \u0026quot;Acionista\u0026quot; \u0026quot;Acionista\u0026quot; \u0026quot;Acionista\u0026quot; \u0026quot;Acionista\u0026quot; ... ## ..$ id.person : chr [1:10] \u0026quot;37071813833 \u0026quot; \u0026quot;09867597087 \u0026quot; \u0026quot;09864784072 \u0026quot; \u0026quot;68595743053 \u0026quot; ... ## ..$ id.nationality : chr [1:10] \u0026quot;Brasileira\u0026quot; \u0026quot;Brasileiro\u0026quot; \u0026quot;Brasileiro\u0026quot; \u0026quot;Brasileiro\u0026quot; ... ## ..$ id.state : chr [1:10] \u0026quot;São Paulo\u0026quot; \u0026quot;Rio Grande do Sul\u0026quot; \u0026quot;Rio Grande do Sul\u0026quot; \u0026quot;Rio Grande do Sul\u0026quot; ... ## ..$ id.country : logi [1:10] NA NA NA NA NA NA ... ## ..$ name.stockholder : chr [1:10] \u0026quot;Gabriella de Camargo Bartelle\u0026quot; \u0026quot;Alexandre Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Pedro Bartelle\u0026quot; ... ## ..$ type.stockholder : chr [1:10] \u0026quot;Fisica\u0026quot; \u0026quot;Fisica\u0026quot; \u0026quot;Fisica\u0026quot; \u0026quot;Fisica\u0026quot; ... ## ..$ qtd.ord.shares : chr [1:10] \u0026quot;28912677\u0026quot; \u0026quot;371651807\u0026quot; \u0026quot;125312376\u0026quot; \u0026quot;35557397\u0026quot; ... ## ..$ perc.ord.shares : chr [1:10] \u0026quot;3.200000\u0026quot; \u0026quot;41.200000\u0026quot; \u0026quot;13.890000\u0026quot; \u0026quot;3.940000\u0026quot; ... ## ..$ qtd.pref.shares : chr [1:10] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; ... ## ..$ perc.pref.shares : chr [1:10] \u0026quot;0.000000\u0026quot; \u0026quot;0.000000\u0026quot; \u0026quot;0.000000\u0026quot; \u0026quot;0.000000\u0026quot; ... ## ..$ controlling.stockholder: logi [1:10] TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ df_capital :\u0026#39;data.frame\u0026#39;: 2 obs. of 9 variables: ## ..$ CNPJ_CIA : chr [1:2] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:2] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:2], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num [1:2] 19615 19615 ## ..$ ID_DOC : num [1:2] 100932 100932 ## ..$ VERSAO : num [1:2] 9 9 ## ..$ stock.type : chr [1:2] \u0026quot;ON\u0026quot; \u0026quot;PN\u0026quot; ## ..$ stock.class: chr [1:2] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; ## ..$ qtd.issued : num [1:2] 9.02e+08 0.00 ## $ df_stock_values :\u0026#39;data.frame\u0026#39;: 2 obs. of 13 variables: ## ..$ CNPJ_CIA : chr [1:2] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:2] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:2], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num [1:2] 19615 19615 ## ..$ ID_DOC : num [1:2] 100932 100932 ## ..$ VERSAO : num [1:2] 9 9 ## ..$ stock.class : chr [1:2] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; ## ..$ stock.type : chr [1:2] \u0026quot;ON\u0026quot; \u0026quot;PN\u0026quot; ## ..$ max.price : num [1:2] 12.7 0 ## ..$ min.price : num [1:2] 7.94 0 ## ..$ avg.price : num [1:2] 10.5 0 ## ..$ flag.missing.avg.price: logi [1:2] FALSE NA ## ..$ qtd.issued : num [1:2] 9.02e+08 0.00 ## $ df_mkt_value :\u0026#39;data.frame\u0026#39;: 1 obs. of 9 variables: ## ..$ CNPJ_CIA : chr \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:1], format: \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num 19615 ## ..$ ID_DOC : num 100932 ## ..$ VERSAO : num 9 ## ..$ mkt.avg.value: num 9.44e+09 ## ..$ mkt.min.value: num 7.16e+09 ## ..$ mkt.max.value: num 1.14e+10 ## $ df_increase_capital :\u0026#39;data.frame\u0026#39;: 0 obs. of 0 variables ## $ df_capital_reduction :\u0026#39;data.frame\u0026#39;: 0 obs. of 0 variables ## $ df_compensation :\u0026#39;data.frame\u0026#39;: 1 obs. of 22 variables: ## ..$ CNPJ_CIA : chr \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:1], format: \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num 19615 ## ..$ ID_DOC : num 100932 ## ..$ VERSAO : num 9 ## ..$ level.remuneration : chr \u0026quot;Management Council\u0026quot; ## ..$ qtd.members : num 6 ## ..$ qtd.remunerated.members : num 6 ## ..$ total.value.remuneration : num 1188000 ## ..$ fixed.salary : num 1188000 ## ..$ fixed.benefits : num 0 ## ..$ fixed.participations : num 0 ## ..$ fixed.others : num 0 ## ..$ variable.bonus : num 0 ## ..$ variable.results.participation : num 0 ## ..$ variable.meetings.participation : num 0 ## ..$ variable.commissions.participation: num 0 ## ..$ variable.others : num 0 ## ..$ post.job.compensation : num 0 ## ..$ ceasing.job.compensation : num 0 ## ..$ stocks.options.benefits : num 0 ## $ df_compensation_summary :\u0026#39;data.frame\u0026#39;: 3 obs. of 13 variables: ## ..$ CNPJ_CIA : chr [1:3] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:3] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:3], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:3] 19615 19615 19615 ## ..$ ID_DOC : num [1:3] 100932 100932 100932 ## ..$ VERSAO : num [1:3] 9 9 9 ## ..$ level.remuneration : chr [1:3] \u0026quot;Management Council\u0026quot; \u0026quot;Statutory Directors\u0026quot; \u0026quot;Fiscal Council\u0026quot; ## ..$ qtd.members : num [1:3] 6 3 3 ## ..$ qtd.remunerated.members: num [1:3] 6 3 3 ## ..$ max.remuneration : num [1:3] 198000 2520995 148740 ## ..$ mean.remuneration : num [1:3] 198000 1988385 148740 ## ..$ min.remuneration : num [1:3] 198000 1210729 148740 ## ..$ observations : logi [1:3] NA NA NA ## $ df_transactions_related :\u0026#39;data.frame\u0026#39;: 41 obs. of 17 variables: ## ..$ CNPJ_CIA : chr [1:41] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:41] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:41], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:41] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:41] 100932 100932 100932 100932 100932 ... ## ..$ VERSAO : num [1:41] 9 9 9 9 9 9 9 9 9 9 ... ## ..$ id.transaction : chr [1:41] \u0026quot;1944\u0026quot; \u0026quot;1945\u0026quot; \u0026quot;1946\u0026quot; \u0026quot;1947\u0026quot; ... ## ..$ name.related.part : chr [1:41] \u0026quot;Dall\u0026#39;Onder Viagens \u0026amp; Turismo Ltda\u0026quot; \u0026quot;Grendene UK Limited\u0026quot; \u0026quot;Grendene Italy SRL\u0026quot; \u0026quot;MHL Calçados Ltda\u0026quot; ... ## ..$ date.transaction : Date[1:41], format: \u0026quot;2017-12-31\u0026quot; \u0026quot;2019-12-31\u0026quot; ... ## ..$ description.related.part : chr [1:41] \u0026quot;Empresa pertencente a família de um dos administradores\u0026quot; \u0026quot;Empresa controlada\u0026quot; \u0026quot;Empresa controlada indireta\u0026quot; \u0026quot;Empresa controlada\u0026quot; ... ## ..$ description.transaction : chr [1:41] \u0026quot;Serviços de assessoria e agenciamento de viagens aéreas\u0026quot; \u0026quot;Venda de calçados para abastecimento do mercado onde a mesma está sediada\u0026quot; \u0026quot;Venda de calçados para abastecimento do mercado onde a mesma está sediada\u0026quot; \u0026quot;Venda de insumos\u0026quot; ... ## ..$ value.transaction : chr [1:41] \u0026quot;479000.00\u0026quot; \u0026quot;832000.00\u0026quot; \u0026quot;1605000.00\u0026quot; \u0026quot;795000.00\u0026quot; ... ## ..$ description.guarantees : chr [1:41] \u0026quot;Não aplicável\u0026quot; \u0026quot;Não aplicável\u0026quot; \u0026quot;Não aplicável\u0026quot; \u0026quot;Não aplicável\u0026quot; ... ## ..$ description.transaction.period: chr [1:41] \u0026quot;Prazo indeterminado\u0026quot; \u0026quot;Prazo indeterminado\u0026quot; \u0026quot;Prazo indeterminado\u0026quot; \u0026quot;Prazo indeterminado\u0026quot; ... ## ..$ description.rescision : chr [1:41] \u0026quot;Encerramento das atividades\u0026quot; \u0026quot;Encerramento das atividades\u0026quot; \u0026quot;Encerramento das atividades\u0026quot; \u0026quot;Encerramento das atividades\u0026quot; ... ## ..$ interest.rate : num [1:41] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ value.balance : chr [1:41] \u0026quot;R$ 0,00\u0026quot; \u0026quot;R$483.000,00\u0026quot; \u0026quot;R$1.318.000,00\u0026quot; \u0026quot;R$1.000,00\u0026quot; ... ## $ df_other_events :\u0026#39;data.frame\u0026#39;: 1 obs. of 12 variables: ## ..$ CNPJ_CIA : chr \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:1], format: \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num 19615 ## ..$ ID_DOC : num 100932 ## ..$ VERSAO : num 9 ## ..$ approval.date : Date[1:1], format: \u0026quot;2018-04-23\u0026quot; ## ..$ type.event : chr \u0026quot;Desdobramento\u0026quot; ## ..$ qtd.ord.shares.before : num 3.01e+08 ## ..$ qtd.ord.shares.after : num 9.02e+08 ## ..$ qtd.pref.shares.before: num 0 ## ..$ qtd.pref.shares.after : num 0 ## $ df_stock_repurchases :\u0026#39;data.frame\u0026#39;: 4 obs. of 16 variables: ## ..$ CNPJ_CIA : chr [1:4] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:4] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:4], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:4] 19615 19615 19615 19615 ## ..$ ID_DOC : num [1:4] 100932 100932 100932 100932 ## ..$ VERSAO : num [1:4] 9 9 9 9 ## ..$ date.decision : Date[1:4], format: \u0026quot;2020-03-25\u0026quot; \u0026quot;2019-04-25\u0026quot; ... ## ..$ date.start.repurchase : Date[1:4], format: \u0026quot;2020-03-25\u0026quot; \u0026quot;2019-04-25\u0026quot; ... ## ..$ date.end.repurchase : Date[1:4], format: \u0026quot;2021-09-16\u0026quot; \u0026quot;2020-03-25\u0026quot; ... ## ..$ available.capital.repurchase : num [1:4] 25205940 29188481 14563536 16117227 ## ..$ type.stock : chr [1:4] \u0026quot;Ordinária\u0026quot; \u0026quot;Ordinária\u0026quot; \u0026quot;Ordinária\u0026quot; \u0026quot;Ordinária\u0026quot; ## ..$ qtd.stocks.repurchased : num [1:4] 0 1467613 1312343 1185681 ## ..$ qtd.stocks.predicted : num [1:4] 2.5e+07 6.0e+06 2.0e+06 1.5e+06 ## ..$ average.price : num [1:4] 0 11.1 26.8 17.6 ## ..$ percent.stock.float.purchased: num [1:4] 0 24.5 65.6 79 ## ..$ percent.stock.float.predicted: chr [1:4] \u0026quot;9.220000\u0026quot; \u0026quot;2.220000\u0026quot; \u0026quot;2.380000\u0026quot; \u0026quot;1.820000\u0026quot; ## $ df_debt_composition :\u0026#39;data.frame\u0026#39;: 2 obs. of 13 variables: ## ..$ CNPJ_CIA : chr [1:2] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:2] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:2], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num [1:2] 19615 19615 ## ..$ ID_DOC : num [1:2] 100932 100932 ## ..$ VERSAO : num [1:2] 9 9 ## ..$ type.debt : chr [1:2] \u0026quot;Empréstimo\u0026quot; \u0026quot;Financiamento\u0026quot; ## ..$ type.debt.guarantee : chr [1:2] \u0026quot;Garantia Real\u0026quot; \u0026quot;Quirografárias\u0026quot; ## ..$ debt.value.under.1.year: num [1:2] 1.04e+07 2.88e+08 ## ..$ debt.value.1.to.3.years: num [1:2] 10340527 72663044 ## ..$ debt.value.3.to.5.years: num [1:2] 0 1064371 ## ..$ debt.value.more.5.years: num [1:2] 0 0 ## ..$ debt.total : num [1:2] 2.07e+07 3.61e+08 ## $ df_board_composition :\u0026#39;data.frame\u0026#39;: 16 obs. of 22 variables: ## ..$ CNPJ_CIA : chr [1:16] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:16] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:16], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:16] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:16] 100932 100932 100932 100932 100932 ... ## ..$ VERSAO : num [1:16] 9 9 9 9 9 9 9 9 9 9 ... ## ..$ person.name : chr [1:16] \u0026quot;Gelson Luis Rostirolla\u0026quot; \u0026quot;Rudimar Dall Onder\u0026quot; \u0026quot;Alceu Demartini de Albuquerque\u0026quot; \u0026quot;Alexandre Grendene Bartelle\u0026quot; ... ## ..$ person.cpf : num [1:16] 1.48e+10 2.55e+10 9.56e+10 9.87e+09 4.30e+09 ... ## ..$ person.profession : chr [1:16] \u0026quot;Administrador de Empresas\u0026quot; \u0026quot;Engenheiro Mecânico\u0026quot; \u0026quot;Administrador de Empresas\u0026quot; \u0026quot;Industrial\u0026quot; ... ## ..$ person.cv : chr [1:16] \u0026quot;Formação: Administração de Empresas (1977) e Ciências Contábeis (1979) pela UNOESC – Universidade do Oeste Cata\u0026quot;| __truncated__ \u0026quot;Formação: Engenharia Mecânica (1981) pela Universidade de Caxias do SUL (UCS). Iniciou suas atividades na Compa\u0026quot;| __truncated__ \u0026quot;Formação: Master of Business Administration. University of Illinois, conclusão julho 2019. Pós-graduado em Rela\u0026quot;| __truncated__ \u0026quot;Fundador da Companhia e Presidente do Conselho de Administração desde 18 de agosto de 2004. \\n\\nFormação: Bacha\u0026quot;| __truncated__ ... ## ..$ person.dob : Date[1:16], format: NA NA ... ## ..$ code.type.board : chr [1:16] \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; ... ## ..$ desc.type.board : chr [1:16] \u0026quot;Director\u0026quot; \u0026quot;Director\u0026quot; \u0026quot;Director\u0026quot; \u0026quot;Management Council\u0026quot; ... ## ..$ desc.type.board2 : logi [1:16] NA NA NA NA NA NA ... ## ..$ code.type.job : chr [1:16] \u0026quot;11\u0026quot; \u0026quot;10\u0026quot; \u0026quot;12\u0026quot; \u0026quot;20\u0026quot; ... ## ..$ desc.job : chr [1:16] \u0026quot;Não ocupa outras funções no emissor.\u0026quot; \u0026quot;Diretor Administrativo Financeiro, Membro do Comitê de Investimentos e Membro do Comitê de Partes Relacionadas\u0026quot; \u0026quot;Membro do Comitê de Investimentos e Membro do Comitê de Partes Relacionadas\u0026quot; \u0026quot;Presidente do comitê de gestão do programa de stock option e membro do Comitê de Investimentos\u0026quot; ... ## ..$ date.election : Date[1:16], format: \u0026quot;2019-02-14\u0026quot; \u0026quot;2019-02-14\u0026quot; ... ## ..$ date.effective : Date[1:16], format: \u0026quot;2019-02-14\u0026quot; \u0026quot;2019-02-14\u0026quot; ... ## ..$ mandate.duration : chr [1:16] \u0026quot;3 anos\u0026quot; \u0026quot;3 anos\u0026quot; \u0026quot;3 anos\u0026quot; \u0026quot;2 anos\u0026quot; ... ## ..$ ellected.by.controller : logi [1:16] TRUE TRUE TRUE TRUE TRUE TRUE ... ## ..$ qtd.consecutive.mandates: num [1:16] 6 6 1 9 9 9 9 9 8 1 ... ## ..$ percentage.participation: num [1:16] 0 0 0 100 100 100 100 100 100 0 ... ## $ df_committee_composition :\u0026#39;data.frame\u0026#39;: 13 obs. of 22 variables: ## ..$ CNPJ_CIA : chr [1:13] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:13] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:13], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:13] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:13] 100932 100932 100932 100932 100932 ... ## ..$ VERSAO : num [1:13] 9 9 9 9 9 9 9 9 9 9 ... ## ..$ person.name : chr [1:13] \u0026quot;Carlos Augusto Leone Piani\u0026quot; \u0026quot;Alceu Demartini de Albuquerque\u0026quot; \u0026quot;Rudimar Dall Onder\u0026quot; \u0026quot;Rafael Vieira Grazziotin\u0026quot; ... ## ..$ person.cpf : num [1:13] 2.53e+09 9.56e+10 2.55e+10 7.00e+10 3.54e+10 ... ## ..$ person.profession : chr [1:13] \u0026quot;Administrador de Empresas\u0026quot; \u0026quot;Administrador de Empresas\u0026quot; \u0026quot;Engenheiro Mecânico\u0026quot; \u0026quot;Advogado\u0026quot; ... ## ..$ person.cv : chr [1:13] \u0026quot;O Sr. Carlos Augusto Leone Piani é presidente da divisão canadense da Kraft Heinz Company, tendo sido diretor d\u0026quot;| __truncated__ \u0026quot;Formação: Master of Business Administration. University of Illinois, conclusão julho 2019. Pós-graduado em Rela\u0026quot;| __truncated__ \u0026quot;Formação: Engenharia Mecânica (1981) pela Universidade de Caxias do SUL (UCS). Iniciou suas atividades na Compa\u0026quot;| __truncated__ \u0026quot;O Sr. Rafael Vieira Grazziotin, advogado graduado pela Universidade de Caxias do Sul, com pós graduação em Dire\u0026quot;| __truncated__ ... ## ..$ person.dob : Date[1:13], format: NA NA ... ## ..$ code.type.committee : chr [1:13] \u0026quot;9\u0026quot; \u0026quot;9\u0026quot; \u0026quot;9\u0026quot; \u0026quot;9\u0026quot; ... ## ..$ desc.type.committee : chr [1:13] \u0026quot;Other Committee\u0026quot; \u0026quot;Other Committee\u0026quot; \u0026quot;Other Committee\u0026quot; \u0026quot;Other Committee\u0026quot; ... ## ..$ code.type.job : chr [1:13] \u0026quot;3\u0026quot; \u0026quot;3\u0026quot; \u0026quot;3\u0026quot; \u0026quot;3\u0026quot; ... ## ..$ desc.committee : chr [1:13] \u0026quot;Comitê de Investimentos\u0026quot; \u0026quot;Comitê de Partes Relacionadas\u0026quot; \u0026quot;Comitê de Partes Relacionadas\u0026quot; \u0026quot;Comitê de Partes Relacionadas\u0026quot; ... ## ..$ desc.job : chr [1:13] \u0026quot;Não ocupa outros cargos/funções no emissor.\u0026quot; \u0026quot;Diretor de Relações com Investidores e Membro do Comitê de Investimentos\u0026quot; \u0026quot;Diretor Presidente, Diretor Administrativo Financeiro, Membro do Comitê de Investimentos\u0026quot; \u0026quot;Não ocupa outros cargos/funções no emissor.\u0026quot; ... ## ..$ date.election : Date[1:13], format: \u0026quot;2020-08-13\u0026quot; \u0026quot;2020-08-13\u0026quot; ... ## ..$ date.effective : Date[1:13], format: \u0026quot;2020-08-13\u0026quot; \u0026quot;2020-08-13\u0026quot; ... ## ..$ mandate.duration : chr [1:13] \u0026quot;3 anos\u0026quot; \u0026quot;2 anos\u0026quot; \u0026quot;2 anos\u0026quot; \u0026quot;2 anos\u0026quot; ... ## ..$ qtd.consecutive.mandates: num [1:13] 1 1 1 1 1 1 1 1 1 1 ... ## ..$ percentage.participation: num [1:13] 100 100 100 100 100 100 100 100 100 100 ... ## ..$ other.committes : chr [1:13] \u0026quot;Comitê de Investimentos\u0026quot; \u0026quot;Comitê de Partes Relacionadas\u0026quot; \u0026quot;Comitê de Partes Relacionadas\u0026quot; \u0026quot;Comitê de Partes Relacionadas\u0026quot; ... ## $ df_family_relations :\u0026#39;data.frame\u0026#39;: 6 obs. of 14 variables: ## ..$ CNPJ_CIA : chr [1:6] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:6] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:6], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:6] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:6] 100932 100932 100932 100932 100932 ... ## ..$ VERSAO : num [1:6] 9 9 9 9 9 9 ## ..$ person.name : chr [1:6] \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Alexandre Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; ... ## ..$ person.cpf : num [1:6] 9.86e+09 9.87e+09 9.86e+09 9.86e+09 9.86e+09 ... ## ..$ person.job : chr [1:6] \u0026quot;Vice Presidente do Conselho de Administração\u0026quot; \u0026quot;Presidente do Conselho de Administração\u0026quot; \u0026quot;Vice Presidente do Conselho de Administração\u0026quot; \u0026quot;Vice Presidente do Conselho de Administração\u0026quot; ... ## ..$ related.person.name: chr [1:6] \u0026quot;Alexandre Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Pedro Bartelle\u0026quot; \u0026quot;Giovana Bartelle Velloso\u0026quot; ... ## ..$ related.person.cpf : num [1:6] 9.87e+09 9.86e+09 6.86e+10 6.86e+10 3.54e+10 ... ## ..$ related.person.job : chr [1:6] \u0026quot;Presidente do Conselho de Administração\u0026quot; \u0026quot;Vice Presidente do Conselho de Administração\u0026quot; \u0026quot;Acionista\u0026quot; \u0026quot;Acionista\u0026quot; ... ## ..$ code.relationship : chr [1:6] \u0026quot;2\u0026quot; \u0026quot;2\u0026quot; \u0026quot;3\u0026quot; \u0026quot;3\u0026quot; ... ## ..$ desc.relationship : chr [1:6] \u0026quot;Irmão ou Irmã (1º grau por consangüinidade)\u0026quot; \u0026quot;Irmão ou Irmã (1º grau por consangüinidade)\u0026quot; \u0026quot;Filho ou Filha (1º grau por consangüinidade)\u0026quot; \u0026quot;Filho ou Filha (1º grau por consangüinidade)\u0026quot; ... ## $ df_family_related_companies:\u0026#39;data.frame\u0026#39;: 15 obs. of 15 variables: ## ..$ CNPJ_CIA : chr [1:15] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:15] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:15], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:15] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:15] 100932 100932 100932 100932 100932 ... ## ..$ VERSAO : num [1:15] 9 9 9 9 9 9 9 9 9 9 ... ## ..$ person.name : chr [1:15] \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Maílson Ferreira da Nóbrega\u0026quot; \u0026quot;Renato Ochman\u0026quot; ... ## ..$ person.cpf : num [1:15] 9.86e+09 9.86e+09 4.30e+09 3.76e+10 3.76e+10 ... ## ..$ person.job : chr [1:15] \u0026quot;Vice-Presidente do Conselho de Administração\u0026quot; \u0026quot;Vice-Presidente do Conselho de Administração\u0026quot; \u0026quot;Conselheiro de Administração\u0026quot; \u0026quot;Conselheiro de Administração\u0026quot; ... ## ..$ type.related.person : chr [1:15] \u0026quot;Cliente\u0026quot; \u0026quot;Cliente\u0026quot; \u0026quot;Fornecedor\u0026quot; \u0026quot;Fornecedor\u0026quot; ... ## ..$ type.relationship : chr [1:15] \u0026quot;Controle\u0026quot; \u0026quot;Controle\u0026quot; \u0026quot;Controle\u0026quot; \u0026quot;Controle\u0026quot; ... ## ..$ observations : chr [1:15] \u0026quot;Venda de insumos e matrizes - Prazo médio de recebimento 21 dias\u0026quot; \u0026quot;Venda de matrizes - Prazo médio de recebimento 43 dias\u0026quot; \u0026quot;Assessoria\u0026quot; \u0026quot;Assessoria\u0026quot; ... ## ..$ related.company.name: chr [1:15] \u0026quot;Vulcabras|azaleia – CE, Calçados e Artigos Esportivos S.A.\u0026quot; \u0026quot;Vulcabras|azaleia – BA, Calçados e Artigos Esportivos S.A.\u0026quot; \u0026quot;Mailson da Nóbrega Consultoria S/C Ltda\u0026quot; \u0026quot;Ochman, Real Amadeo Advogados Associados\u0026quot; ... ## ..$ related.company.cnpj: num [1:15] 9.54e+11 7.34e+11 1.58e+12 6.24e+13 6.24e+13 ... ## ..$ related.company.job : chr [1:15] \u0026quot;Acionista Controlador\u0026quot; \u0026quot;Acionista controlador\u0026quot; \u0026quot;Sócio proprietário\u0026quot; \u0026quot;Sócio proprietário\u0026quot; ... ## $ df_auditing :\u0026#39;data.frame\u0026#39;: 1 obs. of 14 variables: ## ..$ CNPJ_CIA : chr \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:1], format: \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num 19615 ## ..$ ID_DOC : num 100932 ## ..$ VERSAO : num 9 ## ..$ auditor.name : chr \u0026quot;Ernst \u0026amp; Young Auditores Independentes S/S\u0026quot; ## ..$ auditor.cnpj : chr \u0026quot;61366936001105\u0026quot; ## ..$ contract.first.date : Date[1:1], format: NA ## ..$ contract.last.date : Date[1:1], format: NA ## ..$ description.contract : chr \u0026quot;Revisão dos ITR\u0026#39;s (controladora e Consolidado) e auditoria anual de balanço da Controladora e Consolidado.\u0026quot; ## ..$ compensation : chr \u0026quot;Para o exercício encerrado em 31/12/2017 - R$409,2 mil, referente a serviços de auditoria prestados e R$131,1 m\u0026quot;| __truncated__ ## ..$ justification.substitution: logi NA ## ..$ reason.discordance : logi NA ## $ df_responsible_docs :\u0026#39;data.frame\u0026#39;: 2 obs. of 9 variables: ## ..$ CNPJ_CIA : chr [1:2] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:2] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:2], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num [1:2] 19615 19615 ## ..$ ID_DOC : num [1:2] 100932 100932 ## ..$ VERSAO : num [1:2] 9 9 ## ..$ person.cod : chr [1:2] \u0026quot;50\u0026quot; \u0026quot;51\u0026quot; ## ..$ person.name: chr [1:2] \u0026quot;Rudimar Dall Onder\u0026quot; \u0026quot;Alceu Demartini de Albuquerque\u0026quot; ## ..$ person.job : chr [1:2] \u0026quot;Diretor Presidente\u0026quot; \u0026quot;Diretor de Relações com Investidores\u0026quot; ## $ df_stocks_details :\u0026#39;data.frame\u0026#39;: 1 obs. of 16 variables: ## ..$ CNPJ_CIA : chr \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:1], format: \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num 19615 ## ..$ ID_DOC : num 100932 ## ..$ VERSAO : num 9 ## ..$ type.stock.id : chr \u0026quot;1\u0026quot; ## ..$ type.stock.text : chr \u0026quot;Ordinária\u0026quot; ## ..$ tag.along : num 100 ## ..$ preferential.code : chr \u0026quot;0\u0026quot; ## ..$ preferential.text : logi NA ## ..$ dividend.text : chr \u0026quot;Conforme o Estatuto Social da Companhia, art.32, os acionistas fazem jus a dividendo obrigatório anual equivale\u0026quot;| __truncated__ ## ..$ flag.voting.rights : chr \u0026quot;1\u0026quot; ## ..$ flag.voting.text : chr \u0026quot;Pleno\u0026quot; ## ..$ flag.conversibility: chr \u0026quot;Não\u0026quot; ## ..$ other.info.text : chr \u0026quot;Não existem características relevantes adicionais.\u0026quot; ## $ df_dividends_details :\u0026#39;data.frame\u0026#39;: 1 obs. of 11 variables: ## ..$ CNPJ_CIA : chr \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:1], format: \u0026quot;2020-01-01\u0026quot; ## ..$ CD_CVM : num 19615 ## ..$ ID_DOC : num 100932 ## ..$ VERSAO : num 9 ## ..$ net.profit : num 4.95e+08 ## ..$ distributed.dividend: num 2.76e+08 ## ..$ retained.profit : num 2.19e+08 ## ..$ payout : num 55.7 ## ..$ div.yeild.on.equity : num 14.3 ## $ df_intangible_details :\u0026#39;data.frame\u0026#39;: 13 obs. of 10 variables: ## ..$ CNPJ_CIA : chr [1:13] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:13] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:13], format: \u0026quot;2020-01-01\u0026quot; \u0026quot;2020-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:13] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:13] 100932 100932 100932 100932 100932 ... ## ..$ VERSAO : num [1:13] 9 9 9 9 9 9 9 9 9 9 ... ## ..$ id : num [1:13] 1033 1034 1035 1036 1037 ... ## ..$ id.type : num [1:13] 2 2 2 2 2 2 2 2 2 2 ... ## ..$ patent.desc: chr [1:13] \u0026quot;Mel\u0026quot; \u0026quot;Nuar\u0026quot; \u0026quot;Pega Forte\u0026quot; \u0026quot;Galeria Melissa\u0026quot; ... ## ..$ duration : Date[1:13], format: NA NA ...  ","date":1617667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617667200,"objectID":"89c8e680e497dd3f474a42b92d22a4ee","permalink":"http://www.msperlin.com/blog/post/2021-04-06-getfredata-on-cran/","publishdate":"2021-04-06T00:00:00Z","relpermalink":"/blog/post/2021-04-06-getfredata-on-cran/","section":"post","summary":"I’m happy to report that package GetFREData is now available in CRAN. This R package serves as an interface to all corporate datasets available in the FRE system, a vast and official repository of information about many different corporate events. All companies listed at B3 – Brazilian stock exchange – must report to FRE any significant change in their corporate structure. You can find more details about what is available in FRE in its web interface.\nThe R package fetches data from the CVM ftp, downloads and parses the xml files, and output several tables as a list.","tags":["R","GetFREData"],"title":"GetFREData available in CRAN!","type":"post"},{"authors":null,"categories":["R","GetDFPData2"],"content":"  After testing the package extensivelly, GetDFPData2 is finally available in CRAN. GetDFPData2 is the second and backwards incompatible version of GetDPFData, a R package for downloading annual financial reports from B3, the Brazilian financial exchange. Unlike its first iteration, GetDFPData2 imports data using a database of csv files from CVM, which makes it execution much faster than its predecessor. However, the output is slightly different.\nA shiny app – web interface – is also available at https://www.msperlin.com/shiny/GetDFPData2/.\nThe previous version, GetDFPData, is deprecated and will not be developed any further. All efforts goes to GetDFPData2 and GetFREData (soon in CRAN).\nInstallation # available in cran (stable) install.packages(\u0026#39;GetDFPData2\u0026#39;) # github (dev version) devtools::install_github(\u0026#39;msperlin/GetDFPData2\u0026#39;)  Examples of Usage Information about available companies library(GetDFPData2) # information about companies df_info \u0026lt;- get_info_companies(tempdir()) ## Fetching info on B3 companies ## Dowloading file from CVM ## File not found, downloading it.. ## Success ## Reading file from CVM ## Saving cache data ## Got 2331 lines for 2290 companies [Actives = 648 Inactives = 1653] print(df_info ) ## # A tibble: 2,331 x 44 ## CD_CVM DENOM_SOCIAL DENOM_COMERC SETOR_ATIV PF_PJ CNPJ DT_REG DT_CONST ## \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 25224 2W ENERGIA S.A. \u0026lt;NA\u0026gt; Construção… PJ 8773… 29/10… 23/03/2… ## 2 21954 3A COMPANHIA S… TRIPLO A COM… Securitiza… PJ 1139… 08/03… 03/11/2… ## 3 25291 3R PETROLEUM O… \u0026lt;NA\u0026gt; Petróleo e… PJ 1209… 09/11… 08/06/2… ## 4 16330 521 PARTICIPAÇ… 521 PARTICIPA… Emp. Adm. … PJ 1547… 11/07… 30/07/1… ## 5 16284 524 PARTICIPAÇ… 524 PARTICIPA… Emp. Adm. … PJ 1851… 30/05… 02/04/1… ## 6 16349 525 PARTICIPAÇ… 525 PARTICIPA… Emp. Adm. … PJ 1919… 16/07… 02/04/1… ## 7 35 A J RENNER SA … A J RENNER Emp. Adm. … PJ 9265… 24/06… \u0026lt;NA\u0026gt; ## 8 16802 A.P. PARTICIPA… A.P. PARTICIP… Emp. Adm. … PJ 2288… 21/01… 14/12/1… ## 9 13307 ABC DADOS E IN… ABC COMPUTADO… Máquinas, … PJ 2164… 03/06… \u0026lt;NA\u0026gt; ## 10 16934 ABC SUPERMERCA… ABC SUPERMERC… Comércio (… PJ 2258… 27/02… 30/09/1… ## # … with 2,321 more rows, and 36 more variables: DT_CANCEL \u0026lt;chr\u0026gt;, ## # MOTIVO_CANCEL \u0026lt;chr\u0026gt;, SIT_REG \u0026lt;chr\u0026gt;, DT_INI_SIT \u0026lt;chr\u0026gt;, SIT_EMISSOR \u0026lt;chr\u0026gt;, ## # DT_INI_SIT_EMISSOR \u0026lt;chr\u0026gt;, CATEG_REG \u0026lt;chr\u0026gt;, DT_INI_CATEG \u0026lt;chr\u0026gt;, ## # AUDITOR \u0026lt;chr\u0026gt;, CNPJ_AUDITOR \u0026lt;dbl\u0026gt;, TP_ENDER \u0026lt;chr\u0026gt;, LOGRADOURO \u0026lt;chr\u0026gt;, ## # COMPL \u0026lt;chr\u0026gt;, BAIRRO \u0026lt;chr\u0026gt;, CIDADE \u0026lt;chr\u0026gt;, UF \u0026lt;chr\u0026gt;, PAIS \u0026lt;chr\u0026gt;, ## # CD_POSTAL \u0026lt;lgl\u0026gt;, TEL \u0026lt;chr\u0026gt;, FAX \u0026lt;chr\u0026gt;, EMAIL \u0026lt;chr\u0026gt;, TP_RESP \u0026lt;chr\u0026gt;, ## # RESP \u0026lt;chr\u0026gt;, DT_INI_RESP \u0026lt;chr\u0026gt;, LOGRADOURO_RESP \u0026lt;chr\u0026gt;, COMPL_RESP \u0026lt;chr\u0026gt;, ## # BAIRRO_RESP \u0026lt;chr\u0026gt;, CIDADE_RESP \u0026lt;chr\u0026gt;, UF_RESP \u0026lt;chr\u0026gt;, PAIS_RESP \u0026lt;chr\u0026gt;, ## # CEP_RESP \u0026lt;dbl\u0026gt;, TEL_RESP \u0026lt;chr\u0026gt;, FAX_RESP \u0026lt;chr\u0026gt;, EMAIL_RESP \u0026lt;chr\u0026gt;, ## # TP_MERC \u0026lt;chr\u0026gt;, cnpj_number \u0026lt;dbl\u0026gt;  Searching for companies search_company(\u0026#39;grendene\u0026#39;, cache_folder = tempdir()) ## Fetching info on B3 companies ## Found cache file. Loading data.. ## Got 2331 lines for 2290 companies [Actives = 648 Inactives = 1653] ## Found 1 companies: ## GRENDENE SA | situation = ATIVO | sector = Têxtil e Vestuário | CD_CVM = 19615  Downloading Annual Financial Reports # downloading DFP data l_dfp \u0026lt;- get_dfp_data(companies_cvm_codes = 19615, use_memoise = FALSE, clean_data = TRUE, cache_folder = tempdir(), # use local folder in live code type_docs = c(\u0026#39;DRE\u0026#39;), type_format = \u0026#39;con\u0026#39;, first_year = 2019, last_year = 2020) ## Dowloading dfp_cia_aberta_2019.zip ## File not found, downloading it.. ## Success ## Unzipping ## Reading dfp_cia_aberta_DRE_con_2019.csv | Cleaning table ## Got 30 rows | 1 companies ## Dowloading dfp_cia_aberta_2020.zip ## File not found, downloading it.. ## Success ## Unzipping ## Reading dfp_cia_aberta_DRE_con_2020.csv | Cleaning table ## Got 32 rows | 1 companies str(l_dfp) ## List of 1 ## $ DF Consolidado - Demonstração do Resultado: tibble[,16] [62 × 16] (S3: tbl_df/tbl/data.frame) ## ..$ CNPJ_CIA : chr [1:62] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ CD_CVM : num [1:62] 19615 19615 19615 19615 19615 ... ## ..$ DT_REFER : Date[1:62], format: \u0026quot;2019-12-31\u0026quot; \u0026quot;2019-12-31\u0026quot; ... ## ..$ DT_INI_EXERC: Date[1:62], format: \u0026quot;2019-01-01\u0026quot; \u0026quot;2019-01-01\u0026quot; ... ## ..$ DT_FIM_EXERC: Date[1:62], format: \u0026quot;2019-12-31\u0026quot; \u0026quot;2019-12-31\u0026quot; ... ## ..$ DENOM_CIA : chr [1:62] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ VERSAO : num [1:62] 2 2 2 2 2 2 2 2 2 2 ... ## ..$ GRUPO_DFP : chr [1:62] \u0026quot;DF Consolidado - Demonstração do Resultado\u0026quot; \u0026quot;DF Consolidado - Demonstração do Resultado\u0026quot; \u0026quot;DF Consolidado - Demonstração do Resultado\u0026quot; \u0026quot;DF Consolidado - Demonstração do Resultado\u0026quot; ... ## ..$ MOEDA : chr [1:62] \u0026quot;REAL\u0026quot; \u0026quot;REAL\u0026quot; \u0026quot;REAL\u0026quot; \u0026quot;REAL\u0026quot; ... ## ..$ ESCALA_MOEDA: chr [1:62] \u0026quot;MIL\u0026quot; \u0026quot;MIL\u0026quot; \u0026quot;MIL\u0026quot; \u0026quot;MIL\u0026quot; ... ## ..$ ORDEM_EXERC : chr [1:62] \u0026quot;ÚLTIMO\u0026quot; \u0026quot;ÚLTIMO\u0026quot; \u0026quot;ÚLTIMO\u0026quot; \u0026quot;ÚLTIMO\u0026quot; ... ## ..$ CD_CONTA : chr [1:62] \u0026quot;3.01\u0026quot; \u0026quot;3.02\u0026quot; \u0026quot;3.03\u0026quot; \u0026quot;3.04\u0026quot; ... ## ..$ DS_CONTA : chr [1:62] \u0026quot;Receita de Venda de Bens e/ou Serviços\u0026quot; \u0026quot;Custo dos Bens e/ou Serviços Vendidos\u0026quot; \u0026quot;Resultado Bruto\u0026quot; \u0026quot;Despesas/Receitas Operacionais\u0026quot; ... ## ..$ VL_CONTA : num [1:62] 2071034 -1126511 944523 -590995 -530825 ... ## ..$ COLUNA_DF : logi [1:62] NA NA NA NA NA NA ... ## ..$ source_file : chr [1:62] \u0026quot;dfp_cia_aberta_DRE_con_2019.csv\u0026quot; \u0026quot;dfp_cia_aberta_DRE_con_2019.csv\u0026quot; \u0026quot;dfp_cia_aberta_DRE_con_2019.csv\u0026quot; \u0026quot;dfp_cia_aberta_DRE_con_2019.csv\u0026quot; ...   ","date":1617321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617321600,"objectID":"4ce9d83f140f5f65c27218b08d4747b3","permalink":"http://www.msperlin.com/blog/post/2021-04-02-getdfpdata2-on-cran/","publishdate":"2021-04-02T00:00:00Z","relpermalink":"/blog/post/2021-04-02-getdfpdata2-on-cran/","section":"post","summary":"After testing the package extensivelly, GetDFPData2 is finally available in CRAN. GetDFPData2 is the second and backwards incompatible version of GetDPFData, a R package for downloading annual financial reports from B3, the Brazilian financial exchange. Unlike its first iteration, GetDFPData2 imports data using a database of csv files from CVM, which makes it execution much faster than its predecessor. However, the output is slightly different.\nA shiny app – web interface – is also available at https://www.msperlin.com/shiny/GetDFPData2/.\nThe previous version, GetDFPData, is deprecated and will not be developed any further.","tags":["R","GetDFPData2"],"title":"GetDFPData2 available in CRAN!","type":"post"},{"authors":null,"categories":["R","Linux Mint","Ubuntu","Bash"],"content":"  Back in 2017 I wrote a blog post describing a simple bash script for installing R in a Ubuntu setup. The problem with this script, and many others found in the internet, is that they quickly become obsolete due to changes in Ubuntu, R and RStudio. For example, if Ubuntu version changes from “trusty” to “focal”, the link to the CRAN ppa also changes. The same is true with RStudio, which does not provide installation by apt, only downloadable .deb files from its website.\nToday I manage to develop a clever bash script that uses the internet and local files to find out the current version of all software. Using three different methods – apt, snap and custom bash scripts – the script installs all required software in its latest version. The script also installs R packages set in a .txt file and configures RStudio to a dark theme. The best part is that all code is modular and you can easily customize your installs by changing .txt files in each sub-folder.\nYou can find the bash script in https://github.com/msperlin/UBUNTU-Fresh-Install.\nHow to use it Download the github repository as a zip file Unpack the zip file and check all .txt files in all sub-folders. Remove or add software/R packages as needed. Within a terminal, execute the main script:  ./UBUNTU_Install-Bash.sh type your sudo password and wait…   Installed Software The bash script includes the following software:\nUsing apt  libreoffice (lastest) texstudio (latest) obstudio (latest) many others (see file apt-to-install/list_to_install.txt)   Using custom bash scripts  R (latest) R Packages  See file R-pkgs/pkgs_to_install.txt  RStudio (latest  RStudio configuration – color scheme, size font, .. (see file Rstudio-Config/my-rstudio-prefs.json). You can get your own Rstudio preference file locally at ~/.config/rstudio/rstudio-pref.json.  Google Chrome (latest)   Using snap  Microsoft code (latest by snap)    Generating R package list You can generate your own list of used R packages based on your existing code. For that, use the R code below, which will scan your files and retrieve all calls to existing packages. Do notice you’ll need to change the base folder in renv::dependencies.\nlibrary(dplyr) my_r_dir \u0026lt;- \u0026#39;YOUR-FOLDER-HERE\u0026#39; df \u0026lt;- renv::dependencies(my_r_dir) n_to_colect \u0026lt;- 50 # number of pkgs to collect (most to least frequent) tbl_pkgs \u0026lt;- df %\u0026gt;% group_by(Package) %\u0026gt;% count() %\u0026gt;% arrange(-n) %\u0026gt;% #view() %\u0026gt;% ungroup() %\u0026gt;% slice(1:n_to_colect) tbl_pkgs  ","date":1616457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616457600,"objectID":"fab9a6864e417e2893bbb9284a153a76","permalink":"http://www.msperlin.com/blog/post/2021-03-23-ultimate-bash-script/","publishdate":"2021-03-23T00:00:00Z","relpermalink":"/blog/post/2021-03-23-ultimate-bash-script/","section":"post","summary":"Back in 2017 I wrote a blog post describing a simple bash script for installing R in a Ubuntu setup. The problem with this script, and many others found in the internet, is that they quickly become obsolete due to changes in Ubuntu, R and RStudio. For example, if Ubuntu version changes from “trusty” to “focal”, the link to the CRAN ppa also changes. The same is true with RStudio, which does not provide installation by apt, only downloadable .deb files from its website.\nToday I manage to develop a clever bash script that uses the internet and local files to find out the current version of all software.","tags":["R","Linux Mint","Ubuntu","Bash"],"title":"A clever bash script for R Users","type":"post"},{"authors":null,"categories":["R","afedR","exams","bookdown","webex"],"content":"  It’s been three years since I’ve been using package bookdown for compiling and distributing three different books in Amazon and the web. It helped me greatly in all my book projects and I’m always grateful to Yihui Xie for providing such a useful tool at the right time.\nHowever, bookdown offers no support for chapter exercises of any sort. While you can write exercises in plain RMarkdown, it is not a good solution for a long term project such as a technical book. When writing the latest edition of Analyzing Financial and Economical Data with R, I aimed for a work cycle where the 100 plus exercises and their solutions were reproducible and easier to maintain.\nMeanwhile, package exams provides a framework to produce exercises in a reproducible setup, making it possible to export the exercises to any given format such as pdf or html, or even e-learning platforms such as Moodle and Blackboard. I use exams extensively in all my university classes and it works like a charm!\nSo, while writing afedR, I worked towards finding a way to bring the two technologies closer to each other, which is what I’ll report in this blog post. Here are the main advantages of this setup:\n The content of book exercises, their solution and explanation in a single location (no more fidling with different folders). Dynamic output for html, with buttons and solutions available at a single click. Exportable exercises for classes (see this blog post). You can export the same exercises to pdf or Moodle, for example.  bookdown + exams + webex First and foremost, the main part of the hack is to realize that any exercises in a .Rmd file can be broken into a list using exams::xexams. Let’s use an example from the book, with the first three exercises of chapter 01:\n# example from book afedR::copy_book_files(path_to_copy = tempdir()) ## Copying data files files to /tmp/RtmpIs4EpM/afedR files/data ## 37 files copied ## Copying end-of-chapter (eoc) exercises with solutions to /tmp/RtmpIs4EpM/afedR files/eoc-exercises/ ## 99 files copied ## Copying R code to /tmp/RtmpIs4EpM/afedR files/R-code ## 15 files copied # temp folder with exercises eoc_dir \u0026lt;- file.path(tempdir(), \u0026#39;afedR files/eoc-exercises/\u0026#39;) # select exercises my_exercises \u0026lt;- list.files(eoc_dir, pattern = \u0026#39;*.Rmd\u0026#39;, full.names = TRUE) my_exercises \u0026lt;- my_exercises[1:3] # break it down my_l \u0026lt;- exams::xexams(my_exercises) # check it dplyr::glimpse(my_l) ## List of 1 ## $ exam1:List of 3 ## ..$ exercise1:List of 6 ## .. ..$ question : chr [1:3] \u0026quot;\u0026quot; \u0026quot;The R language was developed based on what other programming language?\u0026quot; \u0026quot;\u0026quot; ## .. ..$ questionlist: chr [1:5] \u0026quot;C++\u0026quot; \u0026quot;Python\u0026quot; \u0026quot;Julia\u0026quot; \u0026quot;Javascript\u0026quot; ... ## .. ..$ solution : chr [1:2] \u0026quot;\u0026quot; \u0026quot;Straight from the book, section **What is R**: \\\u0026quot;R is a modern version of S, a programming language originally \u0026quot;| __truncated__ ## .. ..$ solutionlist: NULL ## .. ..$ metainfo :List of 18 ## .. ..$ supplements : Named chr(0) ## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr(0) ## .. .. ..- attr(*, \u0026quot;dir\u0026quot;)= chr \u0026quot;/tmp/RtmpIs4EpM/file4e094f974499/exam1/exercise1\u0026quot; ## ..$ exercise2:List of 6 ## .. ..$ question : chr [1:3] \u0026quot;\u0026quot; \u0026quot;What are the names of the two authors of R?\u0026quot; \u0026quot;\u0026quot; ## .. ..$ questionlist: chr [1:5] \u0026quot;Linus Torvalds and Richard Stallman\u0026quot; \u0026quot;John Chambers and Robert Engle\u0026quot; \u0026quot;Roger Federer and Rafael Nadal\u0026quot; \u0026quot;Guido van Rossum and Bjarne Stroustrup\u0026quot; ... ## .. ..$ solution : chr [1:3] \u0026quot;\u0026quot; \u0026quot;Straight from the book: \\\u0026quot;... The base code of R was developed by two academics, **Ross Ihaka** and **Robert Ge\u0026quot;| __truncated__ \u0026quot;\u0026quot; ## .. ..$ solutionlist: NULL ## .. ..$ metainfo :List of 18 ## .. ..$ supplements : Named chr(0) ## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr(0) ## .. .. ..- attr(*, \u0026quot;dir\u0026quot;)= chr \u0026quot;/tmp/RtmpIs4EpM/file4e094f974499/exam1/exercise2\u0026quot; ## ..$ exercise3:List of 6 ## .. ..$ question : chr [1:4] \u0026quot;\u0026quot; \u0026quot;Why is R special when comparing to other programming languages, such as Python, C++, javascript and others?\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## .. ..$ questionlist: chr [1:5] \u0026quot;It was designed for analyzing data and producing statistical output\u0026quot; \u0026quot;Easy to use\u0026quot; \u0026quot;Works on any plataform such as Windows, Unix, MacOS\u0026quot; \u0026quot;Makes it easy to write mobile apps\u0026quot; ... ## .. ..$ solution : chr [1:2] \u0026quot;\u0026quot; \u0026quot;Undoubtedly, the main differential of the R language is the ease with which data can be analyzed on the platfor\u0026quot;| __truncated__ ## .. ..$ solutionlist: NULL ## .. ..$ metainfo :List of 18 ## .. ..$ supplements : Named chr(0) ## .. .. ..- attr(*, \u0026quot;names\u0026quot;)= chr(0) ## .. .. ..- attr(*, \u0026quot;dir\u0026quot;)= chr \u0026quot;/tmp/RtmpIs4EpM/file4e094f974499/exam1/exercise3\u0026quot; As an example, in this list you can see the main text of the question 01 in slot l_out$exam1$exercise1$question:\nmy_l$exam1$exercise1$question ## [1] \u0026quot;\u0026quot; ## [2] \u0026quot;The R language was developed based on what other programming language?\u0026quot; ## [3] \u0026quot;\u0026quot; And the solution at my_l$exam1$exercise1$solution\nmy_l$exam1$exercise1$solution ## [1] \u0026quot;\u0026quot; ## [2] \u0026quot;Straight from the book, section **What is R**: \\\u0026quot;R is a modern version of S, a programming language originally created in Bell Laboratories (formerly AT\u0026amp;T, now Lucent Technologies).\\\u0026quot;\u0026quot; In my case, I wanted the html version of the book to have all the solutions hidden by a clickable button – just like in webex – while the pdf and ebook would only have the text of the questions. Here are the functions I used:\ncompile_eoc_exercises \u0026lt;- function(files_in, type_doc) { my_counter \u0026lt;\u0026lt;- 1 if (is.null(type_doc)) { type_doc = \u0026#39;html\u0026#39; #type_doc = \u0026#39;latex\u0026#39; } for (i_ex in files_in) { exercise_to_html(i_ex, my_counter = my_counter, type_doc) my_counter \u0026lt;\u0026lt;- my_counter +1 } return(invisible(TRUE)) } exercise_to_html \u0026lt;- function(f_in, my_counter, type_doc) { require(exams) require(webex) require(tidyverse) text_pre_solution \u0026lt;- paste0(\u0026#39;To reach the same result, you must execute the code below. \u0026#39;, \u0026#39;For that, open a new R script in RStudio (Control+shift+N), \u0026#39;, \u0026#39;copy and paste the code, and execute it whole by pressing \u0026#39;, \u0026#39;Control+Shift+Enter or line by line with shortcut \u0026#39;, \u0026#39;Control+Enter.\u0026#39;) my_dir \u0026lt;- file.path(tempdir(), basename(tempfile())) dir.create(my_dir) suppressMessages({ l_out \u0026lt;- exams::xexams(f_in, driver = list(sweave = list(png = TRUE)), dir = my_dir) }) exercise_text \u0026lt;- paste0(l_out$exam1$exercise1$question, collapse = \u0026#39;\\n\u0026#39;) alternatives \u0026lt;- l_out$exam1$exercise1$questionlist correct \u0026lt;- l_out$exam1$exercise1$metainfo$solution solution \u0026lt;- paste0(l_out$exam1$exercise1$solution, collapse = \u0026#39;\\n\u0026#39;) ex_type \u0026lt;- l_out$exam1$exercise1$metainfo$type if (type_doc %in% c(\u0026#39;latex\u0026#39;, \u0026#39;epub3\u0026#39;)) { my_str \u0026lt;- str_glue(\u0026#39;\\n\\n {sprintf(\u0026quot;%02d\u0026quot;, my_counter)} - {exercise_text} \\n\\n \u0026#39;) if (ex_type == \u0026#39;schoice\u0026#39;) { n_alternatives \u0026lt;- length(alternatives) for (i_alt in seq(1, n_alternatives)) { my_str \u0026lt;- paste0(my_str, letters[i_alt],\u0026#39;) \u0026#39;, alternatives[i_alt], \u0026#39;\\n\u0026#39;) } } cat(my_str) return(invisible(TRUE)) } else if (type_doc == \u0026#39;html\u0026#39;) { if (ex_type == \u0026#39;schoice\u0026#39;) { vec_mcq \u0026lt;- sample( c(alternatives[!correct], answer = alternatives[correct]) ) my_answers_text \u0026lt;- str_glue(\u0026#39;\u0026lt;br\u0026gt; Solution: {mcq(vec_mcq)}\u0026#39;) numeric_sol \u0026lt;- alternatives[correct] text_sol \u0026lt;- str_glue(\u0026#39;The solution is {numeric_sol}. {text_pre_solution}\u0026#39;) } else if (ex_type == \u0026#39;num\u0026#39;) { numeric_sol \u0026lt;- correct my_answers_text \u0026lt;- str_glue(\u0026#39;\u0026lt;br\u0026gt;\u0026lt;br\u0026gt; Your Answer: {fitb(correct)}\u0026#39;) text_sol \u0026lt;- str_glue(\u0026#39;The solution is {numeric_sol}. {text_pre_solution}\u0026#39;) } else if (ex_type == \u0026#39;string\u0026#39;) { my_answers_text \u0026lt;- \u0026#39;\u0026#39; numeric_sol \u0026lt;- \u0026#39;\u0026#39; if (stringr::str_detect(solution, \u0026#39;```text\u0026#39;)) { text_sol \u0026lt;- paste0(\u0026#39;In order to execute the code, open a new R script in RStudio (Control+shift+N), \u0026#39;, \u0026#39;copy and paste the code, and execute it whole by pressing \u0026#39;, \u0026#39;Control+Shift+Enter or line by line with shortcut \u0026#39;, \u0026#39;Control+Enter.\u0026#39;) } else { text_sol \u0026lt;- \u0026#39;\u0026#39; } } my_str \u0026lt;- paste0(\u0026#39;\\n\\n \u0026lt;hr\u0026gt; \\n\u0026#39;, webex::total_correct(), \u0026#39;\\n\u0026#39;, \u0026#39;### Q.\u0026#39;, my_counter, \u0026#39;{-} \\n\u0026#39;, exercise_text, \u0026#39;\\n\u0026#39;, my_answers_text) temp_id \u0026lt;- basename(tempfile(pattern = \u0026#39;collapse_\u0026#39;)) sol_str \u0026lt;- str_glue( \u0026#39; \u0026lt;div style=\u0026quot;text-align: left; margin-top: 2px; padding: 13px 0 10px 0;\u0026quot;\u0026gt;\u0026lt;p\u0026gt;\u0026lt;button class=\u0026quot;btn btn-primary\u0026quot; type=\u0026quot;button\u0026quot; data-toggle=\u0026quot;collapse\u0026quot; data-target=\u0026quot;#{temp_id}\u0026quot; aria-expanded=\u0026quot;false\u0026quot; aria-controls=\u0026quot;collapseExample\u0026quot;\u0026gt; Solution \u0026lt;/button\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;collapse\u0026quot; id=\u0026quot;{temp_id}\u0026quot;\u0026gt; {text_sol} \u0026lt;div class=\u0026quot;card card-body\u0026quot;\u0026gt; {solution} \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;\u0026#39;) cat(paste0(my_str, \u0026#39;\\n\u0026#39; , sol_str)) } return(invisible(TRUE)) } Html Exercises The html output for the selected three exercises is given next. Do notice that the correct solution is not highlighted in this blog post due to the lack of css and javascript. In the final result you’ll see that it works correctly. Also, you’ll need to set results='asis' in the knitr options of the chunk (the code output pure html).\ncompile_eoc_exercises(my_exercises, type_doc = \u0026#39;html\u0026#39;)  \n Q.1 The R language was developed based on what other programming language?\nSolution:  Julia Javascript C++ S Python  Solution   The solution is S. To reach the same result, you must execute the code below. For that, open a new R script in RStudio (Control+shift+N), copy and paste the code, and execute it whole by pressing Control+Shift+Enter or line by line with shortcut Control+Enter. Straight from the book, section What is R: “R is a modern version of S, a programming language originally created in Bell Laboratories (formerly AT\u0026amp;T, now Lucent Technologies).”    \n Q.2 What are the names of the two authors of R?\nSolution:  Linus Torvalds and Richard Stallman John Chambers and Robert Engle Roger Federer and Rafael Nadal Guido van Rossum and Bjarne Stroustrup Ross Ihaka and Robert Gentleman  Solution   The solution is Ross Ihaka and Robert Gentleman. To reach the same result, you must execute the code below. For that, open a new R script in RStudio (Control+shift+N), copy and paste the code, and execute it whole by pressing Control+Shift+Enter or line by line with shortcut Control+Enter. Straight from the book: “… The base code of R was developed by two academics, Ross Ihaka and Robert Gentleman, resulting in the programming platform we have today.”.\n   \n Q.3 Why is R special when comparing to other programming languages, such as Python, C++, javascript and others?\nSolution:  Easy to use It was designed for analyzing data and producing statistical output Makes it easy to write mobile apps Quick code execution Works on any plataform such as Windows, Unix, MacOS  Solution   The solution is It was designed for analyzing data and producing statistical output. To reach the same result, you must execute the code below. For that, open a new R script in RStudio (Control+shift+N), copy and paste the code, and execute it whole by pressing Control+Shift+Enter or line by line with shortcut Control+Enter. Undoubtedly, the main differential of the R language is the ease with which data can be analyzed on the platform. Although other languages also allow data analysis, it is in R where this process is supported by a wide range of specialized packages.    Pdf/Ebook Exercises And for latex (pdf) and epub3 (ebook), the result is:\ncompile_eoc_exercises(my_exercises, type_doc = \u0026#39;latex\u0026#39;) 01 - The R language was developed based on what other programming language?\nC++ S Javascript Julia Python  02 - What are the names of the two authors of R?\nGuido van Rossum and Bjarne Stroustrup John Chambers and Robert Engle Roger Federer and Rafael Nadal Ross Ihaka and Robert Gentleman Linus Torvalds and Richard Stallman  03 - Why is R special when comparing to other programming languages, such as Python, C++, javascript and others?\nWorks on any plataform such as Windows, Unix, MacOS Easy to use Quick code execution Makes it easy to write mobile apps It was designed for analyzing data and producing statistical output  As you can see, it works great. So, at the end of each chapter I simply called function compile_eoc_exercises() with the knit chunk options results='asis' and echo=FALSE. Moreover, object my_engine is set as my_engine \u0026lt;- knitr:::pandoc_to(), which will figure out the format within the compilation of the book:\n     Conclusion Its is amazing how much we can accomplish by learning and mixing different technologies. In this case, I used R, Latex, html, javascript and css to bundle reproducible and dynamic exercises for my book. You can find examples of the final output in html, pdf and ebook.\nIf you’re trying it for you own book, make sure to add the correct .js and .css files to the html compilation. In my case, I used my_javascript.js and style_html.css.\n ","date":1616025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616025600,"objectID":"a712dc46101f9d4136a301df931b971e","permalink":"http://www.msperlin.com/blog/post/2021-03-18-bookdown_and_exams/","publishdate":"2021-03-18T00:00:00Z","relpermalink":"/blog/post/2021-03-18-bookdown_and_exams/","section":"post","summary":"It’s been three years since I’ve been using package bookdown for compiling and distributing three different books in Amazon and the web. It helped me greatly in all my book projects and I’m always grateful to Yihui Xie for providing such a useful tool at the right time.\nHowever, bookdown offers no support for chapter exercises of any sort. While you can write exercises in plain RMarkdown, it is not a good solution for a long term project such as a technical book. When writing the latest edition of Analyzing Financial and Economical Data with R, I aimed for a work cycle where the 100 plus exercises and their solutions were reproducible and easier to maintain.","tags":["R","afedR","exams","bookdown","webex"],"title":"bookdown + exams + webex","type":"post"},{"authors":null,"categories":["R","personal"],"content":"  In the 18th of march 2021 I’ll complete exactly ten years since finishing my PhD and taking a professorship position at UFRGS, south of Brazil. In this post I’ll write about what I learned during this period and, hopefully, help other academics that are just starting out.\nThis post is the summary of a talk I had with a former PhD student. I’m fully aware that academic work can be very different across countries and institutions. I don’t claim to have all the answers to all the problems. But, these are a couple of advice that would certainly help me in my starting years.\n1) Find a hobby First and foremost, find a hobby. Yes, sounds weird as a first advice but you would be surprised how a hobby – or anything that takes your mind of work – can give you a great boost in quality of life and work, specially in academia.\nA good hobby should not be related to work, or driven by its consequences. For example, I usually go to the gym twice a week. Not because I enjoy it, but due to the fact that it helps me keeping my body healthy. Honestly, I don’t like gyms, but I go anyway.\nSaid that, sports are a good example of hobbies. You do it for its own sake. So, find a sport you like and invest time in it. You’ll thank me later.\n 2) Plan for the long term One repeated mistake I made in my starting years is to enroll in many different projects. In hindsight, I can see that most of them were doomed to fail before even starting. At first, everyone is excited to join and do something new, but few remain if their motivation is not right. Moreover, students have a tendency to say yes to the teacher, which makes them to agree to projects that they might not be really interested. Everyone will stay in the project until time becomes scarce or something new shows up.\nIncentives play a key role in everything we do and I learned this the hard way. For example, if a student works 8 hours in a bank, he is not taking a weekend off to review the econometrics of that scientific paper we wrote together months ago. And there is nothing wrong with that. The mistake was from my part for 1) not making it clear of how much work a paper needs and 2) assuming the student and I would share the same incentives, which is simply not true.\nWhen playing for the long term academic game, focus on things that you are likely to continue doing in 5, 10, 15 years:\n Make a personal website (yes, many people still don’t). Without it, how will people see your work? Find a research topic that you’re passionate about; this way will be much easier to write, teach and code about the topic. Find colleagues that are also playing the long term game and work with them. Use open software in data analysis and teaching, preferably Python or R. Its smart to keep your workflow reproducible and license-free. This way you’ll be able to take your work anywhere, if necessary; Don’t ever compromise your reputation. A famous quote: “it takes years to build a reputations and a few minutes to lose it”. Learn new tools: linux, server-side scripting, building websites, editing videos, using YouTube, using e-learning platforms, and so on. Whatever it takes to deliver your work to society. Don’t let other people do it for you. You have the time, just site down and learn it.   3) Quantity beats quality One statistic that few people know is that, for a reputable, high ranking journal such as Journal of Finance, the median number of citations per article is close to zero. This means that, what drives the high impact factors of such journals is a couple of papers that become popular in their field of expertise. This means that few articles get all the glory.\nAlso know as the tail effect or Pareto rule, this is a common pattern in business. As an example, there were many search engines before Google but none with its success. The high scalability of business in digital commodities, such as search engines, makes it so that a couple of companies gets all the sales (or search queries). Likewise, if you invest in a portfolio of companies over a long period of time, your total performance would be due to a handful of companies. Those winners with the highest returns will easily compensate for losers, and still give you a handsome financial reward.\nThis is also true for your research work. If you write ten papers, be glad that one of them does well and attracts citations. This might also just be good luck, due to right timing. This means that you should focus your research in quality, but also quantity. The more papers, higher the likelihood of hitting the spot and getting recognition.\n 4) Its not all about papers, but it helps Papers have historically been the main output of academics. I certainly felt that way when starting out. Today, the university work can take many different shapes and colors. You can contribute to society not just by writing research papers but also:\n writing and publishing code writing books writing news articles writing blog posts releasing videos on YouTube local lectures  My suggestion is to try as many as possible and see which type of output you like best.\n 5) You have freedom and autonomy, use it.. Academics generally have the autonomy to define their workflow and control their environment. Use and abuse of that freedom. Very few people have that kind of autonomy and some would probably be willing to pay a good amount of money for it, if they could. With that in mind:\n Its not about the number of hours, but what you do with them. No one cares about how many hours you are at the university, but what you are producing to society, whether it is research articles, lectures or code. Change your working hours as needed. For example, if you live in a large city, why do you need to get from/to work at rush hours? Likewise, why not working on the weekend and taking Monday off? (I’ve done this countless times – its fun..) Don’t work with people you don’t like. Easier said than done, but keep in mind that you always have that choice. Learn to say no, constantly.   6) Academia is changing, fast.. And you should be worried. Historically, universities always had the prestige of being knowledge centers. In contrast, knowledge is now abundant in the digital world. My students are no longer learning by reading books, but by watching YouTube videos or using other platforms. It seems we distanced ourselves from the real world and that is starting to backfire. A reliable sign is a trend in the hiring process of companies of not requiring a college degree, but only proof of work.\nThe change is coming and we have not clue of how universities will adapt in the next 5, 10 years. The only thing you can control is how much you learn and do. So, do more and learn more. Be prepared.\n ","date":1615334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615334400,"objectID":"35c1c2b194c99eb16c71ba59c223cf0e","permalink":"http://www.msperlin.com/blog/post/2021-03-10-advices-young-academics/","publishdate":"2021-03-10T00:00:00Z","relpermalink":"/blog/post/2021-03-10-advices-young-academics/","section":"post","summary":"In the 18th of march 2021 I’ll complete exactly ten years since finishing my PhD and taking a professorship position at UFRGS, south of Brazil. In this post I’ll write about what I learned during this period and, hopefully, help other academics that are just starting out.\nThis post is the summary of a talk I had with a former PhD student. I’m fully aware that academic work can be very different across countries and institutions. I don’t claim to have all the answers to all the problems. But, these are a couple of advice that would certainly help me in my starting years.","tags":["R","personal"],"title":"Ten years as a professor -- six advices to young academics","type":"post"},{"authors":null,"categories":["R","GetDFPData2"],"content":"  GetDFPData is an academic project to provide free and unrestricted access to financial reports from B3, the brazilian exchange. Back in 2020 I split the code of GetDFDData into two distinct packages: GetDFPData2 and GetFREData. In short, I’ve found a new data source at CVM (comissão valores mobiliários) that is much easier to work than B3’s site. While the code in GetDFPData2 is becoming stable and will soon be released in CRAN, the shiny app was missing this important update.\nFinally got some free time to work on the shinny app once again. The main change is that the underlying code is fully based on GetDFPData2, i.e., the data output is exactly the same as using an R session. Previous version used cached data from GetDFPData, which meant that every year I had to recompile and feed the new data to the app. This choice was not accidental as previous version of the package took a long time to parse all xml files from B3. The new version executes data importation very quickly, allowing for a web interface in a modest server such as mine.\nThe app is hosted at link_shiny. See a peek below:\n ","date":1614988800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614988800,"objectID":"5679120e4fa00de0dd8eee8bb8fe4370","permalink":"http://www.msperlin.com/blog/post/2021-03-06-getdfpdata2-web/","publishdate":"2021-03-06T00:00:00Z","relpermalink":"/blog/post/2021-03-06-getdfpdata2-web/","section":"post","summary":"GetDFPData is an academic project to provide free and unrestricted access to financial reports from B3, the brazilian exchange. Back in 2020 I split the code of GetDFDData into two distinct packages: GetDFPData2 and GetFREData. In short, I’ve found a new data source at CVM (comissão valores mobiliários) that is much easier to work than B3’s site. While the code in GetDFPData2 is becoming stable and will soon be released in CRAN, the shiny app was missing this important update.\nFinally got some free time to work on the shinny app once again.","tags":["R","GetDFPData2"],"title":"Update on the Shiny/Web Interface of GetDFPData2","type":"post"},{"authors":null,"categories":["R","afedR"],"content":"  The revised second edition of Analylzing Financial and Economic Data with R presents more than 100 exercises at the end section of all chapters. All exercises are freely available in the exams format, meaning that any R tutor can export the same exercises to pdf, html or e-learning platforms. In this post I’ll show how to compile exercises to pdf, html, Moodle and blackboard.\nInstallation The first step is to install package afedR with devtools:\ndevtools::install_github(\u0026#39;msperlin/afedR\u0026#39;) Another requirement is a working Latex instalation. For that, use tinytex:\ntinytex::install_tinytex()  Compiling Exercises How it works? All book exercises in the exams format: each exercise is a .Rmd file containing code, exercise text and solution. The files themselves can be found in the installation directory of the package:\neoc_dir \u0026lt;- afedR::get_EOC_dir() eoc_chapters \u0026lt;- fs::dir_ls(eoc_dir) basename(eoc_chapters) ## [1] \u0026quot;Chapter01-Introduction\u0026quot; \u0026quot;Chapter02-Basic-Operations\u0026quot; ## [3] \u0026quot;Chapter03-Research-Scripts\u0026quot; \u0026quot;Chapter04-Import-Local\u0026quot; ## [5] \u0026quot;Chapter05-Import-Internet\u0026quot; \u0026quot;Chapter06-Dataframes-and-Others\u0026quot; ## [7] \u0026quot;Chapter07-Basic-Classes\u0026quot; \u0026quot;Chapter08-Programming\u0026quot; ## [9] \u0026quot;Chapter09-Cleaning-and-Structuring\u0026quot; \u0026quot;Chapter10-Figures\u0026quot; ## [11] \u0026quot;Chapter11-FinEcon\u0026quot; \u0026quot;Chapter12-Reporting\u0026quot; ## [13] \u0026quot;Chapter13-Optimizing-Code\u0026quot; Each folder will have several exercises. Let’s try one out:\neoc_files \u0026lt;- fs::dir_ls(eoc_chapters[1]) basename(eoc_files) ## [1] \u0026quot;afedR_Chap-01_01_SPLUS.Rmd\u0026quot; ## [2] \u0026quot;afedR_Chap-01_02_Authors-R.Rmd\u0026quot; ## [3] \u0026quot;afedR_Chap-01_03_About-R.Rmd\u0026quot; ## [4] \u0026quot;afedR_Chap-01_04_name-R.Rmd\u0026quot; ## [5] \u0026quot;afedR_Chap-01_05_about-R.Rmd\u0026quot; ## [6] \u0026quot;afedR_Chap-01_06_Tecnology-R.Rmd\u0026quot; ## [7] \u0026quot;afedR_Chap-01_07_rtools.Rmd\u0026quot; ## [8] \u0026quot;afedR_Chap-01_08_Groups.Rmd\u0026quot; ## [9] \u0026quot;afedR_Chap-01_09_RBloggers.Rmd\u0026quot; ## [10] \u0026quot;afedR_Chap-01_10_Infrastructure-TI.Rmd\u0026quot; We can also read one of the files to show the strucuture of the exercise in code and text:\nreadLines(eoc_files[1]) ## [1] \u0026quot;```{r datageneration, echo = FALSE, results = \\\u0026quot;hide\\\u0026quot;}\u0026quot; ## [2] \u0026quot;my_answers \u0026lt;- c(\u0026#39;S\u0026#39;, \u0026quot; ## [3] \u0026quot; \u0026#39;C++\u0026#39;,\u0026quot; ## [4] \u0026quot; \u0026#39;Python\u0026#39;,\u0026quot; ## [5] \u0026quot; \u0026#39;Julia\u0026#39;,\u0026quot; ## [6] \u0026quot; \u0026#39;Javascript\u0026#39;)\u0026quot; ## [7] \u0026quot;\u0026quot; ## [8] \u0026quot;#check_answers(my_answers)\u0026quot; ## [9] \u0026quot;```\u0026quot; ## [10] \u0026quot;\u0026quot; ## [11] \u0026quot;Question\u0026quot; ## [12] \u0026quot;========\u0026quot; ## [13] \u0026quot;\u0026quot; ## [14] \u0026quot;The R language was developed based on what other programming language?\u0026quot; ## [15] \u0026quot;\u0026quot; ## [16] \u0026quot;```{r questionlist, echo = FALSE, results = \\\u0026quot;asis\\\u0026quot;}\u0026quot; ## [17] \u0026quot;exams::answerlist(my_answers, markup = \\\u0026quot;markdown\\\u0026quot;)\u0026quot; ## [18] \u0026quot;```\u0026quot; ## [19] \u0026quot;\u0026quot; ## [20] \u0026quot;Solution\u0026quot; ## [21] \u0026quot;================\u0026quot; ## [22] \u0026quot;\u0026quot; ## [23] \u0026quot;Straight from the book, section **What is R**: \\\u0026quot;R is a modern version of S, a programming language originally created in Bell Laboratories (formerly AT\u0026amp;T, now Lucent Technologies).\\\u0026quot;\u0026quot; ## [24] \u0026quot;\u0026quot; ## [25] \u0026quot;Meta-information\u0026quot; ## [26] \u0026quot;================\u0026quot; ## [27] \u0026quot;extype: schoice\u0026quot; ## [28] \u0026quot;exsolution: `r mchoice2string(c(TRUE, FALSE, FALSE, FALSE, FALSE), single = TRUE)`\u0026quot; ## [29] \u0026quot;exname: \\\u0026quot;S PLUS\\\u0026quot;\u0026quot; ## [30] \u0026quot;exshuffle: TRUE\u0026quot; ## [31] \u0026quot;\u0026quot; Basically, we define all sections of a question – text, solution, alternatives – using a .Rmd template. Again, you can find more details about using package exams in its own website.\nBe aware that, all exams .rmd files available within afedR are self contained and you can export and compile them directly from exams. An easy way to copy all exercise files to your local folder is using function afedR::path_to_copy:\n# copy to \u0026quot;documents\u0026quot; folder afedR::copy_book_files(path_to_copy = \u0026#39;~\u0026#39;) ## Copying data files files to ~/afedR files/data ## 37 files copied ## Copying end-of-chapter (eoc) exercises with solutions to ~/afedR files/eoc-exercises/ ## 99 files copied ## Copying R code to ~/afedR files/R-code ## 15 files copied All book files – data, code and exercises – are now available at your “Documents” folder (shorcut of ~).\n Compiling to pdf For pdf compilation, you’ll need:\n name of students (will be printed in pdf) students ids (I usually use their university card number) Chapters to include Exercise name Course name  And use the following code\nlibrary(afedR) names_students \u0026lt;- c(\u0026#39;Michael Peterling\u0026#39;, \u0026#39;John Aspper\u0026#39;, \u0026#39;Mr. Beans\u0026#39;) ids_students \u0026lt;- 1:length(names_students) # probably id card? chapters \u0026lt;- 1:3 # chapters from 1 to 13 dir_output \u0026lt;- file.path(tempdir(), \u0026#39;pdf-example\u0026#39;) df_exams \u0026lt;- compile_pdf_exercises(students_names = names_students, students_ids = ids_students, chapters_to_include = chapters, dir_out = dir_output) The output of compile_pdf_exercises is a table with the correct answers for schoice and num type of questions:\nglimpse(df_exams) ## Rows: 69 ## Columns: 4 ## $ i_name \u0026lt;chr\u0026gt; \u0026quot;Michael Peterling\u0026quot;, \u0026quot;Michael Peterling\u0026quot;, \u0026quot;Michael Peterling\u0026quot;… ## $ i_ver \u0026lt;int\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ i_q \u0026lt;int\u0026gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18… ## $ solution \u0026lt;chr\u0026gt; \u0026quot;b\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;b\u0026quot;, \u0026quot;c\u0026quot;, \u0026quot;b\u0026quot;, NA, \u0026quot;a\u0026quot;, NA, NA, NA, NA, NA, NA, NA,… After compilation, all pdf files are available at folder dir_output:\nfs::dir_ls(dir_output) ## /tmp/RtmpuI4IpQ/pdf-example/Sample Exercise_Ver 01_Michael Peterling.pdf ## /tmp/RtmpuI4IpQ/pdf-example/Sample Exercise_Ver 02_John Aspper.pdf ## /tmp/RtmpuI4IpQ/pdf-example/Sample Exercise_Ver 03_Mr. Beans.pdf The final result will be as follows:\n    Compiling to html You can also compile to a html file using afedR::compile_html_exercises:\nlibrary(afedR) names_students \u0026lt;- c(\u0026#39;Michael Peterling\u0026#39;, \u0026#39;John Aspper\u0026#39;, \u0026#39;Mr. Beans\u0026#39;) ids_students \u0026lt;- 1:length(names_students) # probably id card? chapters \u0026lt;- 1:3 # chapters from 1 to 13 dir_output \u0026lt;- file.path(tempdir(), \u0026#39;html-example\u0026#39;) df_exams \u0026lt;- compile_html_exercises(students_names = names_students, students_ids = ids_students, chapters_to_include = chapters, dir_out = dir_output) ## Exams generation initialized. ## ## Output directory: /tmp/RtmpuI4IpQ/exams files file6d236c2d8ef2 ## Exercise directory: /mnt/HDD/Dropbox/05-My Website/01-msperlin.com/content/post ## Supplement directory: /tmp/RtmpuI4IpQ/file6d23249571af ## Temporary directory: /tmp/RtmpuI4IpQ/file6d2338a774fc ## Exercises: /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_01_SPLUS, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_02_Authors-R, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_03_About-R, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_04_name-R, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_05_about-R, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_06_Tecnology-R, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_07_rtools, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_08_Groups, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_09_RBloggers, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter01-Introduction/afedR_Chap-01_10_Infrastructure-TI, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_01_Basic, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_02_Basic, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_03_getwd, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_04_download, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_05_unzip, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_06_installpkgs, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_07_filespkgs, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_08_installpkgs2, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_09_installpkgs3-cranlogs, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_10_devtools, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter02-Basic-Operations/afedR_Chap-02_11_files-in-computer, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter03-Research-Scripts/afedR_Chap-03_01_pesquisa, /home/msperlin/R/x86_64-pc-linux-gnu-library/4.0/afedR/extdata/exams_files/02-EOCE-Rmd/Chapter03-Research-Scripts/afedR_Chap-03_02_folders ## ## Generation of individual exams. ## Exam 1: _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_01_SPLUS (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_02_Authors-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_03_About-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_04_name-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_05_about-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_06_Tecnology-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_07_rtools (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_08_Groups (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_09_RBloggers (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_10_Infrastructure-TI (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_01_Basic (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_02_Basic (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_03_getwd (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_04_download (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_05_unzip (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_06_installpkgs (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_07_filespkgs (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_08_installpkgs2 (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_09_installpkgs3-cranlogs (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_10_devtools (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_11_files-in-computer (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter03-Research-Scripts_afedR_Chap-03_01_pesquisa (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter03-Research-Scripts_afedR_Chap-03_02_folders (srt) ... w ... done. ## Exam 2: _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_01_SPLUS (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_02_Authors-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_03_About-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_04_name-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_05_about-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_06_Tecnology-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_07_rtools (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_08_Groups (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_09_RBloggers (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_10_Infrastructure-TI (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_01_Basic (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_02_Basic (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_03_getwd (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_04_download (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_05_unzip (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_06_installpkgs (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_07_filespkgs (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_08_installpkgs2 (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_09_installpkgs3-cranlogs (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_10_devtools (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_11_files-in-computer (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter03-Research-Scripts_afedR_Chap-03_01_pesquisa (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter03-Research-Scripts_afedR_Chap-03_02_folders (srt) ... w ... done. ## Exam 3: _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_01_SPLUS (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_02_Authors-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_03_About-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_04_name-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_05_about-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_06_Tecnology-R (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_07_rtools (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_08_Groups (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_09_RBloggers (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter01-Introduction_afedR_Chap-01_10_Infrastructure-TI (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_01_Basic (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_02_Basic (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_03_getwd (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_04_download (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_05_unzip (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_06_installpkgs (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_07_filespkgs (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_08_installpkgs2 (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_09_installpkgs3-cranlogs (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_10_devtools (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter02-Basic-Operations_afedR_Chap-02_11_files-in-computer (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter03-Research-Scripts_afedR_Chap-03_01_pesquisa (srt) _home_msperlin_R_x86_64-pc-linux-gnu-library_4.0_afedR_extdata_exams_files_02-EOCE-Rmd_Chapter03-Research-Scripts_afedR_Chap-03_02_folders (srt) ... w ... done. An example of full compiled html file is available here.\n Exporting to Moodle You can also export to e-learning platforms such as Moodle. The process is quite simple as exams package does the heavy work:\nrequire(afedR) my_eoc_dir \u0026lt;- afedR::get_EOC_dir() available_chapters \u0026lt;- fs::dir_ls(my_eoc_dir) exercise_files \u0026lt;- fs::dir_ls(available_chapters[1]) dir_output \u0026lt;- file.path(tempdir(), \u0026#39;moodle-test\u0026#39;) exams::exams2moodle(file = exercise_files, name = \u0026#39;TestingMoodle\u0026#39;, dir = dir_output) fs::dir_ls(dir_output) ## /tmp/RtmpuI4IpQ/moodle-test/TestingMoodle.xml You can later import this .xml file in your Moodle class.\n Exporting to Blackboard require(afedR) my_eoc_dir \u0026lt;- afedR::get_EOC_dir() available_chapters \u0026lt;- fs::dir_ls(my_eoc_dir) exercise_files \u0026lt;- fs::dir_ls(available_chapters[1]) dir_output \u0026lt;- file.path(tempdir(), \u0026#39;blackboard-test\u0026#39;) exams::exams2blackboard(file = exercise_files, name = \u0026#39;TestingBlackBoard\u0026#39;, dir = dir_output) fs::dir_ls(dir_output) ## /tmp/RtmpuI4IpQ/blackboard-test/TestingBlackBoard.zip This .zip file contains all exercises of chapter 01 and can be imported in your blackboard account.\n  ","date":1614470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614470400,"objectID":"4110939fcd8e268c7cac89809540ba3d","permalink":"http://www.msperlin.com/blog/post/2021-02-28-dynamic-exercises-afedr/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/blog/post/2021-02-28-dynamic-exercises-afedr/","section":"post","summary":"The revised second edition of Analylzing Financial and Economic Data with R presents more than 100 exercises at the end section of all chapters. All exercises are freely available in the exams format, meaning that any R tutor can export the same exercises to pdf, html or e-learning platforms. In this post I’ll show how to compile exercises to pdf, html, Moodle and blackboard.\nInstallation The first step is to install package afedR with devtools:\ndevtools::install_github(\u0026#39;msperlin/afedR\u0026#39;) Another requirement is a working Latex instalation. For that, use tinytex:\ntinytex::install_tinytex()  Compiling Exercises How it works?","tags":["R","afedR"],"title":"Compiling Book Exercises to pdf | html | Moodle | Blackboard","type":"post"},{"authors":null,"categories":["R","afedR"],"content":"  I recently launched the third edition of my portuguese R book (adfeR-pt-br), with many due changes from the international version (afedR-en). To make it clear, the second edition of afedR (en) was ahead in content and the third edition of adfeR (pt-br) closed that gap.\nBut, as it usually is with a time evolving platform such as R, the code in afedR-en changed with the deprecation and arrival of new functions and packages. In order to keep the content up to date, I published a revision of the book in Amazon and its web version.\nThis revision is not only for fixing broken code but also improves other important aspects of the book including ebook/html templates and end-of-chapter exercises. Here are the main changes:\n Improved and consistent css template for html and ebook. Over 100 end of chapter exercises. The exercises within the book are in the exams format and can be exported to e-learning platforms. Check out this blog post for details. To help the reader with topics that don’t quite fit with the main text, new text boxes with important and cautionary messages were implemented in all formats. New hardcover format is available at Amazon.  The R books are a lifelong project and I plan to keep improving the work as much as possible over the next years. I’m happy to see that, just like good wine, the content of the book only gets better with the passage of time.\n","date":1614470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614470400,"objectID":"7484d458106ec548ba167d2f24619e0a","permalink":"http://www.msperlin.com/blog/post/2021-02-28-afedr-revision-2021/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/blog/post/2021-02-28-afedr-revision-2021/","section":"post","summary":"I recently launched the third edition of my portuguese R book (adfeR-pt-br), with many due changes from the international version (afedR-en). To make it clear, the second edition of afedR (en) was ahead in content and the third edition of adfeR (pt-br) closed that gap.\nBut, as it usually is with a time evolving platform such as R, the code in afedR-en changed with the deprecation and arrival of new functions and packages. In order to keep the content up to date, I published a revision of the book in Amazon and its web version.","tags":["R","afedR"],"title":"Revision of book \"Analyzing Financial and Economic Data with R\"","type":"post"},{"authors":null,"categories":["R","adfeR"],"content":"  É com muito prazer que comunico o lançamento oficial da terceira edição do livro Análise de Dados Financeiros e Econômicos com o R. Encontrarás a obra na Amazon.com.br como um ebook ou livro impresso. A versão online do livro com os primeiros sete capítulos está disponível neste link. Maiores detalhes, incluindo material suplementar, encontram-se na página do livro.\nA primeira edição foi lançada em 2016 e, desde então, venho atualizando o conteúdo com novos pacotes e novos capítulos. A terceira edição contempla as seguintes mudanças:\n Todo o conteúdo do livro agora é disponibilizado via pacote adfeR – link github – facilitando muito a reprodução de todos os exemplos de código.\n Uso de caixas de textos customizadas para indicar pontos importantes e precauções que os leitores devem ter em cada seção do livro.\n Mais de 100 exercícios de final de capítulo foram criados e agora possuem gabarito em texto e código, disponível na versão web do livro. Todos os exercícios estão disponíveis no formato exams e podem ser compilados para um pdf ou então exportados para plataformas de e-learning, tal como o Moodle ou Blackboard (veja seção Conteúdo para Instrutores no Prefácio do livro). Um exemplo de compilação para pdf e Moodle está disponível neste post do blog.\n Quatro novos pacotes especializados na obtenção de dados financeiros e econômicos estão inclusos na nova edição. São estes: GetDFPData2, GetFREData, GetQuandlData e GetBCBData. Todos pacotes são estáveis, desenvolvidos por mim e serão mantidos ao longo do tempo. Assim, não corremos mais o risco de quebra de código devido a desatualização de um pacote por um autor.\n Um novo capítulo sobre Otimização de Código em R, discutindo melhorias na estrutura de código e também minimização do tempo de execução via estratégias de cacheamento local e processamento paralelo.\n Uso de template customizado para o ebook e html via CSS (Cascading Style Sheets). Agora, o livro possui, sem dúvida, uma cara própria e consistente entre os diferentes formatos.\n  Se gostou do conteúdo, considere comprar o livro e deixar um feedback na página da amazon. Sua opinião é muito importante para promover o livro e ajudar outros a aprender mais sobre o R e RStudio. Como autor independente, certamente apreciarei o gesto e tomarei como fator motivante para futuras edições do livro.\n","date":1613779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613779200,"objectID":"20c0a8dfd73befb5a6e3650d55c15482","permalink":"http://www.msperlin.com/blog/post/2021-02-20-adfer-ed3-announcement/","publishdate":"2021-02-20T00:00:00Z","relpermalink":"/blog/post/2021-02-20-adfer-ed3-announcement/","section":"post","summary":"É com muito prazer que comunico o lançamento oficial da terceira edição do livro Análise de Dados Financeiros e Econômicos com o R. Encontrarás a obra na Amazon.com.br como um ebook ou livro impresso. A versão online do livro com os primeiros sete capítulos está disponível neste link. Maiores detalhes, incluindo material suplementar, encontram-se na página do livro.\nA primeira edição foi lançada em 2016 e, desde então, venho atualizando o conteúdo com novos pacotes e novos capítulos. A terceira edição contempla as seguintes mudanças:\n Todo o conteúdo do livro agora é disponibilizado via pacote adfeR – link github – facilitando muito a reprodução de todos os exemplos de código.","tags":["R","adfeR"],"title":"LANÇAMENTO - Análise de Dados Financeiros e Econômicos com o R (Terceira Edição)","type":"post"},{"authors":null,"categories":["R","adfeR"],"content":"  A terceira edição do livro Análise de Dados Financeiros e Econômicos contém mais de 100 exercícios de final de capítulo, com todas soluções disponíveis na página do livro. Alternativamente, professores e instrutores podem compilar arquivos pdf dos exercícios para seus alunos com o pacote adfeR.\nO primeiro passo é instalar o pacote via devtools e também o exams:\ndevtools::install_github(\u0026#39;msperlin/adfeR\u0026#39;) Outro requisito é a instalação do tinytex e Latex/texlive para a compilação em pdf:\ntinytex::install_tinytex() Como funciona? Todos exercícios do livro estão no formato do pacote exams. Cada exercício é um arquivo .Rmd contendo código, narrativa do exercício e a solução. Os arquivos em si podem ser encontrados no diretório de instalação do próprio pacote:\neoc_dir \u0026lt;- adfeR::get_EOC_dir() eoc_chapters \u0026lt;- fs::dir_ls(eoc_dir) basename(eoc_chapters) ## [1] \u0026quot;Cap01-Introducao\u0026quot; \u0026quot;Cap02-Operacoes-Basicas\u0026quot; ## [3] \u0026quot;Cap03-Scripts-Pesquisa\u0026quot; \u0026quot;Cap04-Import-Local\u0026quot; ## [5] \u0026quot;Cap05-Import-Internet\u0026quot; \u0026quot;Cap06-Objetos-Armazenamento\u0026quot; ## [7] \u0026quot;Cap07-Objetos-Basicos\u0026quot; \u0026quot;Cap08-Programacao\u0026quot; ## [9] \u0026quot;Cap09-Limpeza-e-estruturacao-dados\u0026quot; \u0026quot;Cap10-Figuras\u0026quot; ## [11] \u0026quot;Cap11-Modelagem\u0026quot; \u0026quot;Cap12-Reportando\u0026quot; ## [13] \u0026quot;Cap13-Otimizacao\u0026quot; Cada um dos diretórios acima possui diversos exercícios. Veja o caso do capítulo 01:\nbasename(fs::dir_ls(eoc_chapters[1])) ## [1] \u0026quot;adfeR_Cap-01_01_SPLUS.Rmd\u0026quot; ## [2] \u0026quot;adfeR_Cap-01_02_Autores-R.Rmd\u0026quot; ## [3] \u0026quot;adfeR_Cap-01_03_diferencial-R.Rmd\u0026quot; ## [4] \u0026quot;adfeR_Cap-01_04_nome-R.Rmd\u0026quot; ## [5] \u0026quot;adfeR_Cap-01_05_sobre-R.Rmd\u0026quot; ## [6] \u0026quot;adfeR_Cap-01_06_Tecnologias-R.Rmd\u0026quot; ## [7] \u0026quot;adfeR_Cap-01_07_rtools.Rmd\u0026quot; ## [8] \u0026quot;adfeR_Cap-01_08_Grupos.Rmd\u0026quot; ## [9] \u0026quot;adfeR_Cap-01_09_RBloggers.Rmd\u0026quot; ## [10] \u0026quot;adfeR_Cap-01_10_Infraestrutura-TI.Rmd\u0026quot;  Exportando para pdf Para compilar para pdf os exercícios basta selecionar:\n nome dos estudantes id dos estudantes capítulos para incluir  e usar o código abaixo:\nlibrary(adfeR) ## Loading required package: dplyr ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union names_students \u0026lt;- c(\u0026#39;Marcelo\u0026#39;, \u0026#39;Ricardo\u0026#39;, \u0026#39;Tarcizio\u0026#39;) ids_students \u0026lt;- 1:length(names_students) # probably id card? chapters \u0026lt;- 1:3 # chapters from 1 to 13 dir_output \u0026lt;- file.path(tempdir(), \u0026#39;ExamsFiles\u0026#39;) l_exams \u0026lt;- build_exercises(students_names = names_students, students_ids = ids_students, chapters_to_include = chapters, dir_out = dir_output) ## Building exercise for Marcelo ## Adding content to tex ## Building pdf ## Loading required namespace: tinytex ## Copying final pdf ## Building exercise for Ricardo ## Adding content to tex ## Building pdf ## Copying final pdf ## Building exercise for Tarcizio ## Adding content to tex ## Building pdf ## Copying final pdf ## All exam files are available at folder \u0026quot;/tmp/RtmpboHyLI/ExamsFiles\u0026quot;. Todos os arquivos pdfs estarão disponíveis na pasta dir_output:\nfs::dir_ls(dir_output) ## /tmp/RtmpboHyLI/ExamsFiles/Exercicios Teste_Ver 01_Marcelo.pdf ## /tmp/RtmpboHyLI/ExamsFiles/Exercicios Teste_Ver 02_Ricardo.pdf ## /tmp/RtmpboHyLI/ExamsFiles/Exercicios Teste_Ver 03_Tarcizio.pdf O resultado final será conforme figura abaixo:\n    Exportando para o Moodle Para quem usa o Moodle ou outra plataforma de e-learning (Blackboard, Canvas, etc), a exportação para estes formatos é bem simples, basta indicar os arquivos de exercícios e usar as funções do pacote exams:\nrequire(adfeR) my_eoc_dir \u0026lt;- adfeR::get_EOC_dir() available_chapters \u0026lt;- fs::dir_ls(my_eoc_dir) exercise_files \u0026lt;- fs::dir_ls(available_chapters[1]) dir_output \u0026lt;- file.path(tempdir(), \u0026#39;moodle-test\u0026#39;) exams::exams2moodle(file = exercise_files, name = \u0026#39;TestingMoodle\u0026#39;, dir = dir_output) fs::dir_ls(dir_output) ## /tmp/RtmpboHyLI/moodle-test/TestingMoodle.xml Este arquivo .xml conterá todos os exercícios selecionados e pode ser facilmente importado no Moodle para ser aplicado aos alunos.\n ","date":1613606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613606400,"objectID":"7f42418e3f12a3434b6b0840fb127e55","permalink":"http://www.msperlin.com/blog/post/2021-02-18-dynamic-exercises-adfer/","publishdate":"2021-02-18T00:00:00Z","relpermalink":"/blog/post/2021-02-18-dynamic-exercises-adfer/","section":"post","summary":"A terceira edição do livro Análise de Dados Financeiros e Econômicos contém mais de 100 exercícios de final de capítulo, com todas soluções disponíveis na página do livro. Alternativamente, professores e instrutores podem compilar arquivos pdf dos exercícios para seus alunos com o pacote adfeR.\nO primeiro passo é instalar o pacote via devtools e também o exams:\ndevtools::install_github(\u0026#39;msperlin/adfeR\u0026#39;) Outro requisito é a instalação do tinytex e Latex/texlive para a compilação em pdf:\ntinytex::install_tinytex() Como funciona? Todos exercícios do livro estão no formato do pacote exams. Cada exercício é um arquivo .","tags":["R","adfeR"],"title":"Compilando Exercícios do Livro para pdf | html | Moodle | Blackboard","type":"post"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"Descrição Este livro introduz o leitor ao uso do R e RStudio como plataforma de análise de dados financeiros e econômicos. O livro apresenta toda a base de conhecimento necessária para utilizar o R, desde a sua instalação até a criação de códigos de pesquisa. O livro está organizado com exemplos práticos de uso do código que contextualizam e facilitam o aprendizado em cada etapa do processo.\nOnde Comprar Podes comprar o livro na Amazon no formato ebook, paperback (impresso capa comum) e hardcover (impresso capa dura). A versão online do livro, com os primeiros sete capítulos, está disponível em https://www.msperlin.com/adfeR/.\nCitação Podes citar o livro como:\nPerlin, M. S. Análise de Dados Financeiros com o R. Terceira Edição, Porto Alegre: Marcelo S. Perlin (publicação independente), 2021.\nMaterial Suplementar  01-Soluções de Exercícios\n 02-Dados e Código\n 03-Sumário\n 04-Grupo do Facebook - Para atualizações e solucionar dúvidas sobre o conteúdo do livro.\nLinks do livro R and RStudio:\n https://www.r-project.org/ | https://www.rstudio.com\nCRAN:\n R Views for Finance | R Consortium | R Manual | CRAN Policies\nRMarkdown:\n RMarkdown Tutorial\nCriação de Pacotes:\n Hadleys Guide for creating packages\nEstilo do código em R:\n Google R style guide | Tidyverse style guide\n","date":1613347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613347200,"objectID":"e18bf74280805a843f63446a4cddc9ff","permalink":"http://www.msperlin.com/blog/publication/2021_book-adfer-pt/","publishdate":"2021-02-15T00:00:00Z","relpermalink":"/blog/publication/2021_book-adfer-pt/","section":"publication","summary":"Descrição Este livro introduz o leitor ao uso do R e RStudio como plataforma de análise de dados financeiros e econômicos. O livro apresenta toda a base de conhecimento necessária para utilizar o R, desde a sua instalação até a criação de códigos de pesquisa. O livro está organizado com exemplos práticos de uso do código que contextualizam e facilitam o aprendizado em cada etapa do processo.\nOnde Comprar Podes comprar o livro na Amazon no formato ebook, paperback (impresso capa comum) e hardcover (impresso capa dura). A versão online do livro, com os primeiros sete capítulos, está disponível em https://www.","tags":null,"title":"Análise de Dados Financeiros e Econômicos com o R (Terceira Edição)","type":"publication"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":" ","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"e2fc0ff544d818dd9c6d0f5a72dc6c6e","permalink":"http://www.msperlin.com/blog/talk/2021-02-01-garch-rac/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/blog/talk/2021-02-01-garch-rac/","section":"talk","summary":"Palestra sobre artigo A GARCH Tutorial with R, publicado na RAC.","tags":[],"title":"A GARCH Tutorial with R","type":"talk"},{"authors":null,"categories":["R","personal"],"content":"  Wow, what a long year! The pandemic affected everyone, changing the way we live and relate to one another. This was a year full of lessons and we must be thankful and be able to appreciate life even more. Events such as these show how little our “problems” are when put into perspective.\nI’m lucky that, despite the lockdown, I was able to work from home this year. Lets have a look at the highlights.\nHighlights of 2020 Academic Papers This year I published and co-authored two academic papers:\n A Garch Tutorial in R - RAC Does algorithmic trading harm liquidity? Evidence from Brazil - NAJEF  I also worked towards my paper pipeline in corporate finance, with the following papers, which are currently under review:\n Financial Performance and Academic Titles of Advisers and Directors of Brazilian Companies Boards and Gender for Brazilian Listed Companies Description of Boards in Brazil   Books The second edition of my R book - Analyzing Financial and Economic Data with R was published in January 2020. I spend most of 2019 working towards this edition and I’m very pleased with the result. The book improved a lot, with a more polished content.\nI’m also happy to report that my two other books, padfR and pirf, are receiving great reviews in Amazon.com.br. This is great news, motivating the future editions.\n R Packages This year I “broke” package GetDFPData into two: GetDFPData2 and GetFREData. Both are still in Github but I’ll soon release it to CRAN. You can read more about this decision here.\nAdditionally, I also wrote the book package afedR, which facilitates the reproducibility of all book examples. See more details about it here.\n  Checking 2019’s plans At the end of 2019, my plans for 2020 were:\n Publish afedR (analyzing financial and economic data with R) Done! :)  Finish board papers Almost Done. All papers are now in review and, hopefully, are going to be published soon.  Start “personal finance project” Not done. This is an idea that I’m struggling with. I’m not really sure how to implement it and still have to figure out the details.    Plans for 2021  Publish new R/ggplot2 book (Visualização de Dados com R) This book will be mostly about using ggplot2 in data visualization. It will be written in portuguese and, if it comes out right and do well, I’ll translate into english. I already got 2 chapters written, but still missing a lot of work and content.  Publish third edition of Análise de Dados Financeiros e Econômicos com o R The portuguese version of my R book is already in its third edition and I’m very excited for this new edition. It will incorporate many changes from the english edition and more!  Publish board papers Soon. The three papers are in revision.  Publish paper about academic inbreeding I’m writing a new paper about the effects of academic inbreeding in Brazil using Lattes data. The paper is its final internal review and will soon be sent to for its formal review. The results are very interesting and I hope to write more about it soon.    ","date":1608595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608595200,"objectID":"e11e26c018c9ce8497b1f4edd3246e29","permalink":"http://www.msperlin.com/blog/post/2020-12-22-looking-back-2020/","publishdate":"2020-12-22T00:00:00Z","relpermalink":"/blog/post/2020-12-22-looking-back-2020/","section":"post","summary":"Wow, what a long year! The pandemic affected everyone, changing the way we live and relate to one another. This was a year full of lessons and we must be thankful and be able to appreciate life even more. Events such as these show how little our “problems” are when put into perspective.\nI’m lucky that, despite the lockdown, I was able to work from home this year. Lets have a look at the highlights.\nHighlights of 2020 Academic Papers This year I published and co-authored two academic papers:","tags":["R","looking back"],"title":"Looking back at 2020 and plans for 2021","type":"post"},{"authors":["Marcelo S. Perlin","Mauro Mastella","Daniel Vancin","Henrique P. Ramos"],"categories":null,"content":"","date":1595203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595203200,"objectID":"5952ed5b714ac353f5e25df60bc2b152","permalink":"http://www.msperlin.com/blog/publication/2020_rac-garch/","publishdate":"2020-07-20T00:00:00Z","relpermalink":"/blog/publication/2020_rac-garch/","section":"publication","summary":"In this tutorial paper we will address the topic of volatility modeling in R. We will discuss the underlying logic of GARCH models, their representation and estimation process, along with a descriptive example of a real-world application of volatility modelling: we use a GARCH model to investigate how much time it will take, after the latest crisis, for the Ibovespa index to reach its historical peak once again. The empirical data covers the period between years 2000 and 2020, including the 2009 financial crisis and the current 2020’s episode of the COVID-19 pandemia. We find that, according to our GARCH model, Ibovespa is likely to reach its peak once again in 2,5 years. All data and R code used to produce this tutorial are freely available on the internet and all its results can be easily replicated. ","tags":[],"title":"A GARCH Tutorial with R","type":"publication"},{"authors":null,"categories":["R","GetFREData","GetDFPData2"],"content":" Back in 2017 I wrote the first version of package GetDFPData, along with a paper describing the code and providing an empirical application.\nHowever, maintaining the package over the years has been frustrating. The code is becoming increasingly complex, much due to the fact that it handles FRE and DFP data in a single package. Execution speed for large scale importation – many years and many companies – is not reasonable. In top of that, B3’s website is unstable as a source of data and it seems it will stay like that for a long time.\nAdditionally, back in april 2020 (see this blog post), I started to work with CVM data from its ftp site. The experience has been great. The data is solid, matching all B3’s numbers, with easy and fast access. For example, I can download 10 years of financial data for all available companies in less than 10 minutes.\nAfter some considerable thought, I’m convinced that is much easier to maintain two separate packages, instead of combining both in a single module such as in GetDFPData. With that, I’m releasing two new packages: GetDFPData2, and GetFREData.\nThe first, GetDFPData2 (previously called GetCVMData), is a backwards incompatible version of GetDFPData, using the CVM ftp site as it source and focusing in one purpose: downloading annual and quarterly financial reports. The second, GetFREData only imports corporate data from the FRE system.\nInstallation # not in CRAN, install from github devtools::install_github(\u0026#39;msperlin/GetDFPData2\u0026#39;) # not in CRAN, install from github devtools::install_github(\u0026#39;msperlin/GetFREData\u0026#39;)  Examples of Usage GetDFPData2 library(GetDFPData2) library(tidyverse) # information about companies df_info \u0026lt;- get_info_companies(tempdir()) df_info  ## # A tibble: 2,530 × 47 ## CNPJ DENOM_SOCIAL DENOM_COMERC DT_REG DT_CONST DT_CANCEL ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;date\u0026gt; \u0026lt;date\u0026gt; \u0026lt;date\u0026gt; ## 1 08.773.135/0001-00 2W ENERGIA … \u0026lt;NA\u0026gt; 2020-10-29 2007-03-23 NA ## 2 11.396.633/0001-87 3A COMPANHI… TRIPLO A C… 2010-03-08 2009-11-03 2015-12-18 ## 3 12.091.809/0001-55 3R PETROLEU… \u0026lt;NA\u0026gt; 2020-11-09 2010-06-08 NA ## 4 01.547.749/0001-16 521 PARTICI… 521 PARTICI… 1997-07-11 1996-07-30 NA ## 5 01.851.771/0001-55 524 PARTICI… 524 PARTICI… 1997-05-30 1997-04-02 NA ## 6 01.919.008/0001-19 525 PARTICI… 525 PARTICI… 1997-07-16 1997-04-02 2006-05-30 ## 7 92.659.614/0001-06 A J RENNER … A J RENNER 1969-06-24 NA 1998-06-17 ## 8 02.288.752/0001-25 A.P. PARTIC… A.P. PARTIC… 1998-01-21 1997-12-14 2004-12-23 ## 9 21.649.280/0001-33 ABC DADOS E… ABC COMPUTA… 1988-06-03 NA 1993-03-05 ## 10 02.258.274/0001-00 ABC SUPERME… ABC SUPERME… 1998-02-27 1997-09-30 2001-12-17 ## # … with 2,520 more rows, and 41 more variables: MOTIVO_CANCEL \u0026lt;chr\u0026gt;, ## # SIT_REG \u0026lt;chr\u0026gt;, DT_INI_SIT \u0026lt;date\u0026gt;, CD_CVM \u0026lt;dbl\u0026gt;, SETOR_ATIV \u0026lt;chr\u0026gt;, ## # TP_MERC \u0026lt;chr\u0026gt;, CATEG_REG \u0026lt;chr\u0026gt;, DT_INI_CATEG \u0026lt;date\u0026gt;, SIT_EMISSOR \u0026lt;chr\u0026gt;, ## # DT_INI_SIT_EMISSOR \u0026lt;date\u0026gt;, CONTROLE_ACIONARIO \u0026lt;chr\u0026gt;, TP_ENDER \u0026lt;chr\u0026gt;, ## # LOGRADOURO \u0026lt;chr\u0026gt;, COMPL \u0026lt;chr\u0026gt;, BAIRRO \u0026lt;chr\u0026gt;, MUN \u0026lt;chr\u0026gt;, UF \u0026lt;chr\u0026gt;, ## # PAIS \u0026lt;chr\u0026gt;, CEP \u0026lt;dbl\u0026gt;, DDD_TEL \u0026lt;chr\u0026gt;, TEL \u0026lt;dbl\u0026gt;, DDD_FAX \u0026lt;chr\u0026gt;, FAX \u0026lt;dbl\u0026gt;, ## # EMAIL \u0026lt;chr\u0026gt;, TP_RESP \u0026lt;chr\u0026gt;, RESP \u0026lt;chr\u0026gt;, DT_INI_RESP \u0026lt;date\u0026gt;, … search_company(\u0026#39;grendene\u0026#39;) # downloading DFP data l_dfp \u0026lt;- get_dfp_data(companies_cvm_codes = 19615, use_memoise = FALSE, clean_data = TRUE, type_docs = c(\u0026#39;DRE\u0026#39;), type_format = \u0026#39;con\u0026#39;, first_year = 2010, last_year = 2020) glimpse(l_dfp) ## List of 1 ## $ DF Consolidado - Demonstração do Resultado: tibble [340 × 16] (S3: tbl_df/tbl/data.frame) ## ..$ CNPJ_CIA : chr [1:340] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ CD_CVM : num [1:340] 19615 19615 19615 19615 19615 ... ## ..$ DT_REFER : Date[1:340], format: \u0026quot;2010-12-31\u0026quot; \u0026quot;2010-12-31\u0026quot; ... ## ..$ DT_INI_EXERC: Date[1:340], format: \u0026quot;2010-01-01\u0026quot; \u0026quot;2010-01-01\u0026quot; ... ## ..$ DT_FIM_EXERC: Date[1:340], format: \u0026quot;2010-12-31\u0026quot; \u0026quot;2010-12-31\u0026quot; ... ## ..$ DENOM_CIA : chr [1:340] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ VERSAO : num [1:340] 2 2 2 2 2 2 2 2 2 2 ... ## ..$ GRUPO_DFP : chr [1:340] \u0026quot;DF Consolidado - Demonstração do Resultado\u0026quot; \u0026quot;DF Consolidado - Demonstração do Resultado\u0026quot; \u0026quot;DF Consolidado - Demonstração do Resultado\u0026quot; \u0026quot;DF Consolidado - Demonstração do Resultado\u0026quot; ... ## ..$ MOEDA : chr [1:340] \u0026quot;REAL\u0026quot; \u0026quot;REAL\u0026quot; \u0026quot;REAL\u0026quot; \u0026quot;REAL\u0026quot; ... ## ..$ ESCALA_MOEDA: chr [1:340] \u0026quot;MIL\u0026quot; \u0026quot;MIL\u0026quot; \u0026quot;MIL\u0026quot; \u0026quot;MIL\u0026quot; ... ## ..$ ORDEM_EXERC : chr [1:340] \u0026quot;ÚLTIMO\u0026quot; \u0026quot;ÚLTIMO\u0026quot; \u0026quot;ÚLTIMO\u0026quot; \u0026quot;ÚLTIMO\u0026quot; ... ## ..$ CD_CONTA : chr [1:340] \u0026quot;3.01\u0026quot; \u0026quot;3.02\u0026quot; \u0026quot;3.03\u0026quot; \u0026quot;3.04\u0026quot; ... ## ..$ DS_CONTA : chr [1:340] \u0026quot;Receita de Venda de Bens e/ou Serviços\u0026quot; \u0026quot;Custo dos Bens e/ou Serviços Vendidos\u0026quot; \u0026quot;Resultado Bruto\u0026quot; \u0026quot;Despesas/Receitas Operacionais\u0026quot; ... ## ..$ VL_CONTA : num [1:340] 1604507 -953261 651246 -442833 -377010 ... ## ..$ COLUNA_DF : logi [1:340] NA NA NA NA NA NA ... ## ..$ source_file : chr [1:340] \u0026quot;dfp_cia_aberta_DRE_con_2010.csv\u0026quot; \u0026quot;dfp_cia_aberta_DRE_con_2010.csv\u0026quot; \u0026quot;dfp_cia_aberta_DRE_con_2010.csv\u0026quot; \u0026quot;dfp_cia_aberta_DRE_con_2010.csv\u0026quot; ... dre_annual \u0026lt;- l_dfp$`DF Consolidado - Demonstração do Resultado` p \u0026lt;- ggplot(dre_annual %\u0026gt;% filter(DS_CONTA == \u0026#39;Lucro/Prejuízo Consolidado do Período\u0026#39;), aes(x = DT_REFER, y = VL_CONTA)) + geom_col() + facet_wrap(~DENOM_CIA, scales = \u0026#39;free\u0026#39;) + theme_bw() p  GetFREData library(GetFREData) library(tidyverse) l_fre \u0026lt;- get_fre_data(companies_cvm_codes = 19615, fre_to_read = \u0026#39;last\u0026#39;, first_year = 2018, last_year = 2020) glimpse(l_fre) ## List of 21 ## $ df_stockholders :\u0026#39;data.frame\u0026#39;: 30 obs. of 18 variables: ## ..$ CNPJ_CIA : chr [1:30] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:30] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:30], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:30] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:30] 83396 83396 83396 83396 83396 ... ## ..$ VERSAO : num [1:30] 16 16 16 16 16 16 16 16 16 16 ... ## ..$ type.register : chr [1:30] \u0026quot;Acionista\u0026quot; \u0026quot;Acionista\u0026quot; \u0026quot;Acionista\u0026quot; \u0026quot;Acionista\u0026quot; ... ## ..$ id.person : chr [1:30] \u0026quot;37071813833 \u0026quot; \u0026quot;09867597087 \u0026quot; \u0026quot;09864784072 \u0026quot; \u0026quot;68595743053 \u0026quot; ... ## ..$ id.nationality : chr [1:30] \u0026quot;Brasileira\u0026quot; \u0026quot;Brasileiro\u0026quot; \u0026quot;Brasileiro\u0026quot; \u0026quot;Brasileiro\u0026quot; ... ## ..$ id.state : chr [1:30] \u0026quot;São Paulo\u0026quot; \u0026quot;Rio Grande do Sul\u0026quot; \u0026quot;Rio Grande do Sul\u0026quot; \u0026quot;Rio Grande do Sul\u0026quot; ... ## ..$ id.country : logi [1:30] NA NA NA NA NA NA ... ## ..$ name.stockholder : chr [1:30] \u0026quot;Gabriella de Camargo Bartelle\u0026quot; \u0026quot;Alexandre Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Pedro Bartelle\u0026quot; ... ## ..$ type.stockholder : chr [1:30] \u0026quot;Fisica\u0026quot; \u0026quot;Fisica\u0026quot; \u0026quot;Fisica\u0026quot; \u0026quot;Fisica\u0026quot; ... ## ..$ qtd.ord.shares : chr [1:30] \u0026quot;28912677\u0026quot; \u0026quot;371651807\u0026quot; \u0026quot;125312376\u0026quot; \u0026quot;36465597\u0026quot; ... ## ..$ perc.ord.shares : chr [1:30] \u0026quot;3.200000\u0026quot; \u0026quot;41.200000\u0026quot; \u0026quot;13.890000\u0026quot; \u0026quot;4.040000\u0026quot; ... ## ..$ qtd.pref.shares : chr [1:30] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; ... ## ..$ perc.pref.shares : chr [1:30] \u0026quot;0.000000\u0026quot; \u0026quot;0.000000\u0026quot; \u0026quot;0.000000\u0026quot; \u0026quot;0.000000\u0026quot; ... ## ..$ controlling.stockholder: logi [1:30] TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ df_capital :\u0026#39;data.frame\u0026#39;: 6 obs. of 9 variables: ## ..$ CNPJ_CIA : chr [1:6] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:6] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:6], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:6] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:6] 83396 83396 94693 94693 103471 ... ## ..$ VERSAO : num [1:6] 16 16 18 18 11 11 ## ..$ stock.type : chr [1:6] \u0026quot;ON\u0026quot; \u0026quot;PN\u0026quot; \u0026quot;ON\u0026quot; \u0026quot;PN\u0026quot; ... ## ..$ stock.class: chr [1:6] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; ... ## ..$ qtd.issued : num [1:6] 9.02e+08 0.00 9.02e+08 0.00 9.02e+08 ... ## $ df_stock_values :\u0026#39;data.frame\u0026#39;: 6 obs. of 13 variables: ## ..$ CNPJ_CIA : chr [1:6] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:6] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:6], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:6] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:6] 83396 83396 94693 94693 103471 ... ## ..$ VERSAO : num [1:6] 16 16 18 18 11 11 ## ..$ stock.class : chr [1:6] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; ... ## ..$ stock.type : chr [1:6] \u0026quot;ON\u0026quot; \u0026quot;PN\u0026quot; \u0026quot;ON\u0026quot; \u0026quot;PN\u0026quot; ... ## ..$ max.price : num [1:6] 28.8 0 8.24 0 12.65 ... ## ..$ min.price : num [1:6] 25.32 0 6.59 0 7.94 ... ## ..$ avg.price : num [1:6] 26.7 0 7.6 0 10.5 ... ## ..$ flag.missing.avg.price: logi [1:6] FALSE NA FALSE NA FALSE NA ## ..$ qtd.issued : num [1:6] 9.02e+08 0.00 9.02e+08 0.00 9.02e+08 ... ## $ df_mkt_value :\u0026#39;data.frame\u0026#39;: 3 obs. of 9 variables: ## ..$ CNPJ_CIA : chr [1:3] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:3] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:3], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2019-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:3] 19615 19615 19615 ## ..$ ID_DOC : num [1:3] 83396 94693 103471 ## ..$ VERSAO : num [1:3] 16 18 11 ## ..$ mkt.avg.value: num [1:3] 2.41e+10 6.86e+09 9.44e+09 ## ..$ mkt.min.value: num [1:3] 2.28e+10 5.95e+09 7.16e+09 ## ..$ mkt.max.value: num [1:3] 2.60e+10 7.43e+09 1.14e+10 ## $ df_increase_capital :\u0026#39;data.frame\u0026#39;: 0 obs. of 0 variables ## $ df_capital_reduction :\u0026#39;data.frame\u0026#39;: 0 obs. of 0 variables ## $ df_compensation :\u0026#39;data.frame\u0026#39;: 3 obs. of 22 variables: ## ..$ CNPJ_CIA : chr [1:3] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:3] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:3], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2019-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:3] 19615 19615 19615 ## ..$ ID_DOC : num [1:3] 83396 94693 103471 ## ..$ VERSAO : num [1:3] 16 18 11 ## ..$ level.remuneration : chr [1:3] \u0026quot;Management Council\u0026quot; \u0026quot;Management Council\u0026quot; \u0026quot;Management Council\u0026quot; ## ..$ qtd.members : num [1:3] 6 6 6 ## ..$ qtd.remunerated.members : num [1:3] 6 6 6 ## ..$ total.value.remuneration : num [1:3] 1116000 1146000 1188000 ## ..$ fixed.salary : num [1:3] 1116000 1146000 1188000 ## ..$ fixed.benefits : num [1:3] 0 0 0 ## ..$ fixed.participations : num [1:3] 0 0 0 ## ..$ fixed.others : num [1:3] 0 0 0 ## ..$ variable.bonus : num [1:3] 0 0 0 ## ..$ variable.results.participation : num [1:3] 0 0 0 ## ..$ variable.meetings.participation : num [1:3] 0 0 0 ## ..$ variable.commissions.participation: num [1:3] 0 0 0 ## ..$ variable.others : num [1:3] 0 0 0 ## ..$ post.job.compensation : num [1:3] 0 0 0 ## ..$ ceasing.job.compensation : num [1:3] 0 0 0 ## ..$ stocks.options.benefits : num [1:3] 0 0 0 ## $ df_compensation_summary :\u0026#39;data.frame\u0026#39;: 9 obs. of 13 variables: ## ..$ CNPJ_CIA : chr [1:9] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:9] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:9], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:9] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:9] 83396 83396 83396 94693 94693 ... ## ..$ VERSAO : num [1:9] 16 16 16 18 18 18 11 11 11 ## ..$ level.remuneration : chr [1:9] \u0026quot;Management Council\u0026quot; \u0026quot;Statutory Directors\u0026quot; \u0026quot;Fiscal Council\u0026quot; \u0026quot;Management Council\u0026quot; ... ## ..$ qtd.members : num [1:9] 6 3 3 6 3 3 6 3 3 ## ..$ qtd.remunerated.members: num [1:9] 6 3 3 6 3 3 6 3 3 ## ..$ max.remuneration : num [1:9] 186000 2748408 142500 191000 2876477 ... ## ..$ mean.remuneration : num [1:9] 186000 2050482 142500 191000 2134281 ... ## ..$ min.remuneration : num [1:9] 186000 1423997 142500 191000 1468768 ... ## ..$ observations : logi [1:9] NA NA NA NA NA NA ... ## $ df_transactions_related :\u0026#39;data.frame\u0026#39;: 127 obs. of 17 variables: ## ..$ CNPJ_CIA : chr [1:127] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:127] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:127], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:127] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:127] 83396 83396 83396 83396 83396 ... ## ..$ VERSAO : num [1:127] 16 16 16 16 16 16 16 16 16 16 ... ## ..$ id.transaction : chr [1:127] \u0026quot;2403\u0026quot; \u0026quot;2404\u0026quot; \u0026quot;2405\u0026quot; \u0026quot;2406\u0026quot; ... ## ..$ name.related.part : chr [1:127] \u0026quot;Mailson da Nóbrega Consultoria S/C Ltda\u0026quot; \u0026quot;Vulcabras|Azaleia Argentina S.A.\u0026quot; \u0026quot;Mailson da Nóbrega Consultoria S/C Ltda\u0026quot; \u0026quot;Grendene USA, Inc\u0026quot; ... ## ..$ date.transaction : Date[1:127], format: \u0026quot;2016-12-31\u0026quot; \u0026quot;2015-12-31\u0026quot; ... ## ..$ description.related.part : chr [1:127] \u0026quot;Empresa pertencente a membro do Conselho de Administração\u0026quot; \u0026quot;Empresa controlada por acionista da Grendene S.A.\u0026quot; \u0026quot;Empresa pertencente a membro do Conselho de Administração\u0026quot; \u0026quot;Empresa controlada\u0026quot; ... ## ..$ description.transaction : chr [1:127] \u0026quot;Assessoria na área econômica financeira\u0026quot; \u0026quot;Cliente - venda de insumos\u0026quot; \u0026quot;Assessoria na área econômica financeira\u0026quot; \u0026quot;Cliente - venda de calçados para abastecimento do mercado onde a mesma está sediada\u0026quot; ... ## ..$ value.transaction : chr [1:127] \u0026quot;72000.00\u0026quot; \u0026quot;306000.00\u0026quot; \u0026quot;72000.00\u0026quot; \u0026quot;14641000.00\u0026quot; ... ## ..$ description.guarantees : chr [1:127] \u0026quot;Não aplicável\u0026quot; \u0026quot;Não aplicável\u0026quot; \u0026quot;Não aplicável\u0026quot; \u0026quot;Não aplicável\u0026quot; ... ## ..$ description.transaction.period: chr [1:127] \u0026quot;Prazo indeterminado\u0026quot; \u0026quot;Prazo indeterminado\u0026quot; \u0026quot;Prazo indeterminado\u0026quot; \u0026quot;Prazo indeterminado\u0026quot; ... ## ..$ description.rescision : chr [1:127] \u0026quot;Encerramento das atividades\u0026quot; \u0026quot;Encerramento das atividades\u0026quot; \u0026quot;Encerramento das atividades\u0026quot; \u0026quot;Encerramento das atividades\u0026quot; ... ## ..$ interest.rate : num [1:127] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ value.balance : chr [1:127] \u0026quot;R$0,00\u0026quot; \u0026quot;R$0,00\u0026quot; \u0026quot;R$0,00\u0026quot; \u0026quot;R$9.311.000,00\u0026quot; ... ## $ df_other_events :\u0026#39;data.frame\u0026#39;: 3 obs. of 12 variables: ## ..$ CNPJ_CIA : chr [1:3] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:3] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:3], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2019-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:3] 19615 19615 19615 ## ..$ ID_DOC : num [1:3] 83396 94693 103471 ## ..$ VERSAO : num [1:3] 16 18 11 ## ..$ approval.date : Date[1:3], format: \u0026quot;2018-04-23\u0026quot; \u0026quot;2018-04-23\u0026quot; ... ## ..$ type.event : chr [1:3] \u0026quot;Desdobramento\u0026quot; \u0026quot;Desdobramento\u0026quot; \u0026quot;Desdobramento\u0026quot; ## ..$ qtd.ord.shares.before : num [1:3] 3.01e+08 3.01e+08 3.01e+08 ## ..$ qtd.ord.shares.after : num [1:3] 9.02e+08 9.02e+08 9.02e+08 ## ..$ qtd.pref.shares.before: num [1:3] 0 0 0 ## ..$ qtd.pref.shares.after : num [1:3] 0 0 0 ## $ df_stock_repurchases :\u0026#39;data.frame\u0026#39;: 12 obs. of 16 variables: ## ..$ CNPJ_CIA : chr [1:12] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:12] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:12], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:12] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:12] 83396 83396 83396 83396 94693 ... ## ..$ VERSAO : num [1:12] 16 16 16 16 18 18 18 18 11 11 ... ## ..$ date.decision : Date[1:12], format: \u0026quot;2017-07-27\u0026quot; \u0026quot;2016-02-25\u0026quot; ... ## ..$ date.start.repurchase : Date[1:12], format: \u0026quot;2017-08-25\u0026quot; \u0026quot;2016-02-26\u0026quot; ... ## ..$ date.end.repurchase : Date[1:12], format: \u0026quot;2019-02-21\u0026quot; \u0026quot;2017-08-24\u0026quot; ... ## ..$ available.capital.repurchase : num [1:12] 14563536 16117227 17000000 19072706 29188481 ... ## ..$ type.stock : chr [1:12] \u0026quot;Ordinária\u0026quot; \u0026quot;Ordinária\u0026quot; \u0026quot;Ordinária\u0026quot; \u0026quot;Ordinária\u0026quot; ... ## ..$ qtd.stocks.repurchased : num [1:12] 1312343 547841 0 487096 130000 ... ## ..$ qtd.stocks.predicted : num [1:12] 2.0e+06 1.5e+06 1.5e+06 1.5e+06 6.0e+06 2.0e+06 1.5e+06 1.5e+06 2.5e+07 6.0e+06 ... ## ..$ average.price : num [1:12] 26.78 17.96 0 14.38 7.36 ... ## ..$ percent.stock.float.purchased: num [1:12] 65.6 36.5 0 32.47 2.17 ... ## ..$ percent.stock.float.predicted: chr [1:12] \u0026quot;2.380000\u0026quot; \u0026quot;1.820000\u0026quot; \u0026quot;1.910000\u0026quot; \u0026quot;1.950000\u0026quot; ... ## $ df_debt_composition :\u0026#39;data.frame\u0026#39;: 6 obs. of 13 variables: ## ..$ CNPJ_CIA : chr [1:6] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:6] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:6], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:6] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:6] 83396 83396 94693 94693 103471 ... ## ..$ VERSAO : num [1:6] 16 16 18 18 11 11 ## ..$ type.debt : chr [1:6] \u0026quot;Empréstimo\u0026quot; \u0026quot;Financiamento\u0026quot; \u0026quot;Empréstimo\u0026quot; \u0026quot;Financiamento\u0026quot; ... ## ..$ type.debt.guarantee : chr [1:6] \u0026quot;Garantia Real\u0026quot; \u0026quot;Quirografárias\u0026quot; \u0026quot;Garantia Real\u0026quot; \u0026quot;Quirografárias\u0026quot; ... ## ..$ debt.value.under.1.year: num [1:6] 1.08e+07 3.11e+08 1.07e+07 3.56e+08 1.04e+07 ... ## ..$ debt.value.1.to.3.years: num [1:6] 31389447 2412999 20681054 6930075 10340527 ... ## ..$ debt.value.3.to.5.years: num [1:6] 0 2522127 0 1194397 0 ... ## ..$ debt.value.more.5.years: num [1:6] 0 0 0 0 0 0 ## ..$ debt.total : num [1:6] 4.22e+07 3.16e+08 3.14e+07 3.64e+08 2.07e+07 ... ## $ df_board_composition :\u0026#39;data.frame\u0026#39;: 47 obs. of 22 variables: ## ..$ CNPJ_CIA : chr [1:47] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:47] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:47], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:47] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:47] 83396 83396 83396 83396 83396 ... ## ..$ VERSAO : num [1:47] 16 16 16 16 16 16 16 16 16 16 ... ## ..$ person.name : chr [1:47] \u0026quot;Gelson Luis Rostirolla\u0026quot; \u0026quot;Rudimar Dall Onder\u0026quot; \u0026quot;Francisco Olinto Velo Schmitt\u0026quot; \u0026quot;Alexandre Grendene Bartelle\u0026quot; ... ## ..$ person.cpf : num [1:47] 1.48e+10 2.55e+10 2.64e+10 9.87e+09 4.30e+09 ... ## ..$ person.profession : chr [1:47] \u0026quot;Administrador de Empresas\u0026quot; \u0026quot;Engenheiro Mecânico\u0026quot; \u0026quot;Engenheiro Elétrico\u0026quot; \u0026quot;Industrial\u0026quot; ... ## ..$ person.cv : chr [1:47] \u0026quot;Formação: Administração de Empresas (1977) e Ciências Contábeis (1979) pela UNOESC – Universidade do Oeste Cata\u0026quot;| __truncated__ \u0026quot;Formação: Engenharia Mecânica (1981) pela Universidade de Caxias do SUL (UCS). Iniciou suas atividades na Compa\u0026quot;| __truncated__ \u0026quot;Formação: Engenharia Elétrica pela Universidade Federal do Rio Grande do Sul em 1978; Especialização e Mestrado\u0026quot;| __truncated__ \u0026quot;Fundador da Companhia e Presidente do Conselho de Administração desde 18 de agosto de 2004. \\n\\nFormação: Bacha\u0026quot;| __truncated__ ... ## ..$ person.dob : Date[1:47], format: \u0026quot;1953-02-14\u0026quot; \u0026quot;1956-08-14\u0026quot; ... ## ..$ code.type.board : chr [1:47] \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; \u0026quot;2\u0026quot; ... ## ..$ desc.type.board : chr [1:47] \u0026quot;Director\u0026quot; \u0026quot;Director\u0026quot; \u0026quot;Director\u0026quot; \u0026quot;Management Council\u0026quot; ... ## ..$ desc.type.board2 : logi [1:47] NA NA NA NA NA NA ... ## ..$ code.type.job : chr [1:47] \u0026quot;11\u0026quot; \u0026quot;10\u0026quot; \u0026quot;12\u0026quot; \u0026quot;20\u0026quot; ... ## ..$ desc.job : chr [1:47] \u0026quot;Não ocupa outras funções no emissor.\u0026quot; \u0026quot;Não ocupa outras funções no emissor.\u0026quot; \u0026quot;Diretor Administrativo Financeiro\u0026quot; \u0026quot;Presidente do comitê de gestão do programa de stock option.\u0026quot; ... ## ..$ date.election : Date[1:47], format: \u0026quot;2019-02-14\u0026quot; \u0026quot;2019-02-14\u0026quot; ... ## ..$ date.effective : Date[1:47], format: \u0026quot;2019-02-14\u0026quot; \u0026quot;2019-02-14\u0026quot; ... ## ..$ mandate.duration : chr [1:47] \u0026quot;3 anos\u0026quot; \u0026quot;3 anos\u0026quot; \u0026quot;3 anos\u0026quot; \u0026quot;2 anos\u0026quot; ... ## ..$ ellected.by.controller : logi [1:47] TRUE TRUE TRUE TRUE TRUE TRUE ... ## ..$ qtd.consecutive.mandates: num [1:47] 6 6 5 8 8 8 8 8 7 9 ... ## ..$ percentage.participation: num [1:47] 0 0 0 100 100 100 100 100 100 0 ... ## $ df_committee_composition :\u0026#39;data.frame\u0026#39;: 30 obs. of 22 variables: ## ..$ CNPJ_CIA : chr [1:30] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:30] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:30], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:30] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:30] 83396 83396 83396 94693 94693 ... ## ..$ VERSAO : num [1:30] 16 16 16 18 18 18 18 18 18 18 ... ## ..$ person.name : chr [1:30] \u0026quot;Alexandre Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Renato Ochman\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; ... ## ..$ person.cpf : num [1:30] 9.87e+09 9.86e+09 3.76e+10 9.86e+09 9.86e+09 ... ## ..$ person.profession : chr [1:30] \u0026quot;Industrial\u0026quot; \u0026quot;Industrial\u0026quot; \u0026quot;Advogado\u0026quot; \u0026quot;Industrial\u0026quot; ... ## ..$ person.cv : chr [1:30] \u0026quot;Fundador da Companhia e Presidente do Conselho de Administração desde 18 de agosto de 2004. \\n\\nFormação: Bacha\u0026quot;| __truncated__ \u0026quot;Fundador da Companhia. Vice-Presidente do Conselho de Administração desde 18 de agosto de 2004. \\n\\nFormação: B\u0026quot;| __truncated__ \u0026quot;Membro do Conselho de Administração desde 18 de agosto de 2004. \\n\\nFormação: Advogado, Bacharel em Direito pel\u0026quot;| __truncated__ NA ... ## ..$ person.dob : Date[1:30], format: \u0026quot;1950-01-23\u0026quot; \u0026quot;1950-01-23\u0026quot; ... ## ..$ code.type.committee : chr [1:30] \u0026quot;9\u0026quot; \u0026quot;9\u0026quot; \u0026quot;9\u0026quot; \u0026quot;9\u0026quot; ... ## ..$ desc.type.committee : chr [1:30] \u0026quot;Other Committee\u0026quot; \u0026quot;Other Committee\u0026quot; \u0026quot;Other Committee\u0026quot; \u0026quot;Other Committee\u0026quot; ... ## ..$ code.type.job : chr [1:30] \u0026quot;1\u0026quot; \u0026quot;3\u0026quot; \u0026quot;3\u0026quot; \u0026quot;3\u0026quot; ... ## ..$ desc.committee : chr [1:30] \u0026quot;Gestão do Programa de Stock Option\u0026quot; \u0026quot;Gestão do Programa de Stock Option\u0026quot; \u0026quot;Gestão do Programa de Stock Option\u0026quot; \u0026quot;Gestão do Programa de Stock Option\u0026quot; ... ## ..$ desc.job : chr [1:30] \u0026quot;Presidente do Conselho de Administração\u0026quot; \u0026quot;Vice presidente do Conselho de Administração\u0026quot; \u0026quot;Membro do Conselho de Administração\u0026quot; \u0026quot;Vice presidente do Conselho de Administração e membro do Comitê de Investimentos\u0026quot; ... ## ..$ date.election : Date[1:30], format: \u0026quot;2015-02-12\u0026quot; \u0026quot;2015-02-12\u0026quot; ... ## ..$ date.effective : Date[1:30], format: \u0026quot;2015-02-12\u0026quot; \u0026quot;2015-02-12\u0026quot; ... ## ..$ mandate.duration : chr [1:30] \u0026quot;indeterminado\u0026quot; \u0026quot;Indeterminado\u0026quot; \u0026quot;Indeterminado\u0026quot; \u0026quot;Indeterminado\u0026quot; ... ## ..$ qtd.consecutive.mandates: num [1:30] 1 1 1 1 1 1 1 1 1 1 ... ## ..$ percentage.participation: num [1:30] 100 100 100 100 100 100 100 100 100 100 ... ## ..$ other.committes : chr [1:30] \u0026quot;Gestão do Programa de Stock Option\u0026quot; \u0026quot;Gestão do Programa de Stock Option\u0026quot; \u0026quot;Gestão do Programa de Stock Option\u0026quot; \u0026quot;Gestão do Programa de Stock Option\u0026quot; ... ## $ df_family_relations :\u0026#39;data.frame\u0026#39;: 20 obs. of 14 variables: ## ..$ CNPJ_CIA : chr [1:20] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:20] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:20], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:20] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:20] 83396 83396 83396 83396 83396 ... ## ..$ VERSAO : num [1:20] 16 16 16 16 16 16 16 16 18 18 ... ## ..$ person.name : chr [1:20] \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Alexandre Grendene Bartelle\u0026quot; \u0026quot;Alexandre Grendene Bartelle\u0026quot; ... ## ..$ person.cpf : num [1:20] 9.86e+09 9.86e+09 9.87e+09 9.87e+09 9.86e+09 ... ## ..$ person.job : chr [1:20] \u0026quot;Vice Presidente do Conselho de Administração\u0026quot; \u0026quot;Diretor Vice Presidente\u0026quot; \u0026quot;Presidente do Conselho de Administração\u0026quot; \u0026quot;Diretor Presidente\u0026quot; ... ## ..$ related.person.name: chr [1:20] \u0026quot;Alexandre Grendene Bartelle\u0026quot; \u0026quot;Alexandre Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; ... ## ..$ related.person.cpf : num [1:20] 9.87e+09 9.87e+09 9.86e+09 9.86e+09 6.86e+10 ... ## ..$ related.person.job : chr [1:20] \u0026quot;Presidente do Conselho de Administração\u0026quot; \u0026quot;Diretor Presidente\u0026quot; \u0026quot;Vice Presidente do Conselho de Administração\u0026quot; \u0026quot;Diretor Vice Presidente\u0026quot; ... ## ..$ code.relationship : chr [1:20] \u0026quot;2\u0026quot; \u0026quot;2\u0026quot; \u0026quot;2\u0026quot; \u0026quot;2\u0026quot; ... ## ..$ desc.relationship : chr [1:20] \u0026quot;Irmão ou Irmã (1º grau por consangüinidade)\u0026quot; \u0026quot;Irmão ou Irmã (1º grau por consangüinidade)\u0026quot; \u0026quot;Irmão ou Irmã (1º grau por consangüinidade)\u0026quot; \u0026quot;Irmão ou Irmã (1º grau por consangüinidade)\u0026quot; ... ## $ df_family_related_companies:\u0026#39;data.frame\u0026#39;: 54 obs. of 15 variables: ## ..$ CNPJ_CIA : chr [1:54] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:54] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:54], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:54] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:54] 83396 83396 83396 83396 83396 ... ## ..$ VERSAO : num [1:54] 16 16 16 16 16 16 16 16 16 16 ... ## ..$ person.name : chr [1:54] \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Pedro Grendene Bartelle\u0026quot; \u0026quot;Maílson Ferreira da Nóbrega\u0026quot; ... ## ..$ person.cpf : num [1:54] 9.86e+09 9.86e+09 9.86e+09 4.30e+09 3.76e+10 ... ## ..$ person.job : chr [1:54] \u0026quot;Vice-Presidente do Conselho de Administração\u0026quot; \u0026quot;Vice-Presidente do Conselho de Administração\u0026quot; \u0026quot;Vice-Presidente do Conselho de Administração\u0026quot; \u0026quot;Conselheiro de Administração\u0026quot; ... ## ..$ type.related.person : chr [1:54] \u0026quot;Cliente\u0026quot; \u0026quot;Fornecedor\u0026quot; \u0026quot;Cliente\u0026quot; \u0026quot;Fornecedor\u0026quot; ... ## ..$ type.relationship : chr [1:54] \u0026quot;Controle\u0026quot; \u0026quot;Controle\u0026quot; \u0026quot;Controle\u0026quot; \u0026quot;Controle\u0026quot; ... ## ..$ observations : chr [1:54] \u0026quot;Venda de insumos e matrizes utilizados na produção de calçados - prazo médio de recebimento 32 dias.\u0026quot; \u0026quot;Compra de serviços referente comissões - Prazo médio de pagamentos 11 dias\u0026quot; \u0026quot;Venda de matrizes utilizadas na produção de calçados - Prazo médio de recebimento 33 dias.\u0026quot; \u0026quot;Assessoria\u0026quot; ... ## ..$ related.company.name: chr [1:54] \u0026quot;Vulcabras|Azaleia - CE, Calçados e Artigos Esportivos S.A.\u0026quot; \u0026quot;Vulcabras|Azaleia - CE, Calçados e Artigos Esportivos S.A.\u0026quot; \u0026quot;Vulcabras|Azaleia - BA, Calçados e Artigos Esportivos S.A.\u0026quot; \u0026quot;Mailson da Nóbrega Consultoria S/C Ltda\u0026quot; ... ## ..$ related.company.cnpj: num [1:54] 9.54e+11 9.54e+11 7.34e+11 1.58e+12 6.24e+13 ... ## ..$ related.company.job : chr [1:54] \u0026quot;Acionista controlador\u0026quot; \u0026quot;Acionista controlados\u0026quot; \u0026quot;Acionista controlador\u0026quot; \u0026quot;Sócio proprietário\u0026quot; ... ## $ df_auditing :\u0026#39;data.frame\u0026#39;: 5 obs. of 14 variables: ## ..$ CNPJ_CIA : chr [1:5] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:5] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:5], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:5] 19615 19615 19615 19615 19615 ## ..$ ID_DOC : num [1:5] 83396 83396 94693 94693 103471 ## ..$ VERSAO : num [1:5] 16 16 18 18 11 ## ..$ auditor.name : chr [1:5] \u0026quot;PRICEWATERHOUSECOOPERS AUDITORES INDEPENDENTES (PWC)\u0026quot; \u0026quot;Ernst \u0026amp; Young Auditores Independentes S/S\u0026quot; \u0026quot;PRICEWATERHOUSECOOPERS AUDITORES INDEPENDENTES (PWC)\u0026quot; \u0026quot;Ernst \u0026amp; Young Auditores Independentes S/S\u0026quot; ... ## ..$ auditor.cnpj : chr [1:5] \u0026quot;61562112000635\u0026quot; \u0026quot;61366936000206\u0026quot; \u0026quot;61562112000635\u0026quot; \u0026quot;61366936001105\u0026quot; ... ## ..$ contract.first.date : Date[1:5], format: \u0026quot;2012-01-01\u0026quot; \u0026quot;2017-01-01\u0026quot; ... ## ..$ contract.last.date : Date[1:5], format: \u0026quot;2016-12-31\u0026quot; NA ... ## ..$ description.contract : chr [1:5] \u0026quot;Revisão dos ITR\u0026#39;s (Controladora e Consolidado) e auditoria anual de balanço da Controladora e Consolidado.\u0026quot; \u0026quot;Revisão dos ITR\u0026#39;s (controladora e Consolidado) e auditoria anual de balanço da Controladora e Consolidado.\u0026quot; \u0026quot;Revisão dos ITR\u0026#39;s (Controladora e Consolidado) e auditoria anual de balanço da Controladora e Consolidado.\u0026quot; \u0026quot;Revisão dos ITR\u0026#39;s (controladora e Consolidado) e auditoria anual de balanço da Controladora e Consolidado.\u0026quot; ... ## ..$ compensation : chr [1:5] \u0026quot;No exercício social encerrado em 31/12/2016 o montante total da remuneração dos auditores independentes foi R$4\u0026quot;| __truncated__ \u0026quot;Para o exercício encerrado em 31/12/2017 - R$409,2 mil, referente a serviços de auditoria prestados e R$131,1 m\u0026quot;| __truncated__ \u0026quot;No exercício social encerrado em 31/12/2016 o montante total da remuneração dos auditores independentes foi R$4\u0026quot;| __truncated__ \u0026quot;Para o exercício encerrado em 31/12/2017 - R$409,2 mil, referente a serviços de auditoria prestados e R$131,1 m\u0026quot;| __truncated__ ... ## ..$ justification.substitution: chr [1:5] \u0026quot;Substituição em cumprimento à Instrução da Comissão de Valores Mobiliários nº 308/99 (art.31), que determina a \u0026quot;| __truncated__ NA \u0026quot;Substituição em cumprimento à Instrução da Comissão de Valores Mobiliários nº 308/99 (art.31), que determina a \u0026quot;| __truncated__ NA ... ## ..$ reason.discordance : chr [1:5] \u0026quot;Não houve discordância\u0026quot; NA \u0026quot;Não houve discordância\u0026quot; NA ... ## $ df_responsible_docs :\u0026#39;data.frame\u0026#39;: 6 obs. of 9 variables: ## ..$ CNPJ_CIA : chr [1:6] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:6] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:6], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:6] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:6] 83396 83396 94693 94693 103471 ... ## ..$ VERSAO : num [1:6] 16 16 18 18 11 11 ## ..$ person.cod : chr [1:6] \u0026quot;66\u0026quot; \u0026quot;67\u0026quot; \u0026quot;38\u0026quot; \u0026quot;39\u0026quot; ... ## ..$ person.name: chr [1:6] \u0026quot;Rudimar Dall Onder\u0026quot; \u0026quot;Francisco Olinto Velo Schmitt\u0026quot; \u0026quot;Rudimar Dall Onder\u0026quot; \u0026quot;Alceu Demartini de Albuquerque\u0026quot; ... ## ..$ person.job : chr [1:6] \u0026quot;Diretor Presidente\u0026quot; \u0026quot;Diretor de Relações com Investidores\u0026quot; \u0026quot;Diretor Presidente\u0026quot; \u0026quot;Diretor de Relações com Investidores\u0026quot; ... ## $ df_stocks_details :\u0026#39;data.frame\u0026#39;: 3 obs. of 16 variables: ## ..$ CNPJ_CIA : chr [1:3] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:3] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:3], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2019-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:3] 19615 19615 19615 ## ..$ ID_DOC : num [1:3] 83396 94693 103471 ## ..$ VERSAO : num [1:3] 16 18 11 ## ..$ type.stock.id : chr [1:3] \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; ## ..$ type.stock.text : chr [1:3] \u0026quot;Ordinária\u0026quot; \u0026quot;Ordinária\u0026quot; \u0026quot;Ordinária\u0026quot; ## ..$ tag.along : num [1:3] 100 100 100 ## ..$ preferential.code : chr [1:3] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; ## ..$ preferential.text : logi [1:3] NA NA NA ## ..$ dividend.text : chr [1:3] \u0026quot;Conforme o Estatuto Social da Companhia, art.32, os acionistas fazem jus a dividendo obrigatório anual equivale\u0026quot;| __truncated__ \u0026quot;Conforme o Estatuto Social da Companhia, art.32, os acionistas fazem jus a dividendo obrigatório anual equivale\u0026quot;| __truncated__ \u0026quot;Conforme o Estatuto Social da Companhia, art.32, os acionistas fazem jus a dividendo obrigatório anual equivale\u0026quot;| __truncated__ ## ..$ flag.voting.rights : chr [1:3] \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; \u0026quot;1\u0026quot; ## ..$ flag.voting.text : chr [1:3] \u0026quot;Pleno\u0026quot; \u0026quot;Pleno\u0026quot; \u0026quot;Pleno\u0026quot; ## ..$ flag.conversibility: chr [1:3] \u0026quot;Não\u0026quot; \u0026quot;Não\u0026quot; \u0026quot;Não\u0026quot; ## ..$ other.info.text : chr [1:3] \u0026quot;Não existem características relevantes adicionais.\u0026quot; \u0026quot;Não existem características relevantes adicionais.\u0026quot; \u0026quot;Não existem características relevantes adicionais.\u0026quot; ## $ df_dividends_details :\u0026#39;data.frame\u0026#39;: 3 obs. of 11 variables: ## ..$ CNPJ_CIA : chr [1:3] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ## ..$ DENOM_CIA : chr [1:3] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ## ..$ DT_REFER : Date[1:3], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2019-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:3] 19615 19615 19615 ## ..$ ID_DOC : num [1:3] 83396 94693 103471 ## ..$ VERSAO : num [1:3] 16 18 11 ## ..$ net.profit : num [1:3] 6.61e+08 5.86e+08 4.95e+08 ## ..$ distributed.dividend: num [1:3] 3.78e+08 3.15e+08 2.76e+08 ## ..$ retained.profit : num [1:3] 2.74e+08 2.70e+08 2.19e+08 ## ..$ payout : num [1:3] 57.2 53.8 55.7 ## ..$ div.yeild.on.equity : num [1:3] 22.6 18.2 14.3 ## $ df_intangible_details :\u0026#39;data.frame\u0026#39;: 38 obs. of 10 variables: ## ..$ CNPJ_CIA : chr [1:38] \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; \u0026quot;89.850.341/0001-60\u0026quot; ... ## ..$ DENOM_CIA : chr [1:38] \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; \u0026quot;GRENDENE S.A.\u0026quot; ... ## ..$ DT_REFER : Date[1:38], format: \u0026quot;2018-01-01\u0026quot; \u0026quot;2018-01-01\u0026quot; ... ## ..$ CD_CVM : num [1:38] 19615 19615 19615 19615 19615 ... ## ..$ ID_DOC : num [1:38] 83396 83396 83396 83396 83396 ... ## ..$ VERSAO : num [1:38] 16 16 16 16 16 16 16 16 16 16 ... ## ..$ id : num [1:38] 1111 1112 1113 1114 1115 ... ## ..$ id.type : num [1:38] 2 2 2 2 2 2 2 2 2 2 ... ## ..$ patent.desc: chr [1:38] \u0026quot;Zaxy\u0026quot; \u0026quot;Mel\u0026quot; \u0026quot;Cartago\u0026quot; \u0026quot;Rider\u0026quot; ... ## ..$ duration : Date[1:38], format: NA NA ...   ","date":1595030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595030400,"objectID":"301f1db29bf8f41f651d0765e761e823","permalink":"http://www.msperlin.com/blog/post/2020-07-18-new_packages-getfredata-getdfpdata2/","publishdate":"2020-07-18T00:00:00Z","relpermalink":"/blog/post/2020-07-18-new_packages-getfredata-getdfpdata2/","section":"post","summary":"Back in 2017 I wrote the first version of package GetDFPData, along with a paper describing the code and providing an empirical application.\nHowever, maintaining the package over the years has been frustrating. The code is becoming increasingly complex, much due to the fact that it handles FRE and DFP data in a single package. Execution speed for large scale importation – many years and many companies – is not reasonable. In top of that, B3’s website is unstable as a source of data and it seems it will stay like that for a long time.","tags":["R","GetFREData","GetDFPData2"],"title":"New Packages: GetDFPdata = GetDFPData2 + GetFREData","type":"post"},{"authors":null,"categories":["R","garch"],"content":"  2020-07-22 Update: The final version of the paper is now published at RAC.\nBack in May 2020, I started to work on a new paper regarding the use of Garch models in R. Today we finished the peer review process and finally got a final version of the article and code. I’m glad to report that the content improved significantly.\nIn a nutshell, the paper motivates GARCH models and presents an empirical application using R: given the recent COVID-19 crisis, we investigate the likelihood of Ibovespa index reach its peak value once again in the upcoming years.\nAll code and data used in the study is available in GitHub, so fell free to download the zip file and play around. Likewise, you can run the same code at RStudio Cloud.\n","date":1594080000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594080000,"objectID":"94886fafb808eba25b907e1f9a30078f","permalink":"http://www.msperlin.com/blog/post/2020-07-07-garch-tutorial-in-r-revised/","publishdate":"2020-07-07T00:00:00Z","relpermalink":"/blog/post/2020-07-07-garch-tutorial-in-r-revised/","section":"post","summary":"2020-07-22 Update: The final version of the paper is now published at RAC.\nBack in May 2020, I started to work on a new paper regarding the use of Garch models in R. Today we finished the peer review process and finally got a final version of the article and code. I’m glad to report that the content improved significantly.\nIn a nutshell, the paper motivates GARCH models and presents an empirical application using R: given the recent COVID-19 crisis, we investigate the likelihood of Ibovespa index reach its peak value once again in the upcoming years.","tags":["R","garch"],"title":"A GARCH Tutorial in R (revised)","type":"post"},{"authors":["Henrique P. Ramos","Marcelo S. Perlin"],"categories":null,"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"dfd11a71a2447b3db1b935d44197f574","permalink":"http://www.msperlin.com/blog/publication/2020_najef-hf-liquidity/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/blog/publication/2020_najef-hf-liquidity/","section":"publication","summary":"This paper provides the first evidence of algorithmic trading (AT) reducing liquidity in the Brazilian equities market. Our results are contrary to the majority of work which finds a positive relationship between AT and liquidity. Using the adoption of a new data center for the B3 exchange as an exogenous shock, we report evidence that AT increased realized spreads in both firm fixed-effects and vector autoregression estimates for 26 stocks between 2017 and 2018 using high-frequency data. We also provide evidence that AT increases commonality in liquidity, evidencing correlated transactions between automated traders.","tags":[],"title":"Does algorithmic trading harm liquidity? Evidence from Brazil","type":"publication"},{"authors":null,"categories":["R","call for papers"],"content":" Myself and Henrique Martins (PUC Rio) organized a call for papers on data reuse, for publication in RAC – Revista de Administração Contemporanea. The deadline for submission is 10th october 2020, with expected publication date in july 2021.\nWe will select quality papers that use data already published and shared in other scientific outlet to test new theories or present a tutorial article, helping students see how an econometric result was achieved in practice.\nYou can find more details about this call for papers in this pdf file. I encourage everyone to submit their work. If you got any question, please let me know.\n","date":1590537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590537600,"objectID":"4407535f7218f1a12975662b154f1f9c","permalink":"http://www.msperlin.com/blog/post/2020-05-27-call-for-papers-rac/","publishdate":"2020-05-27T00:00:00Z","relpermalink":"/blog/post/2020-05-27-call-for-papers-rac/","section":"post","summary":"Myself and Henrique Martins (PUC Rio) organized a call for papers on data reuse, for publication in RAC – Revista de Administração Contemporanea. The deadline for submission is 10th october 2020, with expected publication date in july 2021.\nWe will select quality papers that use data already published and shared in other scientific outlet to test new theories or present a tutorial article, helping students see how an econometric result was achieved in practice.\nYou can find more details about this call for papers in this pdf file. I encourage everyone to submit their work.","tags":["R","call for papers"],"title":"Call for papers on data reuse","type":"post"},{"authors":null,"categories":["pirf","investing"],"content":"  Back in early 2019 I decided to try a new writing style, resulting in my book about investing. The main innovation is the use of real data to make conclusions about the best way to invest money in the fixed income market in Brazil.\nI was a lot of fun to learn more and write about money management and investing, a subject so different from the usual programming topics dealt in this blog. The audience is also much larger, making it a more challenging and rewarding effort.\nI’m happy to report that the feedback has been great. As of today, 2020-05-24, the book has 22 five star ratings in Amazon, a remarkable achievement!\nToday I decided to work and publish the online version of the book. The content for the first three chapters is available at https://www.msperlin.com/pirf/. Hope you like it. If you do, please leave a feedback at Amazon.com.\n","date":1590278400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590278400,"objectID":"37a2694639707848b225b79e82373aae","permalink":"http://www.msperlin.com/blog/post/2020-05-24-pirf-is-online/","publishdate":"2020-05-24T00:00:00Z","relpermalink":"/blog/post/2020-05-24-pirf-is-online/","section":"post","summary":"Back in early 2019 I decided to try a new writing style, resulting in my book about investing. The main innovation is the use of real data to make conclusions about the best way to invest money in the fixed income market in Brazil.\nI was a lot of fun to learn more and write about money management and investing, a subject so different from the usual programming topics dealt in this blog. The audience is also much larger, making it a more challenging and rewarding effort.\nI’m happy to report that the feedback has been great.","tags":["pirf","investing"],"title":"New online book available - Poupando e Investindo em Renda Fixa","type":"post"},{"authors":null,"categories":["R"],"content":"  I’ve been researching financial data for over 10 years and gathered a great deal of compiled tables. Most of these comes from my R packages and have been used for creating class material, doing research and even writing a book. These files were mostly found in many copies across different projects.\nLast week I started to organize and centralize all my data files and noticed how valuable these tables could be for other researchers and teachers.\nAs of today, I’ll be hosting all public compiled data in my dataverse website. Most of them are the product of using my R packages in a large scale data grabbing process. For example, I used package BatchGetSymbols to download daily prices of all stocks belonging to the SP500 since 2015 and saved it in a .csv file.\nHope you find these files as useful as they were to me. If you use it for writing a report, make sure to cite the package as the source of data (see the Readme.txt file in each folder for details).\n","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"bfd26afe04ff8f3ada5daa5f3b4baaf1","permalink":"http://www.msperlin.com/blog/post/2020-04-20-free-compiled-data-in-site/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/blog/post/2020-04-20-free-compiled-data-in-site/","section":"post","summary":"I’ve been researching financial data for over 10 years and gathered a great deal of compiled tables. Most of these comes from my R packages and have been used for creating class material, doing research and even writing a book. These files were mostly found in many copies across different projects.\nLast week I started to organize and centralize all my data files and noticed how valuable these tables could be for other researchers and teachers.\nAs of today, I’ll be hosting all public compiled data in my dataverse website.","tags":["R"],"title":"Financial Datasets Available in the Website","type":"post"},{"authors":null,"categories":["R","GetCVMData"],"content":"   2020-07-18: Package GetCVMData is now named GetDFPData2. See this post for details. The old code in GetCVMData is still in Github but will no longer be developed.  Package GetCVMData is an alternative to GetDFPData. Both have the same objective: fetch corporate data of Brazilian companies trading at B3, but diverge in their source. While GetDFPData imports data directly from the DFP and FRE systems, GetCVMData uses the CVM ftp site for grabbing compiled .csv files.\nWhen doing large scale importations, GetDFPData fells sluggish due to the parsing of large xml files. As an example, building the dataset available in my data page takes around six hours of execution using 10 cores of my home computer.\nGetCVMData is lean and fast. Since the data is already parsed in csv files, all the code does is organize the files, download and read. For comparison, all DFP documents, annual financial reports, available in CVM can be imported in less than 1 minute. Additionally, GetCVMData can also parse ITR (quarterly) data, which was not available in GetDFPData.\nHowever, be aware that the output data is not the same. I kept all original column names from CVM and some information, such as tickers, are not available in GetCVMData.\nHere’s an example of usage:\nif (!require(devtools)) install.packages(\u0026#39;devtools\u0026#39;) if (!require(GetCVMData)) devtools::install_github(\u0026#39;msperlin/GetCVMData\u0026#39;) # not in CRAN yet library(GetCVMData) library(tidyverse) # fetch information about companies df_info \u0026lt;- get_info_companies() # search for companies df_search \u0026lt;- search_company(\u0026#39;odontoprev\u0026#39;) # DFP annual data id_cvm \u0026lt;- df_search$CD_CVM[1] # use NULL for all companies df_dfp \u0026lt;- get_dfp_data(companies_cvm_codes = id_cvm, first_year = 2015, last_year = 2019, type_docs = c(\u0026#39;DRE\u0026#39;, \u0026#39;BPA\u0026#39;, \u0026#39;BPP\u0026#39;), # income, assets, liabilities type_format = \u0026#39;con\u0026#39; # consolidated statements ) glimpse(df_dfp) # ITR quarterly data df_itr \u0026lt;- get_itr_data(companies_cvm_codes = id_cvm, first_year = 2010, last_year = 2020, type_docs = c(\u0026#39;DRE\u0026#39;, \u0026#39;BPA\u0026#39;, \u0026#39;BPP\u0026#39;), # income, assets, liabilities type_format = \u0026#39;con\u0026#39; # consolidated statements ) glimpse(df_itr) # FRE data (not yet implemented..) #df_fre \u0026lt;- get_fre_data() Lets plot the quarterly profit of df_itr$DENOM_CIA[1]:\nlibrary(tidyverse) quarterly_profits \u0026lt;- df_itr %\u0026gt;% filter(GRUPO_DFP == \u0026#39;DF Consolidado - Demonstração do Resultado\u0026#39;, DS_CONTA == \u0026#39;Lucro/Prejuízo Consolidado do Período\u0026#39;) # plot it p \u0026lt;- ggplot(quarterly_profits, aes(x = DT_FIM_EXERC, y = VL_CONTA)) + geom_col() + theme_bw() + labs(title = paste0(\u0026#39;Quarterly profits of \u0026#39;, quarterly_profits$DENOM_CIA[1]), caption = \u0026#39;Data from CVM\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;Consolidade Profits\u0026#39;) p ","date":1587340800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587340800,"objectID":"eb85ccbb86281996f1dda7795b1dd7da","permalink":"http://www.msperlin.com/blog/post/2020-04-20-new-package-getcvmdata/","publishdate":"2020-04-20T00:00:00Z","relpermalink":"/blog/post/2020-04-20-new-package-getcvmdata/","section":"post","summary":"2020-07-18: Package GetCVMData is now named GetDFPData2. See this post for details. The old code in GetCVMData is still in Github but will no longer be developed.  Package GetCVMData is an alternative to GetDFPData. Both have the same objective: fetch corporate data of Brazilian companies trading at B3, but diverge in their source. While GetDFPData imports data directly from the DFP and FRE systems, GetCVMData uses the CVM ftp site for grabbing compiled .csv files.\nWhen doing large scale importations, GetDFPData fells sluggish due to the parsing of large xml files.","tags":["R","GetCVMData"],"title":"New R package: GetCVMData","type":"post"},{"authors":null,"categories":["R","GetDFPData"],"content":"  After battling B3’s website for days, I finally managed to gather a master table for all corporate data. I’m happy to report that the 2019’s data is now included in GetDFPData, the CRAN package and shiny interface. This includes new financial statements and company’s FRE data.\nI also want to use this update to formally thank everyone that made a donation in the shiny website. Your donation is not only helping paying for the bills of the server but increasing my motivation for working further in this project.\nAs for R code with then new dataset, let’s give it a try:\nlibrary(GetDFPData) library(tidyverse) name.companies \u0026lt;- c(\u0026#39;PETRÓLEO BRASILEIRO S.A. - PETROBRAS\u0026#39;) first.date \u0026lt;- \u0026#39;2017-01-01\u0026#39; last.date \u0026lt;- \u0026#39;2020-01-01\u0026#39; inflation.index \u0026lt;- \u0026#39;IPCA\u0026#39; df.reports \u0026lt;- gdfpd.GetDFPData(name.companies = name.companies, first.date = first.date, last.date = last.date) glimpse(df.reports) ","date":1587081600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587081600,"objectID":"cd224183e9c9f436cf1e052cb6b77a3d","permalink":"http://www.msperlin.com/blog/post/2020-04-17-update-getdfpdata/","publishdate":"2020-04-17T00:00:00Z","relpermalink":"/blog/post/2020-04-17-update-getdfpdata/","section":"post","summary":"After battling B3’s website for days, I finally managed to gather a master table for all corporate data. I’m happy to report that the 2019’s data is now included in GetDFPData, the CRAN package and shiny interface. This includes new financial statements and company’s FRE data.\nI also want to use this update to formally thank everyone that made a donation in the shiny website. Your donation is not only helping paying for the bills of the server but increasing my motivation for working further in this project.","tags":["R","GetDFPData"],"title":"Update on GetDFPData tables -- 2019's DFP and FRE data","type":"post"},{"authors":null,"categories":["R","garch"],"content":"  Myself, Mauro Mastella, Daniel Vancin and Henrique Ramos, just finished a tutorial paper about GARCH models in R and I believe it is a good content for those learning financial econometrics. You can find the full paper in this link.\nIn a nutshell, the paper introduces motivation behind the GARCH type of models and presents an empirical application: given the recent COVID-19 crisis, we investigate how much time it would take for the Ibovespa index to reach its peak value once again. The results indicate that it would take, on average, about two and half years for the index to recover.\nAll code and data used in the study is available in GitHub, so fell free to download the zip file and play around. You can find all figures of the paper in this link. Worth pointing out that you can reproduce all results in your own computer by executing the source code at GitHub.\n ","date":1585440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585440000,"objectID":"ab7d0fb27681eff9ad15f2f56961c58c","permalink":"http://www.msperlin.com/blog/post/2020-03-29-garch-tutorial-in-r/","publishdate":"2020-03-29T00:00:00Z","relpermalink":"/blog/post/2020-03-29-garch-tutorial-in-r/","section":"post","summary":"Myself, Mauro Mastella, Daniel Vancin and Henrique Ramos, just finished a tutorial paper about GARCH models in R and I believe it is a good content for those learning financial econometrics. You can find the full paper in this link.\nIn a nutshell, the paper introduces motivation behind the GARCH type of models and presents an empirical application: given the recent COVID-19 crisis, we investigate how much time it would take for the Ibovespa index to reach its peak value once again. The results indicate that it would take, on average, about two and half years for the index to recover.","tags":["R","garch"],"title":"A GARCH Tutorial in R","type":"post"},{"authors":null,"categories":["R","afedR"],"content":"   2021-02-28 – Important: this blog post is deprecated. With the last revision in march 2021, the Rmd slides are no longer available within the book material.  The slides for my newly released book Analyzing Financial and Economic Data with R are finally ready! I apologize for keep you guys waiting.\nThe slides are available as independent .Rmd files for all book chapters including:\n## character(0) All content is released with a generous MIT license, so fell free to use and edit the files as you wish.\nYou can download the slides and other book material with the following code:\nif (!require(devtools)) install.packages(\u0026quot;devtools\u0026quot;) devtools::install_github(\u0026#39;msperlin/afedR\u0026#39;) # ignore warning messages (long paths..) afedR::afedR_get_book_files(path_to_copy = \u0026#39;~\u0026#39;) Alternatively, you can download the files directly from github.\nThe bundle also includes other teaching material that may help conduct a complete R tutorial:\n Solutions to end of chapter exercises\n All R Code used in the book\n Dynamic exercises (package exams)\n Data files\n  If you liked the material, please consider purchasing the book and leaving your feedback at Amazon. Your oppinion is very important for promoting the book and help others learn more about R and RStudio. As an author, I certainly appreciate the gesture and will take it as a motivating factor for future editions of the book.\n","date":1582588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582588800,"objectID":"905089070ed26ea795bcbd5ca133de9a","permalink":"http://www.msperlin.com/blog/post/2020-02-25-afedr-ed2-slides-available/","publishdate":"2020-02-25T00:00:00Z","relpermalink":"/blog/post/2020-02-25-afedr-ed2-slides-available/","section":"post","summary":"2021-02-28 – Important: this blog post is deprecated. With the last revision in march 2021, the Rmd slides are no longer available within the book material.  The slides for my newly released book Analyzing Financial and Economic Data with R are finally ready! I apologize for keep you guys waiting.\nThe slides are available as independent .Rmd files for all book chapters including:\n## character(0) All content is released with a generous MIT license, so fell free to use and edit the files as you wish.","tags":["R","afedR"],"title":"Book slides - Analyzing Financial and Economic Data with R","type":"post"},{"authors":null,"categories":["R","afedR"],"content":"   2021-02-28 – The book was revised in march 2021. See this blog post for details.  After a couple of unexpected delays, I am very pleased to announce the publication of the second edition of my book, Analyzing Financial and Economic Data with R. You can find it in Amazon as an ebook or paperback. An online version is available here. More details, including supplementary material, are available in the book webpage.\nThe first edition was released back in 2017 and it was a great journey working once again in this material. Many sections and chapters have been improved, along with new content. Here are the main changes:\n Alignment with the tidyverse  Some base function are presented but priority is for readr, ggplot2, dplyr, stringr, purrr and so on. 100+ pages of new content (a 25% overall increase from previous edition).   Teaching Material  Static end of chapter exercises, with solutions publicly available in the internet; Rmarkdown slides for each chapter; Dynamic 90+ exercises with the exams package. This means you can create and grade randomized unique tests for your students (see this post for details);   Book package afedR  This package makes it easier to import book datasets, slides, functions and solutions for end-of-chapter exercises. Available in GitHub only. See this blog post for instructions on installation.   Three new chapters  Cleaning and Structuring Financial and Economic Data – How to clean financial and economic data by dealing with long/wide dataframes, outlier detection/removal and desinflating prices and indexes. Reporting Results – Using xtable and texreg to report tables and models. Includes a special section about the RMarkdown technology. Optimizing Code – Profiling code for bottlenecks and using vectorization, rcpp and memoise to speed up R computations.   Two new packages in CRAN  simfinR – grabs corporate datasets from the SimFin project; GetQuandlData– uses Quandl json api and caching for easier and faster data importation;    If you liked the material, please consider purchasing it and leaving your feedback at Amazon. Your oppinion is very important for promoting the book and help others learn more about R and RStudio. As an author, I certainly appreciate the gesture and will take it as a motivating factor for future editions of the book.\n","date":1581292800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581292800,"objectID":"b653e4249844be2b52501ead83ee59eb","permalink":"http://www.msperlin.com/blog/post/2020-01-15-afedr-ed2-announcement/","publishdate":"2020-02-10T00:00:00Z","relpermalink":"/blog/post/2020-01-15-afedr-ed2-announcement/","section":"post","summary":"2021-02-28 – The book was revised in march 2021. See this blog post for details.  After a couple of unexpected delays, I am very pleased to announce the publication of the second edition of my book, Analyzing Financial and Economic Data with R. You can find it in Amazon as an ebook or paperback. An online version is available here. More details, including supplementary material, are available in the book webpage.\nThe first edition was released back in 2017 and it was a great journey working once again in this material.","tags":["R","afedR"],"title":"Book release - Analyzing Financial and Economic Data with R (2º edition)","type":"post"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"Where to buy it You can purchase the book from Amazon as an ebook or paperback. An online version of the book, with the first seven chapters is available here.\nCitation You can cite the book as:\nPERLIN, Marcelo S. Analyzing Financial and Economic Data with R. 2 ed. Porto Alegre: Marcelo S. Perlin (independent publisher), 2020.\nDescription This book introduces the reader to the use of R and RStudio as a platform for analyzing financial and economic data. The book covers all necessary knowledge for using R, from its installation in your computer to the organization and development of scripts. For every chapter, the book presents practical and replicable examples of R code, providing context and facilitating the learning process.\nThis is what you\u0026rsquo;ll learn from this book:\n01 - Using R and RStudio\n02 - Importing financial and economic data\n03 - Cleaning, structuring and analyzing the data with R\n04 - Creating a visual analysis of data\n05 - Reporting your results\n06 - Writing better and faster code\nSupplement Material  01-Online edition\n 02-Solutions to end of chapter exercises\n 03-Compiling book exercises to pdf, html or e-learning platforms\n 04-Book package in Github\n 05-Data, Code and Exercises\n 06-Table of Contents\nBook Links Installing R and RStudio:\n https://www.r-project.org/ | https://www.rstudio.com\nCRAN pages:\n R Views for Finance | R Consortium | Official R Manual | CRAN Policies\nRMarkdown:\n Official RMarkdown Tutorial\nCreating Packages:\n Hadleys Guide for creating packages\nR code style:\n Google R style guide | Tidyverse style guide\n","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"e0b6663493bfcc9c8273c40484a9dd07","permalink":"http://www.msperlin.com/blog/publication/2020_book-afedr-en/","publishdate":"2020-02-01T00:00:00Z","relpermalink":"/blog/publication/2020_book-afedr-en/","section":"publication","summary":"Where to buy it You can purchase the book from Amazon as an ebook or paperback. An online version of the book, with the first seven chapters is available here.\nCitation You can cite the book as:\nPERLIN, Marcelo S. Analyzing Financial and Economic Data with R. 2 ed. Porto Alegre: Marcelo S. Perlin (independent publisher), 2020.\nDescription This book introduces the reader to the use of R and RStudio as a platform for analyzing financial and economic data. The book covers all necessary knowledge for using R, from its installation in your computer to the organization and development of scripts.","tags":null,"title":"Analyzing Financial and Economic Data with R (Second Edition)","type":"publication"},{"authors":null,"categories":["R","site"],"content":"  I’m just about to leave for my vacation and, as usual, I’ll write about the highlights of 2019 and my plans for the year to come. First, let’s talk about my work in 2019.\nHighlights of 2019 The year of 2019 was not particularly fruitful in journal publications. I only had two: Accessing Financial Reports and Corporate Events with GetDFPData, published in RBfin and A consumer credit risk structural model based on affordability: balance at risk published in JCR. Both are papers I wrote back in 2017 and 2018 and not new articles. Most of the papers I worked this year will be published in 2020 or 2021.\nThis year, I’m mostly proud of the publication of my book about investing in the fixed income markets Poupando e Investindo em Renda Fixa: Uma Abordagem Baseada em Dados. This was a special project, very different from the usual writing style of scientific research and, lots of fun to write. As a side effect, I learned a lot about the fixed income market in Brasil and it forced me to think objectively about a problem that is inherently subjective, personal finance and investing. Hopefully, in the future, I’ll write another book about the stock market and real state investing, a topic that I’m also very interested.\nMy main project this year is the work in the second edition of my R book Analyzing Financial and Economic Data with R. It took a major part of my working weeks and weekends, but its coming together. Soon I’ll be publishing it in Amazon. You can find more details about it in this previous post.\nIn the programming side, I wrote two new CRAN packages in 2019: simfinR and GetQuandlData. Both are featured in the new edition of my R book (soon to be published).\n My blog posts in 2018 Let’s have a look at my blog posts so so far.\nmy.blog.folder \u0026lt;- \u0026#39;~/Dropbox/11-My Website/01-msperlin.com/content/post/\u0026#39; post.files \u0026lt;- list.files(path = my.blog.folder, pattern = \u0026#39;.Rmd\u0026#39;) post.files ## [1] \u0026quot;2017-02-16-Writing-a-book.Rmd\u0026quot; ## [2] \u0026quot;2017-02-16-Writing-a-book.Rmd.lock~\u0026quot; ## [3] \u0026quot;2017-12-06-Package-GetDFPData.Rmd\u0026quot; ## [4] \u0026quot;2017-12-06-Package-GetDFPData.Rmd.lock~\u0026quot; ## [5] \u0026quot;2017-12-13-Serving-shiny-apps-internet.Rmd\u0026quot; ## [6] \u0026quot;2017-12-13-Serving-shiny-apps-internet.Rmd.lock~\u0026quot; ## [7] \u0026quot;2017-12-30-Looking-Back-2017.Rmd\u0026quot; ## [8] \u0026quot;2017-12-30-Looking-Back-2017.Rmd.lock~\u0026quot; ## [9] \u0026quot;2018-01-22-Update-BatchGetSymbols.Rmd\u0026quot; ## [10] \u0026quot;2018-01-22-Update-BatchGetSymbols.Rmd.lock~\u0026quot; ## [11] \u0026quot;2018-03-16-Writing_Papers_About_Pkgs.Rmd\u0026quot; ## [12] \u0026quot;2018-03-16-Writing_Papers_About_Pkgs.Rmd.lock~\u0026quot; ## [13] \u0026quot;2018-04-22-predatory-scientometrics.Rmd\u0026quot; ## [14] \u0026quot;2018-04-22-predatory-scientometrics.Rmd.lock~\u0026quot; ## [15] \u0026quot;2018-05-12-Investing-Long-Run.Rmd\u0026quot; ## [16] \u0026quot;2018-05-12-Investing-Long-Run.Rmd.lock~\u0026quot; ## [17] \u0026quot;2018-06-12-padfR-ed2.Rmd\u0026quot; ## [18] \u0026quot;2018-06-12-padfR-ed2.Rmd.lock~\u0026quot; ## [19] \u0026quot;2018-06-29-BenchmarkingSSD.Rmd\u0026quot; ## [20] \u0026quot;2018-06-29-BenchmarkingSSD.Rmd.lock~\u0026quot; ## [21] \u0026quot;2018-10-10-BatchGetSymbols-NewVersion.Rmd\u0026quot; ## [22] \u0026quot;2018-10-10-BatchGetSymbols-NewVersion.Rmd.lock~\u0026quot; ## [23] \u0026quot;2018-10-11-Update-GetLattesData.Rmd\u0026quot; ## [24] \u0026quot;2018-10-11-Update-GetLattesData.Rmd.lock~\u0026quot; ## [25] \u0026quot;2018-10-13-NewPackage-PkgsFromFiles.Rmd\u0026quot; ## [26] \u0026quot;2018-10-13-NewPackage-PkgsFromFiles.Rmd.lock~\u0026quot; ## [27] \u0026quot;2018-10-19-R-and-loops.Rmd\u0026quot; ## [28] \u0026quot;2018-10-19-R-and-loops.Rmd.lock~\u0026quot; ## [29] \u0026quot;2018-10-20-Linux-and-R.Rmd\u0026quot; ## [30] \u0026quot;2018-10-20-Linux-and-R.Rmd.lock~\u0026quot; ## [31] \u0026quot;2018-11-03-NewBlog.Rmd\u0026quot; ## [32] \u0026quot;2018-11-03-NewBlog.Rmd.lock~\u0026quot; ## [33] \u0026quot;2018-11-03-RstudioTricks.Rmd\u0026quot; ## [34] \u0026quot;2018-11-03-RstudioTricks.Rmd.lock~\u0026quot; ## [35] \u0026quot;2019-01-08-Looking-Back-2018.Rmd\u0026quot; ## [36] \u0026quot;2019-01-08-Looking-Back-2018.Rmd.lock~\u0026quot; ## [37] \u0026quot;2019-01-12-GetDFPData-ver14.Rmd\u0026quot; ## [38] \u0026quot;2019-01-12-GetDFPData-ver14.Rmd.lock~\u0026quot; ## [39] \u0026quot;2019-03-09-pafdR-promotion.Rmd\u0026quot; ## [40] \u0026quot;2019-03-09-pafdR-promotion.Rmd.lock~\u0026quot; ## [41] \u0026quot;2019-03-10-pafdR-promotion_2.Rmd\u0026quot; ## [42] \u0026quot;2019-03-10-pafdR-promotion_2.Rmd.lock~\u0026quot; ## [43] \u0026quot;2019-03-23-Bettina-Case.Rmd\u0026quot; ## [44] \u0026quot;2019-03-23-Bettina-Case.Rmd.lock~\u0026quot; ## [45] \u0026quot;2019-04-13-Parallel-BatchGetsymbols.Rmd\u0026quot; ## [46] \u0026quot;2019-04-13-Parallel-BatchGetsymbols.Rmd.lock~\u0026quot; ## [47] \u0026quot;2019-04-15-GetBCBData.Rmd\u0026quot; ## [48] \u0026quot;2019-04-15-GetBCBData.Rmd.lock~\u0026quot; ## [49] \u0026quot;2019-05-01-MeanVariance.Rmd\u0026quot; ## [50] \u0026quot;2019-05-01-MeanVariance.Rmd.lock~\u0026quot; ## [51] \u0026quot;2019-05-17-R-in-Brazil.Rmd\u0026quot; ## [52] \u0026quot;2019-05-17-R-in-Brazil.Rmd.lock~\u0026quot; ## [53] \u0026quot;2019-05-20-Lindy-Effect.Rmd\u0026quot; ## [54] \u0026quot;2019-05-20-Lindy-Effect.Rmd.lock~\u0026quot; ## [55] \u0026quot;2019-07-01-ftp-shutdown.Rmd\u0026quot; ## [56] \u0026quot;2019-07-01-ftp-shutdown.Rmd.lock~\u0026quot; ## [57] \u0026quot;2019-08-08-ftp-NOT-shutdown.Rmd\u0026quot; ## [58] \u0026quot;2019-08-08-ftp-NOT-shutdown.Rmd.lock~\u0026quot; ## [59] \u0026quot;2019-10-01-new-package-GetQuandlData.Rmd\u0026quot; ## [60] \u0026quot;2019-10-01-new-package-GetQuandlData.Rmd.lock~\u0026quot; ## [61] \u0026quot;2019-10-12-support-GetDFPData-shiny.Rmd\u0026quot; ## [62] \u0026quot;2019-10-12-support-GetDFPData-shiny.Rmd.lock~\u0026quot; ## [63] \u0026quot;2019-10-16-new-package-GetEdgarData.Rmd\u0026quot; ## [64] \u0026quot;2019-10-16-new-package-GetEdgarData.Rmd.lock~\u0026quot; ## [65] \u0026quot;2019-11-01-new-package-simfinR.Rmd\u0026quot; ## [66] \u0026quot;2019-11-01-new-package-simfinR.Rmd.lock~\u0026quot; ## [67] \u0026quot;2019-11-25-feedback-TOC-afedR.Rmd\u0026quot; ## [68] \u0026quot;2019-11-25-feedback-TOC-afedR.Rmd.lock~\u0026quot; ## [69] \u0026quot;2019-12-02-dynamic-exercises-afedR.Rmd\u0026quot; ## [70] \u0026quot;2019-12-02-dynamic-exercises-afedR.Rmd.lock~\u0026quot; ## [71] \u0026quot;2019-12-15-Looking-Back-2019.Rmd\u0026quot; ## [72] \u0026quot;2019-12-15-Looking-Back-2019.Rmd.lock~\u0026quot; ## [73] \u0026quot;2020-01-15-afedR-ed2-announcement.Rmd\u0026quot; ## [74] \u0026quot;2020-01-15-afedR-ed2-announcement.Rmd.lock~\u0026quot; ## [75] \u0026quot;2020-02-25-afedR-ed2-slides-available.Rmd\u0026quot; ## [76] \u0026quot;2020-02-25-afedR-ed2-slides-available.Rmd.lock~\u0026quot; ## [77] \u0026quot;2020-03-29-garch-tutorial-in-r.Rmd\u0026quot; ## [78] \u0026quot;2020-03-29-garch-tutorial-in-r.Rmd.lock~\u0026quot; ## [79] \u0026quot;2020-04-17-update-getdfpdata.Rmd\u0026quot; ## [80] \u0026quot;2020-04-17-update-getdfpdata.Rmd.lock~\u0026quot; ## [81] \u0026quot;2020-04-20-free-compiled-data-in-site.Rmd\u0026quot; ## [82] \u0026quot;2020-04-20-free-compiled-data-in-site.Rmd.lock~\u0026quot; ## [83] \u0026quot;2020-04-20-new-package-GetCVMData.Rmd\u0026quot; ## [84] \u0026quot;2020-04-20-new-package-GetCVMData.Rmd.lock~\u0026quot; ## [85] \u0026quot;2020-04-25-investments-costs.Rmd\u0026quot; ## [86] \u0026quot;2020-04-25-investments-costs.Rmd.lock~\u0026quot; ## [87] \u0026quot;2020-05-24-pirf-is-online.Rmd\u0026quot; ## [88] \u0026quot;2020-05-24-pirf-is-online.Rmd.lock~\u0026quot; ## [89] \u0026quot;2020-05-27-call-for-papers-rac.Rmd\u0026quot; ## [90] \u0026quot;2020-05-27-call-for-papers-rac.Rmd.lock~\u0026quot; ## [91] \u0026quot;2020-07-07-garch-tutorial-in-r-REVISED.Rmd\u0026quot; ## [92] \u0026quot;2020-07-07-garch-tutorial-in-r-REVISED.Rmd.lock~\u0026quot; ## [93] \u0026quot;2020-07-18-new_packages-GetFREData-GetDFPData2.Rmd\u0026quot; ## [94] \u0026quot;2020-07-18-new_packages-GetFREData-GetDFPData2.Rmd.lock~\u0026quot; ## [95] \u0026quot;2020-12-22-Looking-Back-2020.Rmd\u0026quot; ## [96] \u0026quot;2020-12-22-Looking-Back-2020.Rmd.lock~\u0026quot; ## [97] \u0026quot;2021-02-18-dynamic-exercises-adfeR.Rmd\u0026quot; ## [98] \u0026quot;2021-02-18-dynamic-exercises-adfeR.Rmd.lock~\u0026quot; ## [99] \u0026quot;2021-02-19-dynamic-exercises-adfeR.Rmd.lock~\u0026quot; ## [100] \u0026quot;2021-02-20-adfeR-ed3-announcement.Rmd\u0026quot; ## [101] \u0026quot;2021-02-20-adfeR-ed3-announcement.Rmd.lock~\u0026quot; The blog started in january of 2017 and, over time, I wrote 101 posts, around 33.6666667 per year. Let’s get more information from the .Rmd files. I’ll write function read_blog_files and use it for all post files.\nread_blog_files \u0026lt;- function(f.in) { require(tidyverse) my.front.matter \u0026lt;- rmarkdown::yaml_front_matter(f.in) df.out \u0026lt;- data_frame(post.title = my.front.matter$title, post.date = lubridate::ymd(my.front.matter$date), post.month = as.Date(format(post.date, \u0026#39;%Y-%m-01\u0026#39;)), tags = paste0(my.front.matter$tags, collapse = \u0026#39;;\u0026#39;), categories = paste0(my.front.matter$categories, collapse = \u0026#39;;\u0026#39;), content = paste0(read_lines(f.in), collapse = \u0026#39; \u0026#39;)) return(df.out) } df.posts \u0026lt;- dplyr::bind_rows(purrr::map(post.files, read_blog_files)) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. glimpse(df.posts) ## Rows: 50 ## Columns: 6 ## $ post.title \u0026lt;chr\u0026gt; \u0026quot;Writing a R book and self-publishing it in Amazon\u0026quot;, \u0026quot;Pack… ## $ post.date \u0026lt;date\u0026gt; 2017-02-16, 2017-12-06, 2017-12-13, 2017-12-30, 2018-01-2… ## $ post.month \u0026lt;date\u0026gt; 2017-02-01, 2017-12-01, 2017-12-01, 2017-12-01, 2018-01-0… ## $ tags \u0026lt;chr\u0026gt; \u0026quot;R;book;self-publish\u0026quot;, \u0026quot;R;GetDFPData;corporate events;fina… ## $ categories \u0026lt;chr\u0026gt; \u0026quot;R;book;self-publish\u0026quot;, \u0026quot;R;GetDFPData;B3\u0026quot;, \u0026quot;R;shiny;webserv… ## $ content \u0026lt;chr\u0026gt; \u0026quot;--- title: \\\u0026quot;Writing a R book and self-publishing it in A… First, let’s look at the frequency of posts over time.\ndf.posts \u0026lt;- df.posts %\u0026gt;% filter(post.date \u0026gt;= as.Date(\u0026#39;2019-01-01\u0026#39;), post.date \u0026lt;= as.Date(\u0026#39;2020-01-01\u0026#39;)) print( ggplot(df.posts, aes(x = post.month)) + geom_histogram(stat=\u0026#39;count\u0026#39;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(y = \u0026#39;Number of posts\u0026#39;, x = \u0026#39;\u0026#39;)) ## Warning: Ignoring unknown parameters: binwidth, bins, pad  Checking 2019’s plans At the end of 2018, my plans for 2019 were:\n New edition of “Analyzing Financial and Economic Data with R” in progress!  Work on my new book: “Investing For the Long Term” (provisory title) Done! The first idea was to write a book about investing in general. I soon realized I would not be able to complete such task within one year. So, I decided to first write about the fixed income market and later, perhaps, write about stock markets and real state.  Solidify my research agenda in Board Composition In progress. I’ve got four papers in development, two already submitted and two in the pipeline.    Plans for 2020  Publish afedR (analyzing financial and economic data with R) My expectation is to publish the book in the first months of the year. I believe it is quite doable, unless something unexpected happens.  Finish board papers Also doable. The working papers are in a good shape and should be submitted soon.  Start “personal finance project” I’m not yet sure how to call it, but I’ve got a couple of ideas for creating a project all about helping people with their finances.    ","date":1576108800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576108800,"objectID":"8a39efe5b2791bc5ffd33887c1047373","permalink":"http://www.msperlin.com/blog/post/2019-12-15-looking-back-2019/","publishdate":"2019-12-12T00:00:00Z","relpermalink":"/blog/post/2019-12-15-looking-back-2019/","section":"post","summary":"I’m just about to leave for my vacation and, as usual, I’ll write about the highlights of 2019 and my plans for the year to come. First, let’s talk about my work in 2019.\nHighlights of 2019 The year of 2019 was not particularly fruitful in journal publications. I only had two: Accessing Financial Reports and Corporate Events with GetDFPData, published in RBfin and A consumer credit risk structural model based on affordability: balance at risk published in JCR. Both are papers I wrote back in 2017 and 2018 and not new articles.","tags":["R","site"],"title":"Looking back at 2019 and plans for 2020","type":"post"},{"authors":null,"categories":["R","afedR"],"content":"   This post is deprecated due to changes in package code. See the new post in this link.  In the new edition of my R book, to be released in early 2020 (see current TOC, new packages and notification form), I’m giving special attention to its use in the classroom. For that, I’ve created class slides and R exercises in the static and dynamic form. All the extra content will be freely available in the internet and distributed with package afedR. Anyone can use it, without the need of purchasing the book (but off course it would help).\nTo access the files, first install the package from Github with devtools (ignore warning messages about long paths):\ndevtools::install_github(\u0026#39;msperlin/afedR\u0026#39;) You can copy all book content to a local folder using the following function:\nmy_tempdir \u0026lt;- tempdir() afedR::afedR_get_book_files(path_to_copy = my_tempdir) The static exercises for all chapters are available in the afedR files/eoc-exercises folder:\nlist.files(file.path(my_tempdir, \u0026#39;afedR files/eoc-exercises/\u0026#39;)) Every .Rmd file is self-contained and should compile without any problems in your computer.\nNow, the real benefit of the package is in the dynamic R exercises you can create with package exams. Back in 2017 I already talked about my admiration and use of exams in all of my university classes. In short, you can use exams to create an unique version of a exercise for each student by randomizing numbers and text. All questions are written in .Rmd/.Rnw files and, since its all RMarkdown code, you can make it as dynamic as possible. The amount of hours it saved me so far in creating and grading exams is unbelievable! I even changed the structure of all my classes to a more activity-oriented coursework based on single-choice exercises. The feedback I get from the students has been very positive.\nAs a result of using exams for many years, I wrote a significant database of R single-choice questions that I use in my university courses. It amounts to 91 questions, covering R basics, functions, class objects, programming, econometrics, and much more. All of these exam questions are included in the package and I’ll add more with time. You can find all of them in a compiled html file in this link.\nCreating a Dynamic Exam Creating a dynamic R exam is simple. All you need is the names of all students among other options. Function afedR_build_exam will grab all exercise templates, compile each exam, and output a different .html file for each student. Have a look at a reproducible example:\nlibrary(afedR) library(tidyverse) set.seed(1) student_names \u0026lt;- c(\u0026#39;Roger Federer\u0026#39;, \u0026#39;John Wick\u0026#39;, \u0026#39;Robert Engle\u0026#39;, \u0026#39;Getulio Vargas\u0026#39;, \u0026#39;Mario Quintana\u0026#39;, \u0026#39;Elis Regina\u0026#39;) my_ids \u0026lt;- c(sample(seq(length(student_names)))) # ids (usually a numeric and unique symbol given by the university) class_name \u0026lt;- \u0026#39;R Workshop\u0026#39; exercise_name \u0026lt;- \u0026#39;Introduction to R\u0026#39; temp_dir \u0026lt;- file.path(tempdir(), \u0026#39;html exams\u0026#39;) # where to create exam files l_out \u0026lt;- afedR_build_exam(students_names = student_names, students_ids = my_ids, class_name = class_name, exercise_name = \u0026#39;Introduction to R\u0026#39;, chapters_to_include = 2, # single chapter for simplicity (it goes from 1-13) dir_out = temp_dir) Done. All exams files are available in folder temp_dir:\nlist.files(temp_dir) As an example of html output, file Introduction to R_Roger Federer_Ver 01.html is available in this link.\nGoing further, the output of afedR_build_exam is a list that includes the correct answers for each student/question:\nprint(l_out$answer_key) You can use this table for grading all exams. Currently I use Google Forms to register student’s answers with an online questionnaire. This helps because I can turn all answers in a single Google Spreadsheet, import it in R with package googlesheets4, and effortlessly grade all exams in a R script. Soon, in another post, I’ll write about my detailed workflow in using exams with Google Forms and Google Classroom.\nI hope this content helps all R instructions around the world. But, it is a work in progress. If you find any issue, please let me know by email or posting an issue event at the github code.\nThe book is finally reaching its final version and I’m very excited about it. Its been a long journey. You can find more details about the new book here. If you want to be notified about the publication, just sign this form and I’ll email you as soon as the book becomes available.\n ","date":1575244800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575244800,"objectID":"20ce0f5b6198a95894f3a73aa2cf6e46","permalink":"http://www.msperlin.com/blog/post/2019-12-02-dynamic-exercises-afedr/","publishdate":"2019-12-02T00:00:00Z","relpermalink":"/blog/post/2019-12-02-dynamic-exercises-afedr/","section":"post","summary":"This post is deprecated due to changes in package code. See the new post in this link.  In the new edition of my R book, to be released in early 2020 (see current TOC, new packages and notification form), I’m giving special attention to its use in the classroom. For that, I’ve created class slides and R exercises in the static and dynamic form. All the extra content will be freely available in the internet and distributed with package afedR. Anyone can use it, without the need of purchasing the book (but off course it would help).","tags":["R","afedR"],"title":"Static and Dynamic Book Exercises with R","type":"post"},{"authors":["Marcelo S. Perlin","Guilherme Kirch","Daniel Vancin"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"956d4a181eb87c05145ac6ed8e7eeb09","permalink":"http://www.msperlin.com/blog/publication/2019_rbfin-getdfpdata/","publishdate":"2019-12-01T00:00:00Z","relpermalink":"/blog/publication/2019_rbfin-getdfpdata/","section":"publication","summary":"This paper presents and discusses the contributions and usage of GetDFPData, which is an open and free software for accessing corporate data from the Brazilian financial exchange, B3. The distribution and popularization of an open source algorithm for gathering and managing financial data can improve finance research and practice in two ways. First, by boosting the number and quality of research in accounting and corporate finance. Secondly, by providing retail investors with reliable data that may help their allocation decision. Initially, we analyzed the use of this kind of data in a list of recent publications to show the relevance of financial reports and corporate events data for research in the fields of accounting and finance. Finally, we illustrate the use of GetDFPData in large-scale research, an empirical and reproducible example of a corporate finance study.","tags":[],"title":"Accessing Financial Reports and Corporate Events with GetDFPData","type":"publication"},{"authors":null,"categories":["R","afedR"],"content":"  Back in 2017 I wrote the first international1 edition of my book “Analyzing Financial and Economic Data with R” (online version) . While I was happy with the content of the book at the time of publication, today I know I can make it better. As of early 2019, I’m working in the new edition of the book, taking my time (and weekends!) in fixing all issues, expanding chapters and writing new CRAN packages.\nThe current TOC is available here. Let me summarize the main changes from the previous edition:\n New content with the tidyverse  Total alignment with the tidyverse. Some base function are presented but priority is for readr, ggplot2, dplyr, stringr, purrr and so on. 100+ pages of new content (a 25% overall increase from previous edition).   Teaching Material  Static end of chapter exercises, with solutions publicly available in the internet; Slides for each chapter available in the internet; Dynamic 90+ exercises with the exams package. This means you can create and grade randomized unique tests for your students (more on this in a future post);   Book package afedR  This package makes it easier to import book datasets, copy all content files and reproduce all code in the book. Available in GitHub only.   Three new chapters  Cleaning and Structuring Financial and Economic Data – How to clean financial and economic data by dealing with long/wide dataframes, outlier detection/removal and desinflating prices and indexes. Reporting Results – Using xtable and texreg to report tables and models. Includes a special section on RMarkdown. Optimizing Code – Profiling code for bottlenecks and using vectorization, rcpp and memoise to speed up R computations.   Two new packages  simfinR – grabs data from the SimFin project; GetQuandlData– uses Quandl json api and caching for easier and faster data importation;    Right now, I could use some feedback from the community. Have a look in the TOC and let me know what you liked or disliked and if I missed something about using R in finance and economics. You can reach me in my email (marceloperlin@gmail.com) or using the comment section of this post.\nMy expectation is to finish the book in early 2020. If you want to be notified when I release it, fill up this form and I’ll email you when the book becomes available in Amazon.\nThis book is a special and life-long project. I plan to keep improving it as long as I can. As for access to the content, I’ll follow the same pricing structure from previous edition: the ebook will sell for 9.99 USD in Amazon, the online version will have the first 6 chapters for free in the internet (see previous edition here.\n I also wrote a local version. written in portuguese.↩︎\n   ","date":1574640000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574640000,"objectID":"9e7472fa272ea88ed3af06632052ff08","permalink":"http://www.msperlin.com/blog/post/2019-11-25-feedback-toc-afedr/","publishdate":"2019-11-25T00:00:00Z","relpermalink":"/blog/post/2019-11-25-feedback-toc-afedr/","section":"post","summary":"Back in 2017 I wrote the first international1 edition of my book “Analyzing Financial and Economic Data with R” (online version) . While I was happy with the content of the book at the time of publication, today I know I can make it better. As of early 2019, I’m working in the new edition of the book, taking my time (and weekends!) in fixing all issues, expanding chapters and writing new CRAN packages.\nThe current TOC is available here. Let me summarize the main changes from the previous edition:","tags":["R","afedR"],"title":"Feedback on new book TOC (Table of Contents)","type":"post"},{"authors":null,"categories":["R","simfinR"],"content":"  Introduction In my latest post I wrote about package GetEdgarData, which downloaded structured data from the SEC. I’ve been working on this project and soon realized that the available data at the SEC/DERA section is not complete. For example, all Q4 statements are missing. This seems to be the way all exchanges release the financial documents. I’ve found the same problem here in the Brazilian exchange.\nIt came to my attention that there is an alternative way of fetching corporate data and adjusted prices, the SimFin project. From its own website:\nOur core goal is to make financial data as freely available as possible because we believe that having the right tools for investing/research shouldn\u0026#39;t be the privilege of those that can afford to spend thousands of dollars per year on data. The platform is free with a daily limit of 2000 api calls. This is not bad and should suffice for most users. If you need more calls, the premium version is just 10 euros a month, a fraction of what other data vendors usually request.\nPackage simfinR, available in Github and soon in CRAN, facilitates all calls to the simfin API. It first makes sure the requested data exists and only then calls the api. As usual, all api queries are saved locally using package memoise. This means that the second time you ask for a particular data about a company/year, the function will load a local copy, and will not call the web api.\nPackage GetEdgarData, however, will be discontinued. I’ll keep the files in Github but will no longer develop it. It takes a lot of time to write and maintain R packages, and I fell that simfinR has far more potential.\nAs mentioned before, both new packages, GetQuandlData and simfinR will be part of my next book, “Analyzing Financial and Economic Data with R”, which should be released in early 2020.\n Installation # not in CRAN yet (need to test it further) #install.packages(\u0026#39;simfinR\u0026#39;) # from Github devtools::install_github(\u0026#39;msperlin/simfinR\u0026#39;) Example 01 - Apples Quarterly Net Profit The first step in using simfinR is finding information about available companies:\nlibrary(simfinR) library(tidyverse) # You need to get your own api key at https://simfin.com/ my_api_key \u0026lt;- readLines(\u0026#39;~/Dropbox/98-pass_and_bash/.api_key_simfin.txt\u0026#39;) # get info df_info_companies \u0026lt;- simfinR_get_available_companies(my_api_key) # check it glimpse(df_info_companies) ## Rows: 2,760 ## Columns: 3 ## $ simId \u0026lt;int\u0026gt; 171401, 901704, 901866, 45730, 378251, 987611, 896477, 418866,… ## $ ticker \u0026lt;chr\u0026gt; \u0026quot;ZYXI\u0026quot;, \u0026quot;ZYNE\u0026quot;, \u0026quot;ZVO\u0026quot;, \u0026quot;ZUMZ\u0026quot;, \u0026quot;ZTS\u0026quot;, \u0026quot;ZSAN\u0026quot;, \u0026quot;ZS\u0026quot;, \u0026quot;ZNGA\u0026quot;, \u0026quot;Z… ## $ name \u0026lt;chr\u0026gt; \u0026quot;ZYNEX INC\u0026quot;, \u0026quot;Zynerba Pharmaceuticals, Inc.\u0026quot;, \u0026quot;Zovio Inc\u0026quot;, \u0026quot;Zu… We find information about 2760 companies. Digging deeper we find that the simfin id of Apple is 111052. Let’s use it to download the annual financial information since 2009.\nid_companies \u0026lt;- 111052 # id of APPLE INC type_statements \u0026lt;- \u0026#39;pl\u0026#39; # profit/loss periods = \u0026#39;FY\u0026#39; # final year years = 2009:2018 df_fin_FY \u0026lt;- simfinR_get_fin_statements(id_companies, type_statements = type_statements, periods = periods, year = years, api_key = my_api_key) glimpse(df_fin_FY) ## Rows: 580 ## Columns: 13 ## $ company_name \u0026lt;chr\u0026gt; \u0026quot;APPLE INC\u0026quot;, \u0026quot;APPLE INC\u0026quot;, \u0026quot;APPLE INC\u0026quot;, \u0026quot;APPLE INC\u0026quot;, \u0026quot;A… ## $ company_sector \u0026lt;chr\u0026gt; \u0026quot;Computer Hardware\u0026quot;, \u0026quot;Computer Hardware\u0026quot;, \u0026quot;Computer Ha… ## $ type_statement \u0026lt;fct\u0026gt; pl, pl, pl, pl, pl, pl, pl, pl, pl, pl, pl, pl, pl, pl… ## $ period \u0026lt;fct\u0026gt; FY, FY, FY, FY, FY, FY, FY, FY, FY, FY, FY, FY, FY, FY… ## $ year \u0026lt;int\u0026gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, … ## $ ref_date \u0026lt;date\u0026gt; 2009-12-31, 2009-12-31, 2009-12-31, 2009-12-31, 2009-… ## $ acc_name \u0026lt;chr\u0026gt; \u0026quot;Revenue\u0026quot;, \u0026quot;Sales \u0026amp; Services Revenue\u0026quot;, \u0026quot;Financing Reve… ## $ acc_value \u0026lt;dbl\u0026gt; 4.2905e+10, NA, NA, NA, -2.5683e+10, NA, NA, NA, 1.722… ## $ tid \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;6\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;7\u0026quot;, \u0026quot;8\u0026quot;, \u0026quot;9\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;10\u0026quot;, \u0026quot;11… ## $ uid \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;10\u0026quot;, \u0026quot;11… ## $ parent_tid \u0026lt;chr\u0026gt; \u0026quot;4\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;19\u0026quot;, \u0026quot;19\u0026quot;, \u0026quot;1… ## $ display_level \u0026lt;chr\u0026gt; \u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;,… ## $ check_possible \u0026lt;lgl\u0026gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… And now we plot the results of the “Net Income” (profit/loss) for all years:\nnet_income \u0026lt;- df_fin_FY %\u0026gt;% filter(acc_name == \u0026#39;Net Income\u0026#39;) p \u0026lt;- ggplot(net_income, aes(x = ref_date, y = acc_value)) + geom_col() + labs(title = \u0026#39;Yearly Profit of APPLE INC\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;Yearly Profit\u0026#39;, subtitle = \u0026#39;\u0026#39;, caption = \u0026#39;Data from simfin \u0026lt;https://simfin.com/\u0026gt;\u0026#39;) + theme_bw() print(p) Not bad!\nWe can also grab data for all quarters:\ntype_statements \u0026lt;- \u0026#39;pl\u0026#39; # profit/loss periods = c(\u0026#39;Q1\u0026#39;, \u0026#39;Q2\u0026#39;, \u0026#39;Q3\u0026#39;, \u0026#39;Q4\u0026#39;) # final year years = 2009:2018 df_fin_quarters \u0026lt;- simfinR_get_fin_statements(id_companies, type_statements = type_statements, periods = periods, year = years, api_key = my_api_key) glimpse(df_fin_quarters) ## Rows: 2,320 ## Columns: 13 ## $ company_name \u0026lt;chr\u0026gt; \u0026quot;APPLE INC\u0026quot;, \u0026quot;APPLE INC\u0026quot;, \u0026quot;APPLE INC\u0026quot;, \u0026quot;APPLE INC\u0026quot;, \u0026quot;A… ## $ company_sector \u0026lt;chr\u0026gt; \u0026quot;Computer Hardware\u0026quot;, \u0026quot;Computer Hardware\u0026quot;, \u0026quot;Computer Ha… ## $ type_statement \u0026lt;fct\u0026gt; pl, pl, pl, pl, pl, pl, pl, pl, pl, pl, pl, pl, pl, pl… ## $ period \u0026lt;fct\u0026gt; Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1, Q1… ## $ year \u0026lt;int\u0026gt; 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, … ## $ ref_date \u0026lt;date\u0026gt; 2009-03-31, 2009-03-31, 2009-03-31, 2009-03-31, 2009-… ## $ acc_name \u0026lt;chr\u0026gt; \u0026quot;Revenue\u0026quot;, \u0026quot;Sales \u0026amp; Services Revenue\u0026quot;, \u0026quot;Financing Reve… ## $ acc_value \u0026lt;dbl\u0026gt; 1.188e+10, NA, NA, NA, -7.373e+09, NA, NA, NA, 4.507e+… ## $ tid \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;5\u0026quot;, \u0026quot;6\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;7\u0026quot;, \u0026quot;8\u0026quot;, \u0026quot;9\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;10\u0026quot;, \u0026quot;11… ## $ uid \u0026lt;chr\u0026gt; \u0026quot;1\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;10\u0026quot;, \u0026quot;11… ## $ parent_tid \u0026lt;chr\u0026gt; \u0026quot;4\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;4\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;2\u0026quot;, \u0026quot;19\u0026quot;, \u0026quot;19\u0026quot;, \u0026quot;1… ## $ display_level \u0026lt;chr\u0026gt; \u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;1\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;, \u0026quot;0\u0026quot;,… ## $ check_possible \u0026lt;lgl\u0026gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… And plot the results:\nnet_income \u0026lt;- df_fin_quarters %\u0026gt;% filter(acc_name == \u0026#39;Net Income\u0026#39;) p \u0026lt;- ggplot(net_income, aes(x = period, y = acc_value)) + geom_col() + facet_grid(~year, scales = \u0026#39;free\u0026#39;) + labs(title = \u0026#39;Quarterly Profit of APPLE INC\u0026#39;, x = \u0026#39;Quarters\u0026#39;, y = \u0026#39;Net Profit\u0026#39;) + theme_bw() print(p) Nice and impressive profit record. The first quarter (Q1) seems to present the best performance, probably due to end of year holidays.\n Example 02 - Quarterly Net Profit of Many Companies Package simfinR can also fetch information for many companies in a single call. Let’s run another example by selecting four random companies and creating the same previous graph:\nset.seed(5) my_ids \u0026lt;- sample(df_info_companies$simId, 4) type_statements \u0026lt;- \u0026#39;pl\u0026#39; # profit/loss periods = \u0026#39;FY\u0026#39; # final year years = 2010:2018 df_fin \u0026lt;- simfinR_get_fin_statements(id_companies = my_ids, type_statements = type_statements, periods = periods, year = years, api_key = my_api_key) net_income \u0026lt;- df_fin %\u0026gt;% filter(acc_name == \u0026#39;Net Income\u0026#39;) p \u0026lt;- ggplot(net_income, aes(x = ref_date, y = acc_value)) + geom_col() + labs(title = \u0026#39;Annual Profit/Loss of Four Companies\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;Net Profit/Loss\u0026#39;) + facet_wrap(~company_name, scales = \u0026#39;free_y\u0026#39;) + theme_bw() print(p)  Example 03: Fetching price data The simfin project also provides adjusted prices of stocks. Have a look:\nset.seed(5) my_ids \u0026lt;- sample(df_info_companies$simId, 4) type_statements \u0026lt;- \u0026#39;pl\u0026#39; # profit/loss periods = \u0026#39;FY\u0026#39; # final year years = 2009:2018 df_price \u0026lt;- simfinR_get_price_data(id_companies = my_ids, api_key = my_api_key) p \u0026lt;- ggplot(df_price, aes(x = ref_date, y = close_adj)) + geom_line() + labs(title = \u0026#39;Adjusted stock prices for four companies\u0026#39;, x = \u0026#39;\u0026#39;, y = \u0026#39;Adjusted Stock Prices\u0026#39;) + facet_wrap(~company_name, scales = \u0026#39;free_y\u0026#39;) + theme_bw() print(p) As you can see, the data is comprehensive and should suffice for many different corporate finance research topics.\nGive it a try and, if you’ve found any problem or bug, please let me know at marceloperlin@gmail.com.\n  ","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"ed66c90ab45f887b94c77861bdd61f18","permalink":"http://www.msperlin.com/blog/post/2019-11-01-new-package-simfinr/","publishdate":"2019-11-01T00:00:00Z","relpermalink":"/blog/post/2019-11-01-new-package-simfinr/","section":"post","summary":"Introduction In my latest post I wrote about package GetEdgarData, which downloaded structured data from the SEC. I’ve been working on this project and soon realized that the available data at the SEC/DERA section is not complete. For example, all Q4 statements are missing. This seems to be the way all exchanges release the financial documents. I’ve found the same problem here in the Brazilian exchange.\nIt came to my attention that there is an alternative way of fetching corporate data and adjusted prices, the SimFin project. From its own website:","tags":["R","simfinR"],"title":"New package: simfinR","type":"post"},{"authors":null,"categories":["R","GetEdgarData"],"content":"  Introduction As of 2019-10-31, this package is discontinued and will not longer be updated. See this post for more details about the alternative, package simfinR.\nEvery company traded in the US stock market must report its quarterly and yearly documents to the SEC and the public in general. This includes its accounting statements (10-K, 10-K) and any other corporate event that is relevant to investors.\nEdgar is the interface where we can search for a company’s filling information. By looking up a company’s CIK code, one can find all previous filling information. A complete list of available forms can be found in this link.\nPackage GetEdgarData allows the user import the financial documents from such fillings directly into R. Unlike other packages, the information is not taken from the filling’s xml files, but the structured datasets at the DERA (Division of Economic and Risk Analysis) section . This means we can import a large amount of structured financial data very quickly. The downside is that the available data starts at 2009.\nLike many other packages I’ve wrote for data grabbing, the queries are saved locally using package memoise. This means that the second time you ask for a particular year of data, the function will load a local copy, and will not download the data again.\nBoth new packages, GetEdgarData and GetQuandlData (blog post) are going to be part of the second edition of my book “Analyzing Financial Data with R” (see first edition here). My expectation is to publish the new book in early 2020.\n Installation # not in CRAN yet (need to test it further) #install.packages(\u0026#39;GetEdgarData\u0026#39;) # from github devtools::install_github(\u0026#39;msperlin/GetEdgarData\u0026#39;) Example 01 - Apples Quarterly Net Profit The first step in using GetEdgarData is finding information about available companies:\nlibrary(GetEdgarData) library(tidyverse) my_year \u0026lt;- 2018 type_form \u0026lt;- \u0026#39;10-K\u0026#39; df_info \u0026lt;- get_info_companies(years = my_year, type_data = \u0026#39;yearly\u0026#39;, type_form = type_form) glimpse(df_info) We find information about nrow(df_info) companies for the type_form documents in the year of my_year. Digging deeper we find that the official name of Apple is ‘APPLE INC’. Let’s use it to download the financial information since 2009.\nmy_company \u0026lt;- \u0026#39;APPLE INC\u0026#39; my_years \u0026lt;- 2009:2018 type_data \u0026lt;- \u0026#39;quarterly\u0026#39; df_fin_reports \u0026lt;- get_edgar_fin_data(companies = my_company, years = my_years, type_data = type_data) glimpse(df_fin_reports) And now we filter for the net income (id tag = ‘NetIncomeLoss’) and plot the resulting dataframe:\nnet_income \u0026lt;- df_fin_reports %\u0026gt;% filter(tag == \u0026#39;NetIncomeLoss\u0026#39;) p \u0026lt;- ggplot(net_income, aes(x = ref_date, y = value_ref)) + geom_col() + labs(title = \u0026#39;APPLE Quarterly Net Income (10-Q)\u0026#39;, subtitle = paste0(min(my_years), \u0026#39; - \u0026#39;, max(my_years)), x = \u0026#39;\u0026#39;, y = \u0026#39;Net Income ($)\u0026#39;, caption = paste0(\u0026#39;Data from EDGAR \u0026lt;https://www.sec.gov/edgar/searchedgar/companysearch.html\u0026gt;\u0026#39;, \u0026#39;\\n\u0026#39;, \u0026#39;Downloaded with package GetEdgarData\u0026#39;) ) print(p)  Example 02 - Quarterly Net Profit of Many Companies The package is really handy for fetching information for many companies. This is due to the fact that the SEC/DERA stores data of all companies by year and the package creates a local cache of the resulting data. This means that, by fetching data for one company, we indirectly have information for all companies.\nLet’s see an example by selecting four random companies and creating the same previous graph:\nset.seed(5) my_companies \u0026lt;- sample(df_info$current_name, 4) my_years \u0026lt;- 2009:2018 type_data \u0026lt;- \u0026#39;quarterly\u0026#39; net_income \u0026lt;- get_edgar_fin_data(companies = my_companies, years = my_years, type_data = type_data) %\u0026gt;% filter(tag == \u0026#39;NetIncomeLoss\u0026#39;) p \u0026lt;- ggplot(net_income, aes(x = ref_date, y = value_ref)) + geom_col() + facet_wrap(~current_name, scales = \u0026#39;free\u0026#39;) + labs(title = \u0026#39;Quarterly Net Income for Four Random companies\u0026#39;, subtitle = paste0(min(my_years), \u0026#39; - \u0026#39;, max(my_years)), x = \u0026#39;\u0026#39;, y = \u0026#39;Net Income ($)\u0026#39;, caption = paste0(\u0026#39;Data from EDGAR \u0026lt;https://www.sec.gov/edgar/searchedgar/companysearch.html\u0026gt;\u0026#39;, \u0026#39;\\n\u0026#39;, \u0026#39;Downloaded in R with package GetEdgarData\u0026#39;) ) print(p) Give it a try and, if you’ve found any problem or bug, let me know at marceloperlin@gmail.com.\n  ","date":1571184000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571184000,"objectID":"6d12933be956fdc86e8c93704a3fa645","permalink":"http://www.msperlin.com/blog/post/2019-10-16-new-package-getedgardata/","publishdate":"2019-10-16T00:00:00Z","relpermalink":"/blog/post/2019-10-16-new-package-getedgardata/","section":"post","summary":"Introduction As of 2019-10-31, this package is discontinued and will not longer be updated. See this post for more details about the alternative, package simfinR.\nEvery company traded in the US stock market must report its quarterly and yearly documents to the SEC and the public in general. This includes its accounting statements (10-K, 10-K) and any other corporate event that is relevant to investors.\nEdgar is the interface where we can search for a company’s filling information. By looking up a company’s CIK code, one can find all previous filling information.","tags":["R","GetEdgarData"],"title":"New package: GetEdgarData","type":"post"},{"authors":null,"categories":["R","GetDFPData"],"content":"  The shiny version of GetDFPData is currently hosted in a private server at DigitalOcean. A problem with the basic (5 USD) server I was using is with the low amount of available memory (RAM and HD). With that, I had to limit all xlsx queries for the data, otherwise the shiny app would ran out of memory. After upgrading R in the server, the xlsx option was no longer working.\nToday I tried all tricks in the book for keeping the 5 USD server and get the code to work. Nothing worked effectively. The Microsoft Excel is a very restrictive format, and you should only use it to small projects. If the volume of data is high, as in GetDFPData, you’re going to run into a lot of issues of cell sizes and memory allocation. Despite my explicit recommendation to avoid Excel format as much as possible, people still use it a lot. Not surprisingly, once I took the “xlsx” option from the shiny interface, people complained to my email – a lot.\nI just upgraded the RAM and HD of the server in DigitalOcean. The xlsx option is back and working. The new bill is 10 USD per month. So far I’ve been paying the bill from my own pocket, using revenues from my books. The GetDFPData has no official financial support and yes, I’ll continue to finance it as much as can. But, support from those using the shiny interface of the CRAN package is very much welcomed and will motive further development to keep things running smoothly.\nIf you can, please help donating a small value and keeping the server financed. Once I reach 12 months of payed bills (around 120 USD), I’ll remove the Paypal donation button and only add it back after the cash runs out.\n","date":1570838400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570838400,"objectID":"bdbcc52cf928821c545a9a8d9e22f3b4","permalink":"http://www.msperlin.com/blog/post/2019-10-12-support-getdfpdata-shiny/","publishdate":"2019-10-12T00:00:00Z","relpermalink":"/blog/post/2019-10-12-support-getdfpdata-shiny/","section":"post","summary":"The shiny version of GetDFPData is currently hosted in a private server at DigitalOcean. A problem with the basic (5 USD) server I was using is with the low amount of available memory (RAM and HD). With that, I had to limit all xlsx queries for the data, otherwise the shiny app would ran out of memory. After upgrading R in the server, the xlsx option was no longer working.\nToday I tried all tricks in the book for keeping the 5 USD server and get the code to work.","tags":["R","GetDFPData"],"title":"Help support GetDFPData","type":"post"},{"authors":null,"categories":["R","GetQuandlData"],"content":"  Introduction Quandl is one of the best platforms for finding and downloading financial and economic time series. The collection of free databases is solid and I use it intensively in my research and class material.\nBut, a couple of things from the native package Quandl always bothered me:\n Multiple data is always returned in the wide (column oriented) format (why??); No local caching of data; No control for importing error and status; Not easy to work within the tidyverse collection of packages  As you suspect, I decided to tackle the problem over the weekend. The result is package GetQuandlData. This is what it does differently:\n It uses the json api (and not the Quandl native function), so that some metadata is also returned; The resulting dataframe is always returned in the long format, even for multiple series; Users can set custom names for input series. This is very useful when using along ggplot or making tables; Uses package memoise to set a local caching system. This means that the second time you ask for a particular time series, it will grab it from your hard drive (and not the internet); Always compares the requested dates against dates available in the platform.   Installation # not in CRAN yet (need to test it further) #install.packages(\u0026#39;GetQuandlData\u0026#39;) # from github devtools::install_github(\u0026#39;msperlin/GetQuandlData\u0026#39;) Example 01 - Inflation in the US Let’s download and plot information about inflation in the US:\nlibrary(GetQuandlData) library(tidyverse) my_id \u0026lt;- c(\u0026#39;Inflation USA\u0026#39; = \u0026#39;YALE/SP_CPI\u0026#39;) my_api \u0026lt;- readLines(\u0026#39;~/Dropbox/98-pass_and_bash/.quandl_api.txt\u0026#39;) # you need your own API (get it at https://www.quandl.com/sign-up-modal?defaultModal=showSignUp\u0026gt;) first_date \u0026lt;- \u0026#39;2005-01-01\u0026#39; last_date \u0026lt;- Sys.Date() df \u0026lt;- get_Quandl_series(id_in = my_id, api_key = my_api, first_date = first_date, last_date = last_date, cache_folder = tempdir()) glimpse(df) As you can see, the data is in the long format. Let’s plot it:\nlibrary(tidyverse) p \u0026lt;- ggplot(df, aes(x = ref_date, y = value/100)) + geom_col() + labs(y = \u0026#39;Inflation (%)\u0026#39;, x = \u0026#39;\u0026#39;, title = \u0026#39;Inflation in the US\u0026#39;) + scale_y_continuous(labels = scales::percent) p Beautiful!\n Example 02 - Inflation for many countries Next, lets have a look into a more realistic case, where we need inflation data for several countries:\nFirst, we need to see what are the available datasets from database RATEINF:\nlibrary(GetQuandlData) library(tidyverse) db_id \u0026lt;- \u0026#39;RATEINF\u0026#39; my_api \u0026lt;- readLines(\u0026#39;~/Dropbox/98-pass_and_bash/.quandl_api.txt\u0026#39;) # you need your own API df \u0026lt;- get_database_info(db_id, my_api) knitr::kable(df) Nice. Now we only need to filter the series with YOY inflation:\nidx \u0026lt;- stringr::str_detect(df$name, \u0026#39;Inflation YOY\u0026#39;) df_series \u0026lt;- df[idx, ] and grab the data:\nmy_id \u0026lt;- df_series$quandl_code names(my_id) \u0026lt;- df_series$name first_date \u0026lt;- \u0026#39;2010-01-01\u0026#39; last_date \u0026lt;- Sys.Date() df_inflation \u0026lt;- get_Quandl_series(id_in = my_id, api_key = my_api, first_date = first_date, last_date = last_date) glimpse(df_inflation) And, finally, an elegant plot:\np \u0026lt;- ggplot(df_inflation, aes(x = ref_date, y = value/100)) + geom_col() + labs(y = \u0026#39;Inflation (%)\u0026#39;, x = \u0026#39;\u0026#39;, title = \u0026#39;Inflation in the World\u0026#39;, subtitle = paste0(first_date, \u0026#39; to \u0026#39;, last_date)) + scale_y_continuous(labels = scales::percent) + facet_wrap(~series_name) p   ","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569888000,"objectID":"f788936dd59b61f3cadd7b8ba1ff70e9","permalink":"http://www.msperlin.com/blog/post/2019-10-01-new-package-getquandldata/","publishdate":"2019-10-01T00:00:00Z","relpermalink":"/blog/post/2019-10-01-new-package-getquandldata/","section":"post","summary":"Introduction Quandl is one of the best platforms for finding and downloading financial and economic time series. The collection of free databases is solid and I use it intensively in my research and class material.\nBut, a couple of things from the native package Quandl always bothered me:\n Multiple data is always returned in the wide (column oriented) format (why??); No local caching of data; No control for importing error and status; Not easy to work within the tidyverse collection of packages  As you suspect, I decided to tackle the problem over the weekend.","tags":["R","GetQuandlData"],"title":"New package: GetQuandlData","type":"post"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"Slides  Link to slides\n","date":1567468800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567468800,"objectID":"232ef02a60c42273eee406ace1d8d39c","permalink":"http://www.msperlin.com/blog/talk/2019-09-03-tutorial-este/","publishdate":"2019-09-03T00:00:00Z","relpermalink":"/blog/talk/2019-09-03-tutorial-este/","section":"talk","summary":"Tutorial sobre o R.","tags":[],"title":"Tutorial-Importação online de séries temporais econômicas e financeiras usando R","type":"talk"},{"authors":null,"categories":["R","GetHFData"],"content":"  Update 2019-08-09: The shutdown is just postponed to 2019-11-14. See the official release here.\nSurprise, surprise. B3’s ftp site is still up and running.\nFollowing previous post regarding the shutdown of B3’s ftp site and its impact over GetHFData, I’m happy to report that the site is up and running.\nWe can check it with code:\nlibrary(GetHFData) library(tidyverse) df.ftp \u0026lt;- ghfd_get_ftp_contents(type.market = \u0026#39;equity\u0026#39;) # check time difference max(df.ftp$dates) - min(df.ftp$dates) Let’s download some trade data:\ndf.trades \u0026lt;- ghfd_get_HF_data(my.assets = \u0026#39;PETR3\u0026#39;, type.market = \u0026#39;equity\u0026#39;, first.date = max(df.ftp$dates)-3, last.date = max(df.ftp$dates), type.data = \u0026#39;trades\u0026#39;, type.output = \u0026#39;agg\u0026#39;, first.time = \u0026#39;11:00:00\u0026#39;, last.time = \u0026#39;18:00:00\u0026#39;, dl.dir = tempdir() ) And check it out:\nglimpse(df.trades) Its working fine. The amount of data available at the ftp is more than necessary for research and class material.\nI’m not really sure what happened. It could be a simple delay to the shutdown. Lets keep our fingers crossed.\n","date":1565222400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565222400,"objectID":"3e6d0e080c8bcda1900e93fbd6ab7b91","permalink":"http://www.msperlin.com/blog/post/2019-08-08-ftp-not-shutdown/","publishdate":"2019-08-08T00:00:00Z","relpermalink":"/blog/post/2019-08-08-ftp-not-shutdown/","section":"post","summary":"Update 2019-08-09: The shutdown is just postponed to 2019-11-14. See the official release here.\nSurprise, surprise. B3’s ftp site is still up and running.\nFollowing previous post regarding the shutdown of B3’s ftp site and its impact over GetHFData, I’m happy to report that the site is up and running.\nWe can check it with code:\nlibrary(GetHFData) library(tidyverse) df.ftp \u0026lt;- ghfd_get_ftp_contents(type.market = \u0026#39;equity\u0026#39;) # check time difference max(df.ftp$dates) - min(df.ftp$dates) Let’s download some trade data:\ndf.trades \u0026lt;- ghfd_get_HF_data(my.assets = \u0026#39;PETR3\u0026#39;, type.market = \u0026#39;equity\u0026#39;, first.date = max(df.ftp$dates)-3, last.date = max(df.","tags":["R","GetHFData"],"title":"B3 is NOT shutting down its ftp site, for now..","type":"post"},{"authors":null,"categories":null,"content":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as\nimport pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head()  Math Academic supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.toml file.\nTo render inline or block math, wrap your LaTeX math with $...$ or $$...$$, respectively.\nExample math block:\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |} {\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$  renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left |\\nabla F(\\mathbf{x}{n}) - \\nabla F(\\mathbf{x}{n-1}) \\right |^2}$$\nExample inline math $\\nabla F(\\mathbf{x}_{n})$ renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the \\\\ math linebreak:\n$$f(k;p_0^*) = \\begin{cases} p_0^* \u0026amp; \\text{if }k=1, \\\\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$  renders as\n$$f(k;p_0^) = \\begin{cases} p_0^ \u0026amp; \\text{if }k=1, \\ 1-p_0^* \u0026amp; \\text {if }k=0.\\end{cases}$$\nDiagrams Academic supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ```  renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2]  An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ```  renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good!  An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ```  renders as\ngantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d  An example class diagram:\n```mermaid classDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() } ```  renders as\nclassDiagram Class01 \u0026lt;|-- AveryLongClass : Cool \u0026lt;\u0026lt;interface\u0026gt;\u0026gt; Class01 Class09 --\u0026gt; C2 : Where am i? Class09 --* C3 Class09 --|\u0026gt; Class07 Class07 : equals() Class07 : Object[] elementData Class01 : size() Class01 : int chimp Class01 : int gorilla class Class10 { \u0026lt;\u0026lt;service\u0026gt;\u0026gt; int id size() }  An example state diagram:\n```mermaid stateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*] ```  renders as\nstateDiagram [*] --\u0026gt; Still Still --\u0026gt; [*] Still --\u0026gt; Moving Moving --\u0026gt; Still Moving --\u0026gt; Crash Crash --\u0026gt; [*]  Todo lists You can even write your todo lists in Academic too:\n- [x] Write math example - [x] Write diagram example - [ ] Do something else  renders as\n Write math example Write diagram example Do something else  Tables Represent your data in tables:\n| First Header | Second Header | | ------------- | ------------- | | Content Cell | Content Cell | | Content Cell | Content Cell |  renders as\n   First Header Second Header     Content Cell Content Cell   Content Cell Content Cell    Asides Academic supports a shortcode for asides, also referred to as notices, hints, or alerts. By wrapping a paragraph in {{% alert note %}} ... {{% /alert %}}, it will render as an aside.\n{{% alert note %}} A Markdown aside is useful for displaying notices, hints, or definitions to your readers. {{% /alert %}}  renders as\n A Markdown aside is useful for displaying notices, hints, or definitions to your readers.   Icons Academic enables you to use a wide range of icons from Font Awesome and Academicons in addition to emojis.\nHere are some examples using the icon shortcode to render icons:\n{{\u0026lt; icon name=\u0026quot;terminal\u0026quot; pack=\u0026quot;fas\u0026quot; \u0026gt;}} Terminal {{\u0026lt; icon name=\u0026quot;python\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} Python {{\u0026lt; icon name=\u0026quot;r-project\u0026quot; pack=\u0026quot;fab\u0026quot; \u0026gt;}} R  renders as\n  Terminal\n Python\n R\nDid you find this page helpful? Consider sharing it 🙌 ","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"http://www.msperlin.com/blog/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/blog/post/writing-technical-content/","section":"post","summary":"Academic is designed to give technical content creators a seamless experience. You can focus on the content and Academic handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you\u0026rsquo;ll find some examples of the types of technical content that can be rendered with Academic.\nExamples Code Academic supports a Markdown extension for highlighting code syntax. You can enable this feature by toggling the highlight option in your config/_default/params.toml file.\n```python import pandas as pd data = pd.read_csv(\u0026quot;data.csv\u0026quot;) data.head() ```  renders as","tags":null,"title":"Writing technical content in Academic","type":"post"},{"authors":["Marcelo S. Perlin","Guilherme Kirch","Daniel Vancin","Mauro Mastella"],"categories":null,"content":"Slides  ","date":1562198400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562198400,"objectID":"2b0ac6bf4182e0b85442ed9c63ddf681","permalink":"http://www.msperlin.com/blog/talk/2019-07-04-ebfin-boards-and-lattes/","publishdate":"2019-07-04T00:00:00Z","relpermalink":"/blog/talk/2019-07-04-ebfin-boards-and-lattes/","section":"talk","summary":"Apresentação de artigo no XIX EBFIN, Rio de Janeiro.","tags":[],"title":"Acadêmicos Compensam? O Efeito da Titulação e Produção Cientı́fica no Desempenho das Empresas Brasileiras","type":"talk"},{"authors":null,"categories":["R","GetHFData"],"content":"  Well, bad news travels fast. \nOver the last couple of weeks I’ve been receiving a couple of emails regarding B3’s decision of shutting down its ftp site. More specifically, users are eager to know how it will impact my data grabbing packages in CRAN. I’ll use this post to explain the situation for everyone.\nThe only package affected directly will be GetHFData, which uses the ftp site for downloading the raw zipped files with trades and quotes. The main function will no longer work as all internet files are not available. However, the function that reads the local files, GetHFData::ghfd_read_file(), will still work as long as you have the files available in your computer.\nSoon I’ll release an update to GetHFData that will bypass the ftp checking process. Users will be able to load up the code with local files. Btw, before anyone asks, I’m not aware of any other site that distributes the zipped files. In this topic, everyone should know that B3’s web policy does not allow the redistribution of their data.\nAs for my personal opinion on the event, B3 is a private company and can do whatever they want with their data. In fact, it is standard for many international exchanges to sell their high-frequency trade\u0026amp;quote data. However, I fell that they could still offer a free sample of past raw data for students and researchers, keeping GetHFData alive. I’m not sure how this would hurt their business. In fact, it is easy to argue that the “free” training would help them.\nOn the research side, studying microstructure of the Brazilian financial market will become even more difficult now, without easy access to the datasets.\nAs a clever stoic man once said, we should only worry about things we can control.\nLife (and research) goes on..\n","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"e8f744c4d77403d2ed114e737a934869","permalink":"http://www.msperlin.com/blog/post/2019-07-01-ftp-shutdown/","publishdate":"2019-07-01T00:00:00Z","relpermalink":"/blog/post/2019-07-01-ftp-shutdown/","section":"post","summary":"Well, bad news travels fast. \nOver the last couple of weeks I’ve been receiving a couple of emails regarding B3’s decision of shutting down its ftp site. More specifically, users are eager to know how it will impact my data grabbing packages in CRAN. I’ll use this post to explain the situation for everyone.\nThe only package affected directly will be GetHFData, which uses the ftp site for downloading the raw zipped files with trades and quotes. The main function will no longer work as all internet files are not available.","tags":["R","GetHFData"],"title":"B3 is shutting down its ftp site","type":"post"},{"authors":["Marcelo S. Perlin","Marcelo Brutti Righi","Tiago P. Filomena"],"categories":null,"content":"","date":1560816000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560816000,"objectID":"53c571bcfe2aaf70c7225878f2b94672","permalink":"http://www.msperlin.com/blog/publication/2019_jcr-bar/","publishdate":"2019-06-18T00:00:00Z","relpermalink":"/blog/publication/2019_jcr-bar/","section":"publication","summary":"This paper introduces an approach designed for personal credit risk, with possible applications in risk assessment and optimization of debt contracts. We define a structural model related to the financial balance of an individual, allowing for cashflow seasonality and deterministic trends in the process. Based on the proposed model, we develop risk measures associated with the probability of default rates conditional on time. This formulation is best suited to short-term loans, where the dynamics of individuals’ cashflow, such as seasonality and uncertainty, can significantly impact future default rates. In the empirical section of this paper, we illustrate an application by estimating risk measures using simulated data. We also present the specific case of optimization of a financial contract, where, based on an estimated model, we find the yield rate/time to maturity pair that maximizes the expected profit or minimizes the default risk of a short-term debt contract.","tags":[],"title":"A consumer credit risk structural model based on affordability: balance at risk","type":"publication"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"","date":1560211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560211200,"objectID":"20d208f9de927ded8d0c89a48c56eb00","permalink":"http://www.msperlin.com/blog/talk/2019-06-11-sobre-r/","publishdate":"2019-06-11T00:00:00Z","relpermalink":"/blog/talk/2019-06-11-sobre-r/","section":"talk","summary":"Palestra sobre a plataforma R e os seus usos.","tags":[],"title":"A plataforma R","type":"talk"},{"authors":null,"categories":["R","investments","lindy effect"],"content":"  One of the investment concepts that every long term investor should know is the effect of consistency over corporate performance. The main idea is that older and profitable companies are likely to continue to be profitable and even improve its performance in the upcoming years. Likewise, companies with constant losses are likely to continue in the same path.\nThis idea is related to the Lindy Effect. Quoting directly from wikipedia:\n The Lindy effect is a theory that the future life expectancy of some non-perishable things like a technology or an idea is proportional to their current age, so that every additional period of survival implies a longer remaining life expectancy.\n As you should suspect by now, I am going to test this idea by looking at the predictive effect of consistent net profits/losses for companies traded at B3, the Brazilian financial exchange. First, let’s import the data and take a glimpse at it.\nlibrary(tidyverse) library(readxl) my.f \u0026lt;- \u0026#39;~/Dropbox/03-Computer Code/01-R Code/02-Finance Code/Lindy effect on profit/data/LL.xlsx\u0026#39; df \u0026lt;- read_excel(my.f) %\u0026gt;% glimpse() ## Rows: 6,592 ## Columns: 3 ## $ id \u0026lt;chr\u0026gt; \u0026quot;AALR\u0026quot;, \u0026quot;AALR\u0026quot;, \u0026quot;AALR\u0026quot;, \u0026quot;AALR\u0026quot;, \u0026quot;AALR\u0026quot;, \u0026quot;ABCB\u0026quot;, \u0026quot;ABCB\u0026quot;, \u0026quot;A… ## $ year \u0026lt;dbl\u0026gt; 2014, 2015, 2016, 2017, 2018, 2004, 2005, 2006, 2007, 2008… ## $ net_income \u0026lt;dbl\u0026gt; -4, -11, 29, 15, 52, 53, 58, 61, 83, 137, 151, 202, 235, 2… The structure is straightforward. The data was obtained from a financial portal1 and already organized in a long format, saving myself from the work of restructuring it. As for the columns, it is all very basics. We have company’s id, year and net income.\nNext I’ll write a function that will do all the dirty work. The idea is to calculate, for each year/company/horizon, the cases where we find a particular result based on \\(k\\) results from the past. Confusing right? Let me try again. For example, let’s say you observe at a particular time \\(t\\) that a company performed the five past years with profits. What we want to know is if such a information can predict the profit in the next year.\nIn other terms, conditional on the information about past results, what is the likelihood that the next net income will also be positive? By answering this question for every possible horizon, we can build a figure that relates the probability with the time consistency. If the Lindy effect is true for companies, we should see a positive association, that is, the longer the horizon of consecutive results, higher the chances of the same result.\nThe code for the function is set below.\nfct_prob_calc_LL \u0026lt;- function(y, years, company) { require(tidyverse) nT \u0026lt;- length(y) df.res \u0026lt;- tibble() for (i.year in 2:length(years)) { lags.to.test \u0026lt;- 1:(i.year-1) for (i.lag in lags.to.test) { test.vec \u0026lt;- y[(i.year-i.lag):(i.year-1)] my.test1\u0026lt;- all(test.vec \u0026gt; 0) my.result1 \u0026lt;- y[i.year] \u0026gt; 0 my.test2\u0026lt;- all(test.vec \u0026lt; 0) my.result2 \u0026lt;- y[i.year] \u0026lt; 0 my.test3\u0026lt;- all(na.omit(diff(test.vec) \u0026gt; 0)) my.result3 \u0026lt;- y[i.year] - y[i.year-1] \u0026gt; 0 my.test4\u0026lt;- all(na.omit(diff(test.vec) \u0026lt; 0)) my.result4 \u0026lt;- y[i.year] - y[i.year-1] \u0026lt; 0 df.res \u0026lt;- bind_rows(df.res, tibble(company = company[1], year = years[i.year], horizon = i.lag, type.test = c(\u0026#39;Positive Net Income (profit)\u0026#39;, \u0026#39;Negative Net Income (loss)\u0026#39;, \u0026#39;Increase Net Income\u0026#39;, \u0026#39;Decrease Net Income\u0026#39;), test.flag = c(my.test1, my.test2, my.test3, my.test4), result = c(my.result1, my.result2, my.result3, my.result4)) ) } } return(df.res) } And now we use it for all companies:\nlibrary(purrr) library(furrr) l.args \u0026lt;- list(y = split(df$net_income, f = df$id), years = split(df$year, f = df$id), company = split(df$id, f = df$id)) plan(multisession(workers = 10)) # get results tab.test \u0026lt;- bind_rows(future_pmap(.l = l.args, .f = fct_prob_calc_LL, .progress = TRUE)) %\u0026gt;% mutate(years = factor(year) )  ## Progress: ──────────── 100% Progress: ─────────────────── 100% Progress: ──────────────────────────── 100% Progress: ───────────────────────────────────── 100% Progress: ───────────────────────────────────────────── 100% Progress: ───────────────────────────────────────────────── 100% Progress: ────────────────────────────────────────────────── 100% Progress: ────────────────────────────────────────────────── 100% Progress: ────────────────────────────────────────────────── 100% Progress: ─────────────────────────────────────────────────── 100% # build prob table tab \u0026lt;- tab.test %\u0026gt;% filter(test.flag == TRUE) %\u0026gt;% group_by(horizon, type.test) %\u0026gt;% summarise(prob = mean(result, na.rm = TRUE), n = n(), n.companies = length(unique(company))) And finally build the plot with ggplot2:\np \u0026lt;- ggplot(tab %\u0026gt;% filter(horizon \u0026lt;= 10), aes(x = horizon, y = prob, group = type.test)) + geom_point(aes(shape= type.test), size = 3) + geom_line(size = 0.75) + #facet_wrap(~type.test, nrow = 2) + labs(x = \u0026#39;Number of consecutive years\u0026#39;, y = \u0026#39;Probability\u0026#39;, title = \u0026#39;The effect of Consistency over Net Income (Lindy Effect)\u0026#39;, subtitle = str_c(\u0026#39;The plot shows the probability of observing a future profit/loss given a number \\n\u0026#39;, \u0026#39; of consecutive profits/losses. See details in \\n\u0026#39;, \u0026#39; \u0026lt;https://en.wikipedia.org/wiki/Lindy_effect\u0026gt;\u0026#39;), caption = str_c(\u0026#39;Made by Marcelo S. Perlin (www.msperlin.com/blog)\\n\u0026#39;, \u0026#39;The data covers net income of Brazilian companies from 2000 to 2019\u0026#39;) ) + scale_y_continuous(labels = scales::percent, breaks = seq(0, 1, by = 0.1), limits = c(0,1)) + scale_x_continuous(breaks = min(tab$horizon):max(tab$horizon)) + theme_light() + #scale_shape_discrete(name = \u0026#39;Legenda\u0026#39;) + theme(legend.title = element_blank(), legend.key.size = unit(1,\u0026quot;line\u0026quot;), legend.position = \u0026#39;bottom\u0026#39;) + guides(shape=guide_legend(nrow=2,byrow=TRUE)) print(p) Let me summarize the main conclusions from the plot:\n Time is an ally to the profit of the company. The more consistent the company was in producing profit in the past, higher the chances of a profit in the future. Net losses also cluster, but with a lower probability than profits. Notice how the line for losses is always below the line for profits. This means that company with past losses for a given horizon has more chance to turn a profit than a company that shows consecutive profits to turn a loss. Changes in net income also have persistence memory, specially for increases. Companies that have increasing profits are likely to continue to increase its earnings. Notice, however, that the probability of increased net income is distributed between 50% and 70%, much lower than for the chance of a positive net income. Companies with repeated decreases in its net income are more likely to turn an increase than to continue to decrease (see line at bottom of chart). Notice also that the code can’t find cases for a company with nine or more year of decreases in profit to exist. These are the companies that were bought or bankrupted.  The message from the data and the analysis is clear. In the corporate world, financial inertia is the rule, not the exception. Good and profitable companies continue to be good and profitable enterprises, while bad and unprofitable companies also continue in the same path.\nFrom the investment point of view, the results suggests that time is a friend of the investor. Overall, companies tend to improve its earnings. This corroborates the results from a previous post, where I analyzed the effect of the investment holding period over nominal and real returns to the investor. In short, the more time you stay in the market, the better.\n Unfortunately I cannot distribute this structured dataset as it was one of the restrictions on receiving and publishing results based on it.↩︎\n   ","date":1558310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558310400,"objectID":"2b7ef1c6f5da3485b8ea7af8bd8c2095","permalink":"http://www.msperlin.com/blog/post/2019-05-20-lindy-effect/","publishdate":"2019-05-20T00:00:00Z","relpermalink":"/blog/post/2019-05-20-lindy-effect/","section":"post","summary":"One of the investment concepts that every long term investor should know is the effect of consistency over corporate performance. The main idea is that older and profitable companies are likely to continue to be profitable and even improve its performance in the upcoming years. Likewise, companies with constant losses are likely to continue in the same path.\nThis idea is related to the Lindy Effect. Quoting directly from wikipedia:\n The Lindy effect is a theory that the future life expectancy of some non-perishable things like a technology or an idea is proportional to their current age, so that every additional period of survival implies a longer remaining life expectancy.","tags":["R","investments","lindy effect"],"title":"The Effect of Consistency on Corporate Net Income","type":"post"},{"authors":null,"categories":["R","Brazil","cranlogs"],"content":"  I’m using R for at least five years and always been curious about its usage in Brazil. I see some minor personal evidence that the number of users is increasing over time. My book in portuguese is steadily increasing its sales, and I’ve been receiving far more emails about my R packages. Conference are also booming. Every year there are at least two or three R conferences in Brazil.\nWhat I learned from experience is that software choice is a group decision. It is very likely that you will use whatever your peer group uses. For example, if you are a PhD student, you will never convince your adviser to change research software, even if you have perfectly good reasons!\nIt takes some independence and autonomy to be able to break free from bad group choices. In academia, you can only do that later on, when you finish your PhD and start your career. Then you can use whatever rocks your boat. And, even for that, it takes courage and humbleness to relearn all you research tricks, from data acquisition to reporting your results.\nIn this post I’ll investigate the use of R in Brazil. Rstudio publishes a log page covering all R downloads and package installations. The data is organized by day and very easy to download and parse within R. After downloading it, I organized it by filtering only downloads in Brazil, and saved it in a .rds file. Let’s explore it.\nlibrary(tidyverse) df.dls \u0026lt;- read_rds(\u0026#39;data/r-downloads-brazil.rds\u0026#39;) glimpse(df.dls) ## Rows: 72,853 ## Columns: 7 ## $ date \u0026lt;date\u0026gt; 2012-10-31, 2012-10-31, 2012-10-31, 2012-10-31, 2012-10-31, … ## $ time \u0026lt;time\u0026gt; 16:17:34, 18:21:35, 19:26:20, 19:28:03, 19:26:03, 19:06:32, … ## $ size \u0026lt;dbl\u0026gt; 49351035, 33301364, 49351035, 49351024, 1424794, 66523409, 45… ## $ version \u0026lt;chr\u0026gt; \u0026quot;2.15.2\u0026quot;, \u0026quot;2.15.2\u0026quot;, \u0026quot;2.15.2\u0026quot;, \u0026quot;2.15.2\u0026quot;, \u0026quot;2.15.2\u0026quot;, \u0026quot;2.15.2\u0026quot;, \u0026quot;… ## $ os \u0026lt;chr\u0026gt; \u0026quot;win\u0026quot;, \u0026quot;win\u0026quot;, \u0026quot;win\u0026quot;, \u0026quot;win\u0026quot;, \u0026quot;win\u0026quot;, \u0026quot;osx\u0026quot;, \u0026quot;win\u0026quot;, \u0026quot;win\u0026quot;, \u0026quot;win\u0026quot;… ## $ country \u0026lt;chr\u0026gt; \u0026quot;BR\u0026quot;, \u0026quot;BR\u0026quot;, \u0026quot;BR\u0026quot;, \u0026quot;BR\u0026quot;, \u0026quot;BR\u0026quot;, \u0026quot;BR\u0026quot;, \u0026quot;BR\u0026quot;, \u0026quot;BR\u0026quot;, \u0026quot;BR\u0026quot;, \u0026quot;BR\u0026quot;, \u0026quot;… ## $ ip_id \u0026lt;dbl\u0026gt; 30, 59, 73, 30, 87, 90, 143, 213, 231, 260, 260, 134, 181, 10… As you can see, we have the date, time, version, os (platform), country and ip (randomized daily). First of all, let’s see how many downloads per day we have for Brazil. I’m also including the different release dates for major R versions.\ndf_by_day \u0026lt;- df.dls %\u0026gt;% group_by(ref.date = date) %\u0026gt;% summarise(n = n()) df.R.releases \u0026lt;- tibble(ref.date = as.Date(c(\u0026#39;2013-04-03\u0026#39;, \u0026#39;2014-04-10\u0026#39;,\u0026#39;2015-04-16\u0026#39;, \u0026#39;2016-05-03\u0026#39;, \u0026#39;2017-04-21\u0026#39;, \u0026#39;2018-04-23\u0026#39;, \u0026#39;2019-04-26\u0026#39;)), R_version = c(\u0026#39;3.0.0\u0026#39;, \u0026#39;3.1.0\u0026#39;,\u0026#39;3.2.0\u0026#39;, \u0026#39;3.3.0\u0026#39;,\u0026#39;3.4.0\u0026#39;, \u0026#39;3.5.0\u0026#39;, \u0026#39;3.6.0\u0026#39;) ) p \u0026lt;- ggplot(data = df_by_day, aes(y = n, x = ref.date) ) + geom_point() + geom_smooth(size = 2) + labs(x = \u0026#39;Date (day)\u0026#39;, y= \u0026#39;Number of Downloads\u0026#39;, title = paste0(\u0026#39;Number of R downloads in Brazil\u0026#39;), subtitle = \u0026#39;Data from Rstudio logs \u0026lt;http://cran-logs.rstudio.com/\u0026gt;\u0026#39;) + geom_vline(data = df.R.releases, aes(xintercept = ref.date, color = R_version ), size = 1) + scale_color_grey(start = 0.8, end = 0.2 ) print(p) The number of downloads is steadily increasing over time. The new releases of R also seems to explain the outliers in the dataset. Let’s clean it a bit by decreasing the frequency and calculating the number of downloads per month, instead of by day.\ndf_by_month \u0026lt;- df.dls %\u0026gt;% group_by(ref.month = lubridate::ymd(format(date, \u0026#39;%Y-%m-01\u0026#39;))) %\u0026gt;% summarise(n = n()) p \u0026lt;- ggplot(data = df_by_month, aes(y = n, x = ref.month) ) + geom_point() + geom_smooth(size = 2) + labs(x = \u0026#39;Date (month)\u0026#39;, y= \u0026#39;Number of Downloads\u0026#39;, title = paste0(\u0026#39;Number of R downloads in Brazil\u0026#39;), subtitle = \u0026#39;Data from Rstudio logs \u0026lt;http://cran-logs.rstudio.com/\u0026gt;\u0026#39;) + geom_vline(data = df.R.releases, aes(xintercept = ref.date, color = R_version ), size = 1) + scale_color_grey(start = 0.8, end = 0.2 ) print(p) Much better! Overall, R downloads average about 910.7 per month, with a monthly compound rate of 6%. It means that, each month, the number of downloads is increasing by 6% from previous month.\nThe data also includes information about the operating system. Let’s check its distribution:\ndf_by_os \u0026lt;- df.dls %\u0026gt;% group_by(os) %\u0026gt;% count() %\u0026gt;% na.omit() %\u0026gt;% ungroup() %\u0026gt;% mutate(os = fct_recode(os, \u0026quot;Windows\u0026quot; = \u0026quot;win\u0026quot;, \u0026#39;Mac OS\u0026#39; = \u0026#39;osx\u0026#39;, \u0026#39;Linux\u0026#39; = \u0026#39;src\u0026#39;)) p \u0026lt;- ggplot(df_by_os, aes(x = os, y = n)) + geom_col() + labs(x = \u0026#39;Operation System\u0026#39;, y = \u0026#39;Number of Download Cases\u0026#39;, title = \u0026#39;Distribution of OS\u0026#39;, subtitle = \u0026#39;Data from Rstudio logs \u0026lt;http://cran-logs.rstudio.com/\u0026gt;\u0026#39;) print(p) Not unexpectedly, Windows is the winner! I’m very surprised to see that Mac OS presents more downloads than Linux. With an unfavorable exchange rate and many import taxes, the price of a Mac computer — desktop or laptop — are exorbitantly expensive in Brazil. This tells a lot about the purchase power of R users.\nI hope you liked this post. Next time I’ll analyze the logs of package installation in Brazil.\n","date":1558051200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558051200,"objectID":"7c372f588f3039c84b6cf65ec190fc46","permalink":"http://www.msperlin.com/blog/post/2019-05-17-r-in-brazil/","publishdate":"2019-05-17T00:00:00Z","relpermalink":"/blog/post/2019-05-17-r-in-brazil/","section":"post","summary":"I’m using R for at least five years and always been curious about its usage in Brazil. I see some minor personal evidence that the number of users is increasing over time. My book in portuguese is steadily increasing its sales, and I’ve been receiving far more emails about my R packages. Conference are also booming. Every year there are at least two or three R conferences in Brazil.\nWhat I learned from experience is that software choice is a group decision. It is very likely that you will use whatever your peer group uses.","tags":["R","Brazil","cranes"],"title":"R usage in Brazil","type":"post"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"Descrição Este livro apresenta uma forma de investir com foco no longo prazo, resiliência e na qualidade de vida do investidor. Contrário a crença popular, investir para o longo prazo é fácil e pode ser feito por qualquer pessoa com vontade de aprender.\nO grande diferencial da obra é o uso frequente de dados financeiros reais para entender melhor como o mercado de renda fixa funciona, os efeitos dos tributos e custos operacionais, e quais são as melhores opções para o investidor pessoa física. Cada capítulo do livro apresenta e analisa os principais produtos disponíveis na renda fixa: produtos bancários (CDBs, LCA, entre outros), Tesouro Direto (Prefixado, IPCA+, SELIC), fundos de renda fixa e debêntures.\nTodos os capítulos exploram a análise de dados para tirar conclusões importantes a respeito da atratividade de cada produto financeiro. No último capítulo apresenta-se a prática do ciclo de investimento, desde o registro inicial em uma corretora ou banco, até o resgate e reinvestimento do dinheiro aplicado.\nEste livro é recomendado a qualquer pessoa com interesse em se tornar um investidor de longo prazo. Não é necessário conhecimento prévio algum. O único requisito é ter vontade de aprender e ganhar mais autonomia na administração de suas finanças pessoais.\nOnde Comprar O livro está disponível na Amazon no formato ebook (leitura no kindle) e livro impresso. Alternativamente, o livro impresso pode ser adquirido na Bastter.com.\nAqui um ponto importante, no caso do livro impresso pela Amazon, o envio é realizado dos Estados Unidos, aumentando o valor e tempo do frete. O livro impresso da Bastter tem valor um pouco mais alto, mas frete grátis dentro do Brasil.\n Versão Online A versão online do livro, com os três primeiros capítulos disponibilizados de forma gratuita, é encontrada neste link.\nMaterial Suplementar O material também é acompanhado por um aplicativo web disponível ao público, onde é possível simular o investimento e resgate líquido de vários produtos da renda fixa. Acesse o aplicativo aqui.\nO apêndice técnico online da obra está disponibilizado aqui.\n","date":1557878400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557878400,"objectID":"1c68b82fdb8d0e0ed9e6039dd68495cd","permalink":"http://www.msperlin.com/blog/publication/2019_book-pirf/","publishdate":"2019-05-15T00:00:00Z","relpermalink":"/blog/publication/2019_book-pirf/","section":"publication","summary":"Descrição Este livro apresenta uma forma de investir com foco no longo prazo, resiliência e na qualidade de vida do investidor. Contrário a crença popular, investir para o longo prazo é fácil e pode ser feito por qualquer pessoa com vontade de aprender.\nO grande diferencial da obra é o uso frequente de dados financeiros reais para entender melhor como o mercado de renda fixa funciona, os efeitos dos tributos e custos operacionais, e quais são as melhores opções para o investidor pessoa física. Cada capítulo do livro apresenta e analisa os principais produtos disponíveis na renda fixa: produtos bancários (CDBs, LCA, entre outros), Tesouro Direto (Prefixado, IPCA+, SELIC), fundos de renda fixa e debêntures.","tags":null,"title":"Poupando e Investindo em Renda Fixa: Uma Abordagem Baseada em Dados","type":"publication"},{"authors":null,"categories":["R","BatchGetSymbols","GetBCBData"],"content":"  One of the subjects that I teach in my undergraduate finance class is the relationship between risk and expected returns. In short, the riskier the investment, more returns should be expected by the investor. It is not a difficult argument to make. All that you need to understand is to remember that people are not naive in financial markets. Whenever they make a big gamble, the rewards should also be large. Rational investors, on theory, would not invest in risky stocks that are likelly to yield low returns.\nGoing further, one the arguments I make to support this idea is looking at historical data. By assuming that expected returns is the average yearly return rate on a stock and the risk is the standard deviation of the same returns, we can check for a positive relationship by plotting the data in a scatter plot.\nIn this post I’ll show how you can do it easily in R using BatchGetSymbols, GetBCBData and tidyverse.\nFirst, we will gather and organize all data sets. Here I’m using the stock components of Ibovespa, the Brazilian market index, and also CDI, a common risk free rate in Brazil. The next code will:\nImport the data organize it in the same structure (same columns) bind it all together  # get stock data library(tidyverse) library(BatchGetSymbols) library(GetBCBData) first.date \u0026lt;- \u0026#39;2008-01-01\u0026#39; # last date is Sys.Date by default # get stock data df.ibov \u0026lt;- GetIbovStocks() mkt.idx \u0026lt;- c(\u0026#39;^BVSP\u0026#39;) my.tickers \u0026lt;- c(mkt.idx, paste0(df.ibov$tickers, \u0026#39;.SA\u0026#39;) ) df.prices \u0026lt;- BatchGetSymbols(tickers = my.tickers, first.date = first.date, freq.data = \u0026#39;yearly\u0026#39;, be.quiet = TRUE)[[2]] tab.stocks \u0026lt;- df.prices %\u0026gt;% na.omit() %\u0026gt;% group_by(ticker) %\u0026gt;% summarise(mean.ret = mean(ret.adjusted.prices), sd.ret = sd(ret.adjusted.prices)) %\u0026gt;% mutate(ticker = str_replace_all(ticker, fixed(\u0026#39;.SA\u0026#39;), \u0026#39;\u0026#39;) ) tab.mkt.idx \u0026lt;- tab.stocks %\u0026gt;% filter(ticker %in% mkt.idx) tab.stocks \u0026lt;- tab.stocks %\u0026gt;% filter(!(ticker %in% mkt.idx)) # get CDI (risk free rate) my.id \u0026lt;- c(CDI = 4389) tab.CDI \u0026lt;- gbcbd_get_series(my.id, first.date = first.date) %\u0026gt;% rename(ticker = series.name ) %\u0026gt;% mutate(ref.date = format(ref.date, \u0026#39;%Y\u0026#39;), value = value/100) %\u0026gt;% group_by(ref.date, ticker) %\u0026gt;% summarise(ret = mean(value)) %\u0026gt;% group_by(ticker) %\u0026gt;% summarise(mean.ret = mean(ret), sd.ret = sd(ret)) Now that we have the data, lets use ggplot to build our graph.\nlibrary(ggplot2) p \u0026lt;- ggplot(tab.stocks, aes(x = sd.ret, y = mean.ret, group = ticker)) + geom_point() + geom_text(data = tab.stocks, aes(x = sd.ret, y = mean.ret, label = ticker), nudge_y = 0.03, check_overlap = TRUE, nudge_x = 0.05 ) + geom_point(data = tab.CDI, aes(x = sd.ret, y = mean.ret, color = ticker), size =5) + geom_point(data = tab.mkt.idx, aes(x = sd.ret, y = mean.ret, color = ticker), size =5) + labs(x = \u0026#39;Risk (standard deviation)\u0026#39;, y =\u0026#39;Expected Returns (average)\u0026#39;, title = \u0026#39;Mean X Variance map for B3\u0026#39;, subtitle = paste0(nrow(tab.stocks), \u0026#39; stocks, \u0026#39;, lubridate::year(min(df.prices$ref.date)), \u0026#39; - \u0026#39;, lubridate::year(max(df.prices$ref.date)))) + scale_x_continuous(labels = scales::percent) + scale_y_continuous(labels = scales::percent) print(p) Looks pretty! What do we learn?\n Overall, most of the stocks did better than the risk free rate (CDI);\n There is a positive relationship between risk and return. The higher the standard deviation (x-axis), the higher the mean of returns (y-axis). However, notice that it is not a perfect relationship. If we followed the mean-variance gospel, there are lots of opportunities of arbitrage. We would mostly invest in those stocks in the upper-left part of the plot;\n Surprisingly, the market index, Ibovespa (^BVSP), is not well positioned in the graph. Since it is a diversified portfolio, I expected it to be closer to the frontier, around stock EQTL3.\n  ","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"74414eabebd2b97838f7de2e79e311e4","permalink":"http://www.msperlin.com/blog/post/2019-05-01-meanvariance/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/blog/post/2019-05-01-meanvariance/","section":"post","summary":"One of the subjects that I teach in my undergraduate finance class is the relationship between risk and expected returns. In short, the riskier the investment, more returns should be expected by the investor. It is not a difficult argument to make. All that you need to understand is to remember that people are not naive in financial markets. Whenever they make a big gamble, the rewards should also be large. Rational investors, on theory, would not invest in risky stocks that are likelly to yield low returns.","tags":["R","BatchGetSymbols","GetBCBData"],"title":"Risk and return for B3","type":"post"},{"authors":null,"categories":["R","GetBCBData"],"content":"  The Central Bank of Brazil (BCB) offers access to the SGS system (sistema gerenciador de series temporais) with a official API available here.\nOver time, I find myself using more and more of the available datasets in my regular research and studies. Last weekend I decided to write my own API package that would make my life (and others) a lot easier.\nPackage GetBCBData can fetch data efficiently and rapidly:\n Use of a caching system with package memoise to speed up repeated requests of data; Users can utilize all cores of the machine (parallel computing) when fetching a large batch of time series; Allows the choice for format output: long (row oriented, tidy data) or wide (column oriented) Error handling internally. Even if requested series does not exist, the function will still return all results.  Installation # CRAN (official release) - IN CHECK install.packages(\u0026#39;GetBCBData\u0026#39;) # Github (dev version) devtools::install_github(\u0026#39;msperlin/GetBCBData\u0026#39;)  A simple example Let’s have a look at unemployment rates around the world. After searching for the ids in the SGS system, we find the ids for 6 countries and set it as input id.\nNow, let’s download the data with GetBCBData:\n#devtools::install_github(\u0026#39;msperlin/GetBCBData\u0026#39;) library(GetBCBData) library(tidyverse) my.countries \u0026lt;- c(\u0026#39;Germany\u0026#39;, \u0026#39;Canada\u0026#39;, \u0026#39;USA\u0026#39;, \u0026#39;France\u0026#39;, \u0026#39;Italy\u0026#39;, \u0026#39;Japan\u0026#39;) my.ids \u0026lt;- c(3785:3790) names(my.ids) \u0026lt;- paste0(\u0026#39;Unemp. rate - \u0026#39;, my.countries) df.bcb \u0026lt;- gbcbd_get_series(id = my.ids , first.date = \u0026#39;2000-01-01\u0026#39;, last.date = Sys.Date()) glimpse(df.bcb) ## Rows: 1,196 ## Columns: 4 ## $ ref.date \u0026lt;date\u0026gt; 2000-01-01, 2000-02-01, 2000-03-01, 2000-04-01, 2000-05-… ## $ value \u0026lt;dbl\u0026gt; 8.2, 8.1, 8.1, 8.0, 8.0, 8.0, 7.9, 7.9, 7.9, 7.8, 7.8, 7.… ## $ id.num \u0026lt;int\u0026gt; 3785, 3785, 3785, 3785, 3785, 3785, 3785, 3785, 3785, 378… ## $ series.name \u0026lt;chr\u0026gt; \u0026quot;Unemp. rate - Germany\u0026quot;, \u0026quot;Unemp. rate - Germany\u0026quot;, \u0026quot;Unemp.… p \u0026lt;- ggplot(df.bcb, aes(x = ref.date, y = value) ) + geom_line() + labs(title = \u0026#39;Unemploymnent Rates Around the World\u0026#39;, subtitle = paste0(min(df.bcb$ref.date), \u0026#39; to \u0026#39;, max(df.bcb$ref.date)), x = \u0026#39;\u0026#39;, y = \u0026#39;Percentage*100\u0026#39;) + facet_wrap(~series.name) print(p)  ","date":1555286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555286400,"objectID":"984b214efd5039f0d1b0ce7867e3b6dc","permalink":"http://www.msperlin.com/blog/post/2019-04-15-getbcbdata/","publishdate":"2019-04-15T00:00:00Z","relpermalink":"/blog/post/2019-04-15-getbcbdata/","section":"post","summary":"The Central Bank of Brazil (BCB) offers access to the SGS system (sistema gerenciador de series temporais) with a official API available here.\nOver time, I find myself using more and more of the available datasets in my regular research and studies. Last weekend I decided to write my own API package that would make my life (and others) a lot easier.\nPackage GetBCBData can fetch data efficiently and rapidly:\n Use of a caching system with package memoise to speed up repeated requests of data; Users can utilize all cores of the machine (parallel computing) when fetching a large batch of time series; Allows the choice for format output: long (row oriented, tidy data) or wide (column oriented) Error handling internally.","tags":["R","GetBCBData"],"title":"New package: GetBCBData","type":"post"},{"authors":null,"categories":["R","BatchGetSymbols"],"content":"  BatchGetSymbols is my most downloaded package by any count. Computation time, however, has always been an issue. While downloading data for 10 or less stocks is fine, doing it for a large ammount of tickers, say the SP500 composition, gets very boring.\nI’m glad to report that time is no longer an issue. Today I implemented a parallel option for BatchGetSymbols. If you have a high number of cores in your computer, you can seriously speep up the importation process. Importing SP500 compositition, over 500 stocks, is a breeze.\nGive a try. The new version is already available in Github:\ndevtools::install_github(\u0026#39;msperlin/BatchGetSymbols\u0026#39;) It should be in CRAN soon.\nHow to use parallel Very simple. Just set you parallel plan with future::plan and use input do.parallel = TRUE in BatchGetSymbols. If you are not sure how many cores you have available, just run the following code to figure it out:\nfuture::availableCores() ## system ## 16 #devtools::install_github(\u0026#39;msperlin/BatchGetSymbols\u0026#39;) library(BatchGetSymbols) # get tickers from SP500 df.sp500 \u0026lt;- GetSP500Stocks() tickers \u0026lt;- df.sp500$Tickers future::plan(future::multisession, workers = 10) # use 10 cores (future::availableCores()) # dowload data for 50 stocks l.out \u0026lt;- BatchGetSymbols(tickers = tickers[1:50], first.date = \u0026#39;2010-01-01\u0026#39;, last.date = \u0026#39;2019-01-01\u0026#39;, do.parallel = TRUE, do.cache = FALSE) ## Progress: ───────────────────────────────────────── 100% Progress: ─────────────────────────────────────────────── 100% Progress: ───────────────────────────────────────────────── 100% Progress: ─────────────────────────────────────────────────── 100% glimpse(l.out) ## List of 2 ## $ df.control: tibble [50 × 6] (S3: tbl_df/tbl/data.frame) ## ..$ ticker : chr [1:50] \u0026quot;MMM\u0026quot; \u0026quot;ABT\u0026quot; \u0026quot;ABBV\u0026quot; \u0026quot;ABMD\u0026quot; ... ## ..$ src : chr [1:50] \u0026quot;yahoo\u0026quot; \u0026quot;yahoo\u0026quot; \u0026quot;yahoo\u0026quot; \u0026quot;yahoo\u0026quot; ... ## ..$ download.status : chr [1:50] \u0026quot;OK\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;OK\u0026quot; ... ## ..$ total.obs : int [1:50] 2264 2264 1510 2264 2264 2264 2264 2264 2264 2264 ... ## ..$ perc.benchmark.dates: num [1:50] 1 1 0.667 1 1 ... ## ..$ threshold.decision : chr [1:50] \u0026quot;KEEP\u0026quot; \u0026quot;KEEP\u0026quot; \u0026quot;OUT\u0026quot; \u0026quot;KEEP\u0026quot; ... ## $ df.tickers:\u0026#39;data.frame\u0026#39;: 106408 obs. of 10 variables: ## ..$ price.open : num [1:106408] 83.1 82.8 83.9 83.3 83.7 ... ## ..$ price.high : num [1:106408] 83.4 83.2 84.6 83.8 84.3 ... ## ..$ price.low : num [1:106408] 82.7 81.7 83.5 82.1 83.3 ... ## ..$ price.close : num [1:106408] 83 82.5 83.7 83.7 84.3 ... ## ..$ volume : num [1:106408] 3043700 2847000 5268500 4470100 3405800 ... ## ..$ price.adjusted : num [1:106408] 63.5 63.1 64 64.1 64.5 ... ## ..$ ref.date : Date[1:106408], format: \u0026quot;2010-01-04\u0026quot; \u0026quot;2010-01-05\u0026quot; ... ## ..$ ticker : chr [1:106408] \u0026quot;MMM\u0026quot; \u0026quot;MMM\u0026quot; \u0026quot;MMM\u0026quot; \u0026quot;MMM\u0026quot; ... ## ..$ ret.adjusted.prices: num [1:106408] NA -0.006263 0.014182 0.000717 0.007047 ... ## ..$ ret.closing.prices : num [1:106408] NA -0.006264 0.014182 0.000717 0.007046 ...  ","date":1555113600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555113600,"objectID":"d284eb16dc959d88c17d02dcc522c908","permalink":"http://www.msperlin.com/blog/post/2019-04-13-parallel-batchgetsymbols/","publishdate":"2019-04-13T00:00:00Z","relpermalink":"/blog/post/2019-04-13-parallel-batchgetsymbols/","section":"post","summary":"BatchGetSymbols is my most downloaded package by any count. Computation time, however, has always been an issue. While downloading data for 10 or less stocks is fine, doing it for a large ammount of tickers, say the SP500 composition, gets very boring.\nI’m glad to report that time is no longer an issue. Today I implemented a parallel option for BatchGetSymbols. If you have a high number of cores in your computer, you can seriously speep up the importation process. Importing SP500 compositition, over 500 stocks, is a breeze.","tags":["R","BatchGetSymbols"],"title":"BatchGetSymbols is now parallel!","type":"post"},{"authors":null,"categories":["R","investments"],"content":"   In the last few weeks we’ve seen a great deal of controversy in Brazil regarding financial investments. Too keep it short, Empiricus, an ad-based company that massively sells online courses and subscriptions, posted a YouTube ad where a young girl, Bettina, says the following:\nHi, I\u0026#39;m Bettina, I am 22 years old and, starting with R$ 1,500, I now own R$ 1,042,000 of accumulated wealth. She later explains that she earned the money by investing in the stock market over three years. For my international audience, the proposed investment is equivalent of turning 394 dolars into 263169.2 dolars over a three year period.\nAnyone with a economics or business background will easily spot that the financial returns stated in the ad is simply not possible. Even if Bettina is a very good investor, reaching this level of returns over a three year period in the stock market is unheard of. The yearly rate of return of the investment is equal to 774% per year. The monthly rate proposed in the ad is equivalent to 20% per month.\nGiving perspective, Buffet, one of the greatest long term investor of all times, has reached the approximate rate of 19% per year, around 1.46% per month. So, Bettina is either a financial genius that, with only 22 years old, was able to beat Buffet in its own game, or the ad is not fully committed to the truth. To be fair, even if we took the difference of inflation rates between Brazil and US into account, the difference is still very impressive and misleading.\nOthers have pointed out that if you compound these return over time, the result will be economically unrealistic. See next what happens to R$ 1.500 if we assume that you can replicate the alledged investment return of Bettina (774% per year) over a 10 year period.\n Table 1: Compounding returns for Bettina    Number of years  Investiment value      1  R$ 13.103,89    2  R$ 114.474,71    3  R$ 1.000.043,00    4  R$ 8.736.305,50    5  R$ 76.319.752,11    6  R$ 666.724.001,23    7  R$ 5.824.454.109,93    8  R$ 50.882.022.569,93    9  R$ 444.501.780.243,15    10  R$ 3.883.136.374.301,74     If Bettina is a genius and can replicate her result over the years, she will be a billionaire in 7 years and a trillionaire in 10. If she waited one more year, she could even buy the whole country if she wanted to. The current GDP of Brazil is around 2 trillion USD (7.5 trillion in R$). She can easily reach this amount of cash in 12 years or more.\nBut lets go further in this endeavor. Let’s stop being skeptic about her returns and see whether its possible to achieve such returns in the stock market. As you can expect, I’m taking a data based approach. I’ll compare the returns of Bettina to GodBot, a computer algorithm that can perfectly predict stock prices.\nFirst, let’s download some stock data from B3, the Brazilian stock exchange.\nlibrary(BatchGetSymbols) df.ibov \u0026lt;- GetIbovStocks() my.tickers \u0026lt;- paste0(df.ibov$tickers, \u0026#39;.SA\u0026#39;) df.stocks \u0026lt;- BatchGetSymbols(tickers = my.tickers, first.date = \u0026#39;2010-01-01\u0026#39;, last.date = \u0026#39;2019-01-01\u0026#39;, cache.folder = \u0026#39;~/.mem_cache/BGS_Cache\u0026#39;)[[2]] My god bot has a single and simple rule:\n For each month, it will always invest 100% in the stock with the highest return for the month  Next we code this bot using tidyverse:\nlibrary(tidyverse) res.inv \u0026lt;- df.stocks %\u0026gt;% mutate(ref.month = as.Date(format(ref.date, \u0026#39;%Y-%m-01\u0026#39;))) %\u0026gt;% group_by(ref.month, ticker) %\u0026gt;% summarise(ret.month = last(price.adjusted)/first(price.adjusted) - 1) %\u0026gt;% group_by(ref.month) %\u0026gt;% summarise(best.ticker = ticker[which.max(ret.month)], best.return = ret.month[which.max(ret.month)]) Now, let’s have a look in those returns:\nlibrary(ggplot2) # bettinas returns initial.cash \u0026lt;- 1500 last.cash \u0026lt;- 1000043 my.T \u0026lt;- 3 # years r.aa \u0026lt;- (last.cash/initial.cash)^(1/3) -1 r.am \u0026lt;- (last.cash/initial.cash)^(1/(3*12)) -1 p \u0026lt;- ggplot(res.inv, aes(x = ref.month, y = best.return)) + geom_col() + geom_hline(yintercept =r.am, color = \u0026#39;red\u0026#39;, size =1.5) + labs(x = \u0026#39;Time\u0026#39;, y = \u0026#39;Monthly Returns\u0026#39;, title = \u0026#39;Monthly Returns of Bettina and GodBot\u0026#39;, subtitle = paste0(\u0026#39;- This plot shows the \u0026quot;alleged\u0026quot; returns from Bettina against a perfect predictor \\n for the BR stock market\\n\u0026#39;, \u0026#39;- The horizontal red line represents the return of Bettina (19.79% monthly)\u0026#39;), caption = \u0026#39;www.msperlin.com/blog\u0026#39; ) + scale_y_continuous(labels = scales::percent) p As you can see, Bettina did good with a constant monthly return of 20%. But, GodBot is better. Bettina is clearly missing something out!\nWhen looking at the nominal value of the investment, the effect of compound returns explodes the value of the portfolio.\nlibrary(tidyr) format.cash \u0026lt;- function(x) { require(scales) x.formatted \u0026lt;- dollar(x, prefix = \u0026#39;R$ \u0026#39;, decimal.mark = \u0026#39;,\u0026#39;, big.mark = \u0026#39;.\u0026#39;, largest_with_cents = Inf) return(x.formatted) } df.cumret \u0026lt;- res.inv %\u0026gt;% mutate(cumret.godbot = initial.cash*cumprod(1+res.inv$best.return), cumret.bettina =initial.cash*cumprod(1+rep(r.am, n())) ) %\u0026gt;% select(-best.ticker, -best.return) df.to.plot \u0026lt;- gather(df.cumret, \u0026#39;Investor\u0026#39;, \u0026quot;Value\u0026quot;, -ref.month ) p \u0026lt;- ggplot(df.to.plot, aes(x = ref.month, y = Value, color = Investor)) + geom_line(size =2) + labs(x = \u0026#39;Time\u0026#39;, y = \u0026#39;Value of Return\u0026#39;, title = \u0026#39;Accumulated Value of Portfolio\u0026#39;, subtitle = \u0026#39;This figure shows the value of accumulated return for Bettina in comparison to GodBot, a perfect predictor of stock markets\u0026#39;) + scale_y_continuous(labels = format.cash) p The results show that Bettina’s returns are possible. All you need to do is to perfectly predict, for each month, which stock will do best in the market. If you haven’t sensed my irony, let me be crystal clear: Market prices are impossible to predict. The result I just showed you is only possible in my computer. No one will ever be able to replicate it in practice.\nThe ad from Empiricus is very misleading. In my opinion as a finance professor, the real problem in this episode is that the great majority of the Brazilian population is not financially educated. Many people will believe that is legally possible to reach a 20% return over a month.\nI’ve seen countless cases of financial pyramids, usually tied to some exotic cryptocurrency, to rise and shortly burn here in Brazil. This is specially – and sadly – most frequent in the poorer areas of the country. Those that follow Empiricus advice will soon learn its lesson. Making money in short run in stocks is very difficult.\nUnfortunately, every disapointed person that followed Empiricus advice is never going back to investing in financial markets. They will miss what is probably the greatest system ever designed for investing and passively creating wealth in the long run.\n","date":1553299200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553299200,"objectID":"283c4859adfaf6f3555aec4ffb2b6be9","permalink":"http://www.msperlin.com/blog/post/2019-03-23-bettina-case/","publishdate":"2019-03-23T00:00:00Z","relpermalink":"/blog/post/2019-03-23-bettina-case/","section":"post","summary":"In the last few weeks we’ve seen a great deal of controversy in Brazil regarding financial investments. Too keep it short, Empiricus, an ad-based company that massively sells online courses and subscriptions, posted a YouTube ad where a young girl, Bettina, says the following:\nHi, I\u0026#39;m Bettina, I am 22 years old and, starting with R$ 1,500, I now own R$ 1,042,000 of accumulated wealth. She later explains that she earned the money by investing in the stock market over three years. For my international audience, the proposed investment is equivalent of turning 394 dolars into 263169.","tags":["R","investments"],"title":"Can you turn 1500 USD into 1.000.430 USD by investing in the stock market for three years?","type":"post"},{"authors":null,"categories":["R","book"],"content":"  I received many messages regarding my book promotion (see previous post ). I’ll use this post to answer the most frequent questions:\n Does the paperback edition have a discount?\nNo. The price drop is only valid for the ebook edition but not by choice. Unfortunately, Amazon does not let me do countdown promotions for the paperback edition.\nSo, in favor of those, like myself, that like the smell of a fresh book page, I manually dropped the price of the paperback to 17.99 USD (it was 24.99 USD). Printings costs are heavy, which is why I can’t go all the way to a 50% discount. The system tells me that the new price should be live within the next 72 hours. I’ll keep the new price until the end of this week.\n When will the second edition be released?\nMy schedule is to start to work on the new edition of Processing and Analyzing.. on june 2019. Hopefully I can publish it before january 2020. Sorry but I can’t give much detail about the new content yet. But be sure I’ll keep you guys posted.\n Best.\n","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552176000,"objectID":"b8450c0df3ff2ed2b9908d33a17fc4ff","permalink":"http://www.msperlin.com/blog/post/2019-03-10-pafdr-promotion_2/","publishdate":"2019-03-10T00:00:00Z","relpermalink":"/blog/post/2019-03-10-pafdr-promotion_2/","section":"post","summary":"I received many messages regarding my book promotion (see previous post ). I’ll use this post to answer the most frequent questions:\n Does the paperback edition have a discount?\nNo. The price drop is only valid for the ebook edition but not by choice. Unfortunately, Amazon does not let me do countdown promotions for the paperback edition.\nSo, in favor of those, like myself, that like the smell of a fresh book page, I manually dropped the price of the paperback to 17.99 USD (it was 24.99 USD).","tags":["R","book"],"title":"(2/2) Book promotion (paperback edition) - \"Processing and Analyzing Financial Data with R\"","type":"post"},{"authors":null,"categories":["R","book"],"content":"  I recently did a book promotion for my R book in portuguese and it was a big sucess!\nMy english book is now being sold with the same promotion. You can purchase it with a 50% discount if you buy it on the 10th day of march. See it here. The discount will be valid throughout the week, with daily price increases.\nIf you want to learn more about R and its use in Finance and Economics, this book is a great opportunity.\nYou can find more details about the book, including datasets, R code and all that good stuff here. An online and public version with the first 7 chapters is available in this link.\nIf you liked the work (or not), please provide a honest review at Amazon.com. As an author, I certainly apreciate it and will use the feedback for the next edition of the book.\nBest,\n","date":1552089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552089600,"objectID":"660598d131fdb9fb1a6e2e7ace9fb78e","permalink":"http://www.msperlin.com/blog/post/2019-03-09-pafdr-promotion/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/blog/post/2019-03-09-pafdr-promotion/","section":"post","summary":"I recently did a book promotion for my R book in portuguese and it was a big sucess!\nMy english book is now being sold with the same promotion. You can purchase it with a 50% discount if you buy it on the 10th day of march. See it here. The discount will be valid throughout the week, with daily price increases.\nIf you want to learn more about R and its use in Finance and Economics, this book is a great opportunity.\nYou can find more details about the book, including datasets, R code and all that good stuff here.","tags":["R","book"],"title":"Book promotion - \"Processing and Analyzing Financial Data with R\"","type":"post"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"Slides  ","date":1549929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549929600,"objectID":"e6209dbdd371bd87def781751060263c","permalink":"http://www.msperlin.com/blog/talk/2019-02-10-self-publish/","publishdate":"2019-02-12T00:00:00Z","relpermalink":"/blog/talk/2019-02-10-self-publish/","section":"talk","summary":"Palestra realizada na EA sobre tópico de publicação independente","tags":[],"title":"Publicando livros de forma independente","type":"talk"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"","date":1549411200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549411200,"objectID":"c37c43bbff31f61e59ee1ef58092043b","permalink":"http://www.msperlin.com/blog/talk/2019-02-06-user-sicredi/","publishdate":"2019-02-06T00:00:00Z","relpermalink":"/blog/talk/2019-02-06-user-sicredi/","section":"talk","summary":"Palestra realizada para o grupo useR de Porto Alegre","tags":[],"title":"R and Financial Datasets","type":"talk"},{"authors":["admin"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab  Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata ( front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"http://www.msperlin.com/blog/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/blog/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"http://www.msperlin.com/blog/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/blog/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":["R","GetDFPData"],"content":"  I just released a major update to package GetDFPData. Here are the main changes:\nNaming conventions for caching system are improved so that it reflects different versions of FRE and DFP files. This means the old caching system no longer works. If you have built yourself your own cache folder with many companies, do clean up the cache by deleting all folders. Run your code again and it will rebuild all files. Unfortinatelly this is a “brute force”, but necessary step. The code and data is now explicit about the version of downloaded files. If a company updates its FRE files, for example, the package will detect it and download and read the new information.\nFixed issue with dates in FRE. Many people reported that the dates from the FRE tables did not match the ones in the website. The reason is that the FRE column “ref.date” was set as (year.fre -1)-12-31. This made sense for many of the FRE tables, but not all. The idea was to use column ref.date to bind the DFP and FRE datasets together. In order to be more transparent about this choice, a new column “year.fre” is added to all FRE data. It contains the original year of the FRE file. This way the user will always know where the FRE datasets are coming from.\nMany improvements. Bugs were chased and fixed. The code is now more mantainable and runs with more smoothly.\nThe new version is already available at github and should be in CRAN in a few days.\nThe datasets from the shinny version are also updated with this new dataset.\n","date":1547251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547251200,"objectID":"97a4ca04d51a77a4aaec2203056a7d07","permalink":"http://www.msperlin.com/blog/post/2019-01-12-getdfpdata-ver14/","publishdate":"2019-01-12T00:00:00Z","relpermalink":"/blog/post/2019-01-12-getdfpdata-ver14/","section":"post","summary":"I just released a major update to package GetDFPData. Here are the main changes:\nNaming conventions for caching system are improved so that it reflects different versions of FRE and DFP files. This means the old caching system no longer works. If you have built yourself your own cache folder with many companies, do clean up the cache by deleting all folders. Run your code again and it will rebuild all files. Unfortinatelly this is a “brute force”, but necessary step. The code and data is now explicit about the version of downloaded files.","tags":["R","GetDFPData"],"title":"GetDFPData Ver 1.4","type":"post"},{"authors":null,"categories":["R","site"],"content":"  At the end of every year I plan to write about the highlight of the current year and set plans for the future. First, let’s talk about my work in 2018.\nHighlights of 2018 Research wise, my scientometrics paper Is predatory publishing a real threat? Evidence from a large database study was featured in many news outlets. Its altmetric page is doing great, with over 1100 downloads and featured at top 5% of all research output measured by altmetric. This is, by far, the most impactful research piece I ever wrote. Its rewarding to see my work featured in the local and international media.\nThis year I also released the first version of GetDFPData, a R package for accessing a large database of financial information from B3, the Brazilian exchange. I’m glad to report that many people are using it for their own research. I can see the number of visits in the web interface and the frequent emails I get about the package. The feedback from other researchers has been great but, off course, there are always ways to improve the code. I’ve been constantly developing it over time.\nThe GetDFPData package also had an impact in my own research. I’ve always been biased towards the topic of capital markets and now I’m doing research in corporate finance, mostly due to the new access to a large database of corporate events. Currently, I have three paper initiatives in analyzing the effect of boards formation towards financial performance of Brazilian companies. These will likely probably be published in 2019 or 2020.\nIn late 2018 I started my YouTube series padfeR, with video tutorials about using R for Finance and Economics. The idea is to have a greater impact and help those that are starting to use R. So far, all videos are in Portuguese but I do have plans for doing it in english in the future. Hopefully I’ll find some time in 2019 to start it.\nOverall, 2018 was a great year. I’m always thankful for having the opportunity of working in a job that I love and look forward to work (almost) every single day.\n My blog posts in 2018 In november I changed the technology behind my blog from Jekyll to Hugo. Can’t stress enough how much I’m liking the Academic template built with blogdown and hosted in my own server. It is far easier to write posts and maintain the website.\nFirst, let’s see how many posts I have so far.\nmy.blog.folder \u0026lt;- \u0026#39;~/Dropbox/11-My Website/01-msperlin.com/content/post/\u0026#39; post.files \u0026lt;- list.files(path = my.blog.folder, pattern = \u0026#39;.Rmd\u0026#39;) post.files ## [1] \u0026quot;2017-02-16-Writing-a-book.Rmd\u0026quot; ## [2] \u0026quot;2017-02-16-Writing-a-book.Rmd.lock~\u0026quot; ## [3] \u0026quot;2017-12-06-Package-GetDFPData.Rmd\u0026quot; ## [4] \u0026quot;2017-12-06-Package-GetDFPData.Rmd.lock~\u0026quot; ## [5] \u0026quot;2017-12-13-Serving-shiny-apps-internet.Rmd\u0026quot; ## [6] \u0026quot;2017-12-13-Serving-shiny-apps-internet.Rmd.lock~\u0026quot; ## [7] \u0026quot;2017-12-30-Looking-Back-2017.Rmd\u0026quot; ## [8] \u0026quot;2017-12-30-Looking-Back-2017.Rmd.lock~\u0026quot; ## [9] \u0026quot;2018-01-22-Update-BatchGetSymbols.Rmd\u0026quot; ## [10] \u0026quot;2018-01-22-Update-BatchGetSymbols.Rmd.lock~\u0026quot; ## [11] \u0026quot;2018-03-16-Writing_Papers_About_Pkgs.Rmd\u0026quot; ## [12] \u0026quot;2018-03-16-Writing_Papers_About_Pkgs.Rmd.lock~\u0026quot; ## [13] \u0026quot;2018-04-22-predatory-scientometrics.Rmd\u0026quot; ## [14] \u0026quot;2018-04-22-predatory-scientometrics.Rmd.lock~\u0026quot; ## [15] \u0026quot;2018-05-12-Investing-Long-Run.Rmd\u0026quot; ## [16] \u0026quot;2018-05-12-Investing-Long-Run.Rmd.lock~\u0026quot; ## [17] \u0026quot;2018-06-12-padfR-ed2.Rmd\u0026quot; ## [18] \u0026quot;2018-06-12-padfR-ed2.Rmd.lock~\u0026quot; ## [19] \u0026quot;2018-06-29-BenchmarkingSSD.Rmd\u0026quot; ## [20] \u0026quot;2018-06-29-BenchmarkingSSD.Rmd.lock~\u0026quot; ## [21] \u0026quot;2018-10-10-BatchGetSymbols-NewVersion.Rmd\u0026quot; ## [22] \u0026quot;2018-10-10-BatchGetSymbols-NewVersion.Rmd.lock~\u0026quot; ## [23] \u0026quot;2018-10-11-Update-GetLattesData.Rmd\u0026quot; ## [24] \u0026quot;2018-10-11-Update-GetLattesData.Rmd.lock~\u0026quot; ## [25] \u0026quot;2018-10-13-NewPackage-PkgsFromFiles.Rmd\u0026quot; ## [26] \u0026quot;2018-10-13-NewPackage-PkgsFromFiles.Rmd.lock~\u0026quot; ## [27] \u0026quot;2018-10-19-R-and-loops.Rmd\u0026quot; ## [28] \u0026quot;2018-10-19-R-and-loops.Rmd.lock~\u0026quot; ## [29] \u0026quot;2018-10-20-Linux-and-R.Rmd\u0026quot; ## [30] \u0026quot;2018-10-20-Linux-and-R.Rmd.lock~\u0026quot; ## [31] \u0026quot;2018-11-03-NewBlog.Rmd\u0026quot; ## [32] \u0026quot;2018-11-03-NewBlog.Rmd.lock~\u0026quot; ## [33] \u0026quot;2018-11-03-RstudioTricks.Rmd\u0026quot; ## [34] \u0026quot;2018-11-03-RstudioTricks.Rmd.lock~\u0026quot; ## [35] \u0026quot;2019-01-08-Looking-Back-2018.Rmd\u0026quot; ## [36] \u0026quot;2019-01-08-Looking-Back-2018.Rmd.lock~\u0026quot; ## [37] \u0026quot;2019-01-12-GetDFPData-ver14.Rmd\u0026quot; ## [38] \u0026quot;2019-01-12-GetDFPData-ver14.Rmd.lock~\u0026quot; ## [39] \u0026quot;2019-03-09-pafdR-promotion.Rmd\u0026quot; ## [40] \u0026quot;2019-03-09-pafdR-promotion.Rmd.lock~\u0026quot; ## [41] \u0026quot;2019-03-10-pafdR-promotion_2.Rmd\u0026quot; ## [42] \u0026quot;2019-03-10-pafdR-promotion_2.Rmd.lock~\u0026quot; ## [43] \u0026quot;2019-03-23-Bettina-Case.Rmd\u0026quot; ## [44] \u0026quot;2019-03-23-Bettina-Case.Rmd.lock~\u0026quot; ## [45] \u0026quot;2019-04-13-Parallel-BatchGetsymbols.Rmd\u0026quot; ## [46] \u0026quot;2019-04-13-Parallel-BatchGetsymbols.Rmd.lock~\u0026quot; ## [47] \u0026quot;2019-04-15-GetBCBData.Rmd\u0026quot; ## [48] \u0026quot;2019-04-15-GetBCBData.Rmd.lock~\u0026quot; ## [49] \u0026quot;2019-05-01-MeanVariance.Rmd\u0026quot; ## [50] \u0026quot;2019-05-01-MeanVariance.Rmd.lock~\u0026quot; ## [51] \u0026quot;2019-05-17-R-in-Brazil.Rmd\u0026quot; ## [52] \u0026quot;2019-05-17-R-in-Brazil.Rmd.lock~\u0026quot; ## [53] \u0026quot;2019-05-20-Lindy-Effect.Rmd\u0026quot; ## [54] \u0026quot;2019-05-20-Lindy-Effect.Rmd.lock~\u0026quot; ## [55] \u0026quot;2019-07-01-ftp-shutdown.Rmd\u0026quot; ## [56] \u0026quot;2019-07-01-ftp-shutdown.Rmd.lock~\u0026quot; ## [57] \u0026quot;2019-08-08-ftp-NOT-shutdown.Rmd\u0026quot; ## [58] \u0026quot;2019-08-08-ftp-NOT-shutdown.Rmd.lock~\u0026quot; ## [59] \u0026quot;2019-10-01-new-package-GetQuandlData.Rmd\u0026quot; ## [60] \u0026quot;2019-10-01-new-package-GetQuandlData.Rmd.lock~\u0026quot; ## [61] \u0026quot;2019-10-12-support-GetDFPData-shiny.Rmd\u0026quot; ## [62] \u0026quot;2019-10-12-support-GetDFPData-shiny.Rmd.lock~\u0026quot; ## [63] \u0026quot;2019-10-16-new-package-GetEdgarData.Rmd\u0026quot; ## [64] \u0026quot;2019-10-16-new-package-GetEdgarData.Rmd.lock~\u0026quot; ## [65] \u0026quot;2019-11-01-new-package-simfinR.Rmd\u0026quot; ## [66] \u0026quot;2019-11-01-new-package-simfinR.Rmd.lock~\u0026quot; ## [67] \u0026quot;2019-11-25-feedback-TOC-afedR.Rmd\u0026quot; ## [68] \u0026quot;2019-11-25-feedback-TOC-afedR.Rmd.lock~\u0026quot; ## [69] \u0026quot;2019-12-02-dynamic-exercises-afedR.Rmd\u0026quot; ## [70] \u0026quot;2019-12-02-dynamic-exercises-afedR.Rmd.lock~\u0026quot; ## [71] \u0026quot;2019-12-15-Looking-Back-2019.Rmd\u0026quot; ## [72] \u0026quot;2019-12-15-Looking-Back-2019.Rmd.lock~\u0026quot; ## [73] \u0026quot;2020-01-15-afedR-ed2-announcement.Rmd\u0026quot; ## [74] \u0026quot;2020-01-15-afedR-ed2-announcement.Rmd.lock~\u0026quot; ## [75] \u0026quot;2020-02-25-afedR-ed2-slides-available.Rmd\u0026quot; ## [76] \u0026quot;2020-02-25-afedR-ed2-slides-available.Rmd.lock~\u0026quot; ## [77] \u0026quot;2020-03-29-garch-tutorial-in-r.Rmd\u0026quot; ## [78] \u0026quot;2020-03-29-garch-tutorial-in-r.Rmd.lock~\u0026quot; ## [79] \u0026quot;2020-04-17-update-getdfpdata.Rmd\u0026quot; ## [80] \u0026quot;2020-04-17-update-getdfpdata.Rmd.lock~\u0026quot; ## [81] \u0026quot;2020-04-20-free-compiled-data-in-site.Rmd\u0026quot; ## [82] \u0026quot;2020-04-20-free-compiled-data-in-site.Rmd.lock~\u0026quot; ## [83] \u0026quot;2020-04-20-new-package-GetCVMData.Rmd\u0026quot; ## [84] \u0026quot;2020-04-20-new-package-GetCVMData.Rmd.lock~\u0026quot; ## [85] \u0026quot;2020-04-25-investments-costs.Rmd\u0026quot; ## [86] \u0026quot;2020-04-25-investments-costs.Rmd.lock~\u0026quot; ## [87] \u0026quot;2020-05-24-pirf-is-online.Rmd\u0026quot; ## [88] \u0026quot;2020-05-24-pirf-is-online.Rmd.lock~\u0026quot; ## [89] \u0026quot;2020-05-27-call-for-papers-rac.Rmd\u0026quot; ## [90] \u0026quot;2020-05-27-call-for-papers-rac.Rmd.lock~\u0026quot; ## [91] \u0026quot;2020-07-07-garch-tutorial-in-r-REVISED.Rmd\u0026quot; ## [92] \u0026quot;2020-07-07-garch-tutorial-in-r-REVISED.Rmd.lock~\u0026quot; ## [93] \u0026quot;2020-07-18-new_packages-GetFREData-GetDFPData2.Rmd\u0026quot; ## [94] \u0026quot;2020-07-18-new_packages-GetFREData-GetDFPData2.Rmd.lock~\u0026quot; ## [95] \u0026quot;2020-12-22-Looking-Back-2020.Rmd\u0026quot; ## [96] \u0026quot;2020-12-22-Looking-Back-2020.Rmd.lock~\u0026quot; ## [97] \u0026quot;2021-02-18-dynamic-exercises-adfeR.Rmd\u0026quot; ## [98] \u0026quot;2021-02-18-dynamic-exercises-adfeR.Rmd.lock~\u0026quot; ## [99] \u0026quot;2021-02-19-dynamic-exercises-adfeR.Rmd.lock~\u0026quot; ## [100] \u0026quot;2021-02-20-adfeR-ed3-announcement.Rmd\u0026quot; ## [101] \u0026quot;2021-02-20-adfeR-ed3-announcement.Rmd.lock~\u0026quot; ## [102] \u0026quot;2021-02-28-dynamic-exercises-afedR.Rmd\u0026quot; ## [103] \u0026quot;2021-02-28-dynamic-exercises-afedR.Rmd.lock~\u0026quot; The blog started in january 2017 and, over time, I wrote 103 posts. That feels alright. I’m not felling forced to write and I do it whenever I fell like I have something to share.\nLet’s get more information from the .Rmd files. I’ll write function read_blog_files and use it for all post files.\nread_blog_files \u0026lt;- function(f.in) { require(tidyverse) my.front.matter \u0026lt;- rmarkdown::yaml_front_matter(f.in) df.out \u0026lt;- data_frame(post.title = my.front.matter$title, post.date = lubridate::ymd(my.front.matter$date), post.month = as.Date(format(post.date, \u0026#39;%Y-%m-01\u0026#39;)), tags = paste0(my.front.matter$tags, collapse = \u0026#39;;\u0026#39;), categories = paste0(my.front.matter$categories, collapse = \u0026#39;;\u0026#39;), content = paste0(read_lines(f.in), collapse = \u0026#39; \u0026#39;)) return(df.out) } df.posts \u0026lt;- dplyr::bind_rows(purrr::map(post.files, read_blog_files)) ## Loading required package: tidyverse ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.3.3 ✔ purrr 0.3.4 ## ✔ tibble 3.1.0 ✔ dplyr 1.0.4 ## ✔ tidyr 1.1.2 ✔ stringr 1.4.0 ## ✔ readr 1.4.0 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## Warning: `data_frame()` was deprecated in tibble 1.1.0. ## Please use `tibble()` instead. glimpse(df.posts) ## Rows: 51 ## Columns: 6 ## $ post.title \u0026lt;chr\u0026gt; \u0026quot;Writing a R book and self-publishing it in Amazon\u0026quot;, \u0026quot;Packa… ## $ post.date \u0026lt;date\u0026gt; 2017-02-16, 2017-12-06, 2017-12-13, 2017-12-30, 2018-01-22… ## $ post.month \u0026lt;date\u0026gt; 2017-02-01, 2017-12-01, 2017-12-01, 2017-12-01, 2018-01-01… ## $ tags \u0026lt;chr\u0026gt; \u0026quot;R;book;self-publish\u0026quot;, \u0026quot;R;GetDFPData;corporate events;finan… ## $ categories \u0026lt;chr\u0026gt; \u0026quot;R;book;self-publish\u0026quot;, \u0026quot;R;GetDFPData;B3\u0026quot;, \u0026quot;R;shiny;webserve… ## $ content \u0026lt;chr\u0026gt; \u0026quot;--- title: \\\u0026quot;Writing a R book and self-publishing it in Am… First, we’ll look at the frequency of posts over time.\ndf.posts.2018 \u0026lt;- df.posts %\u0026gt;% filter(post.date \u0026gt; as.Date(\u0026#39;2018-01-01\u0026#39;)) print( ggplot(df.posts.2018, aes(x = post.month)) + geom_histogram(stat=\u0026#39;count\u0026#39;) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + labs(y = \u0026#39;Number of posts\u0026#39;, x = \u0026#39;\u0026#39;)) ## Warning: Ignoring unknown parameters: binwidth, bins, pad Seems to average about once a month. The blank spaces show that I did not write for a couple of months.\n Checking 2018’s plans In the end of 2017 my plans for 2018 were:\nWork on the second edition of the portuguese book.\nDone! I’m glad to report that the second edition of the book was published in June 2018. It was great to review the book and add several new chapters and sections. As I mentioned in the publication post, this is the largest and longest project I ever worked and it is very satisfying to see it develop over time. Even more satisfying is to receive positive feedback of readers that are reading and using the book to learn to code in R! Many teachers in Economics and Business are also starting to use it in the classroom.\nThe book will continue to be update every couple of years. One of the greatest things about R, among many others, is that the language is continually evolving and changing. I have no doubt that there will always be new material to write about.\nStart a portal for financial data in Brazil\nUnfortunately this project did not launch. I wrote a couple of R scripts for fetching and saving data automatically in my server but it never became a webpage. I started to work on other projects and the website was not a priority.\n Plans for 2019 New edition of “Processing and Analyzing Financial Data with R”\nThe international version of my book pafdR was published in january 2017. I fell its time to update it with the new chapters and structure from the second edition in portuguese. There are many improvements to the book, with an emphasis in the tidyverse universe.\nWork on my new book: “Investing For the Long Term” (provisory title)\nThere is a huge deficit of financial knowledge in Brazil, specially in saving and investing. I’ve been a long term investor for most of my career as an academic and I fell there is a lot I can contribute to the topic of financial education by bringing data science into the problem of investing.\nThe book will be a introduction to investments for the common person in Brazil, with a heavy data-based approach. It will not be about trading strategies or anything related to short term trading. The idea is to bring data analysis for the common long term investor, showing how the financial market works and how one can build passive income by constantly buying good financial contracts.\nI have no clue if it will be published em 2019. Unlike my previous book, I’m taking my time to write this one. No rush and no deadlines :).\nSolidify my research agenda in Board Composition\nAs I mentioned before, my research agenda has shifted from capital markets to board compositions. This is a very interesting topic with many implications for listed companies. I’m leaning a lot from researching into these topics.\nCurrently, I have four initiatives with different co-authors:\n Gender and board composition Politics and board composition Professors in the Board of Companies Board description of Brazilian Companies  Hoepfully, these will be published in 2019 or 2020.\n ","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546905600,"objectID":"7cadb91817940b4c6101021974232924","permalink":"http://www.msperlin.com/blog/post/2019-01-08-looking-back-2018/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/blog/post/2019-01-08-looking-back-2018/","section":"post","summary":"At the end of every year I plan to write about the highlight of the current year and set plans for the future. First, let’s talk about my work in 2018.\nHighlights of 2018 Research wise, my scientometrics paper Is predatory publishing a real threat? Evidence from a large database study was featured in many news outlets. Its altmetric page is doing great, with over 1100 downloads and featured at top 5% of all research output measured by altmetric. This is, by far, the most impactful research piece I ever wrote.","tags":["R","site"],"title":"Looking back at 2018 and plans for 2019","type":"post"},{"authors":null,"categories":["R","site"],"content":"  I while ago I wrote about purchasing my own webserver in digital ocean and hosting my shinny applications. Last week I finally got some time to migrate my blog from Github to my new domain, www.msperlin.com. While doing that, I also decided to change the technology behind making the blog, from Jekyll to Hugo. Here are my reasons.\nJekyll is great for making simple static sites, specially with this template from Dean Attali. It was easy to set it up and host it in Github. My problems with this configuration are:\n With Jekyll, all posts in the blog are created as .md files and not as original .Rmd. For every post with R code I had to compile the .Rmd file to .md and manually add figures and other files to the site directory. In Hugo, and using blogdown, I can compile it directly from Rmarkdown (.Rmd).\n The Jekyll template in msperlin.github.io is very clean and simple. This is exactly what I needed at the time. Hugo’s template academic is specially made for academics and offers lots of new functionalities.\n The github internet address doesn’t fell very profissional. I know this can personal opinion but a custom url tells me that the author has spend some time and money to set its webpage properly.\n There is a lag between writing changes in files and seeing it live in msperlin.github.io. This can get annoying. With my own domain, all changes are instantaneous.\n  Hosting the static website in www.msperlin.com is very easy. I wrote a simple bash script that compiles the site locally using blogdown::build_site() and copy all files to my server with ssh. Going further, I also added documentation about my CRAN packages using pkgdown. See an example for PkgsFromFiles here.\nI am very pleased with this setup and I hope that I won’t need to change it in the next couple of years. The transition is a lot of work! I had to recompile all Rmd posts and copy and paste all other textual content.\nIf you are a teacher or researcher, you really should look into Hugo-Academic. This is specially true if you use R, as you can integrate you Rmd files with blogdown.\n","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541203200,"objectID":"e1e5a8661620f962e420a038794d79a2","permalink":"http://www.msperlin.com/blog/post/2018-11-03-newblog/","publishdate":"2018-11-03T00:00:00Z","relpermalink":"/blog/post/2018-11-03-newblog/","section":"post","summary":"I while ago I wrote about purchasing my own webserver in digital ocean and hosting my shinny applications. Last week I finally got some time to migrate my blog from Github to my new domain, www.msperlin.com. While doing that, I also decided to change the technology behind making the blog, from Jekyll to Hugo. Here are my reasons.\nJekyll is great for making simple static sites, specially with this template from Dean Attali. It was easy to set it up and host it in Github. My problems with this configuration are:","tags":["R","site"],"title":"New blog site: From Jekyll to Hugo","type":"post"},{"authors":null,"categories":["R","rstudio"],"content":"  I’ve been using Rstudio for a long time and I got some tricks to share. These are simple and useful commands and shortcuts that really help the productivity of my students. If you got a suggestion of trick, use the comment section and I’ll add it in this post.\nPackage rstudioapi When using Rstudio, package rstudioapi gives you lots of information about your session. The most useful one is the script location. You can use it to automatically change the working folder to where you have the file locally saved.\nFunction rstudioapi::getActiveDocumentContext gives you details about the file being currently edited in RStudio. Have a look:\nmy.d \u0026lt;- rstudioapi::getActiveDocumentContext() print(my.d) ## Document Context: ## - id: \u0026#39;FA202F79\u0026#39; ## - path: \u0026#39;~/Dropbox/11-My Website/www.msperlin.com-blog/content/post/2018-11-03-RstudioTricks.Rmd\u0026#39; ## - contents: \u0026lt;69 rows\u0026gt; ## Document Selection: ## - [24, 6] -- [24, 6]: \u0026#39;\u0026#39; You can see that the file location is available in path. Let’s grab it:\nmy.file.location \u0026lt;- rstudioapi::getActiveDocumentContext()$path Now, if we want the name of the directory, just call dirname(my.file.location):\nmy.dir \u0026lt;- dirname(my.file.location) print(my.dir) ## [1] \u0026quot;/home/msperlin/Dropbox/11-My Website/www.msperlin.com-blog/content/post\u0026quot; So, if you want to change the working directory automatically to where the script is locally saved, just write:\nmy.dir \u0026lt;- dirname(rstudioapi::getActiveDocumentContext()$path) setwd(my.dir) This is very practical and I use it in all of my R scripts. If you copy the script to another folder, it will run without any directory problem. If you send the script to someone else within a zipped folder, he/she can run it without modifications as the working directory will change automatically.\nBe aware, however, this only works in RStudio. If you run the code without the IDE, in a bash script for example, package rstudioapi will not be available. In this case, you’ll need to set the directory explicitly.\n Dark theme for Rstudio A dark theme is a productivity life-changer if you spend a lot of time in front of a computer. Before I used it, my eyes were always strained after a long period of work. By the end of the day, using tablets or even my phone was disconforting. You can change the theme in Rstudio by going into “tools” -\u0026gt; “global options” -\u0026gt; “appearance”. There are many dark themes available. Pick one that pleases you the most. See the difference between a white and dark theme next:\nGoing further, I also advise to change the theme of you operating system. I can assure you that, in the long run, it is worth it!\n Autocomplete (tab) is your friend! A commom misconception about programming is that you must memorize lot of names. This is far from the truth. You never need to memorize anything when using Rstudio! From function arguments to variable names and names of files, everything can be searched by pressing the tab key on your keyboard. When using naming conventions for functions and objects, this becomes even more useful. For example, every dataframe in my code starts with “df”, like in “df.prices”, “df.tickers” and so on. When I’m looking for the name of a dataframe, I just write “df.” and press tab. The result is a list of object names.\nThe autocomplete function also works for function arguments, directory and file locations and packages. In my book I have a whole section about it. Check it out.\nAutocomplete for files\n Autocomplete for packages\n  Section naming with ---- You can name a section in any R script in Rstudio using the textual clue ----. This section will show up in the bottom left of the RStudio editor screen. When you want to jump to that section, just press the key. So, you can organized your code with sections like this:\n# Get data ---- ## code here # clean data ---- ## code here # report results ---- ## code here  ","date":1541203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541203200,"objectID":"c24e8ffa0b85aa29ddcf92080cdd6d64","permalink":"http://www.msperlin.com/blog/post/2018-11-03-rstudiotricks/","publishdate":"2018-11-03T00:00:00Z","relpermalink":"/blog/post/2018-11-03-rstudiotricks/","section":"post","summary":"I’ve been using Rstudio for a long time and I got some tricks to share. These are simple and useful commands and shortcuts that really help the productivity of my students. If you got a suggestion of trick, use the comment section and I’ll add it in this post.\nPackage rstudioapi When using Rstudio, package rstudioapi gives you lots of information about your session. The most useful one is the script location. You can use it to automatically change the working folder to where you have the file locally saved.","tags":["R","rstudio"],"title":"Some Useful Tricks in RStudio","type":"post"},{"authors":null,"categories":["R","tutorial"],"content":"  Loops in R First, if you are new to programming, you should know that loops are a way to tell the computer that you want to repeat some operation for a number of times. This is a very common task that can be found in many programming languages. For example, let’s say you invited five friends for dinner at your home and the whole cost of four pizzas will be split evenly. Assume now that you must give instructions to a computer on calculating how much each one will pay at the end of dinner. For that, you need to sum up the individual tabs and divide by the number of people. Your instructions to the computer could be: start with a value of x = zero, take each individual pizza cost and sum it to x until all costs are processed, dividing the result by the number of friends at the end.\nThe great thing about loops is that the length of it is dynamically set. Using the previous example, if we had 500 friends (and a large dinner table!), we could use the same instructions for calculating the individual tabs. That means we can encapsulate a generic procedure for processing any given number of friends at dinner. With it, you have at your reach a tool for the execution of any sequential process. In other words, you are the boss of your computer and, as long as you can write it down clearly, you can set it to do any kind of repeated task for you.\nNow, about the code, we could write the solution to the pizza problem in R as:\npizza.costs \u0026lt;- c(50, 80, 30, 60) # each cost of pizza n.friends \u0026lt;- 5 # number of friends x \u0026lt;- 0 # set first cost to zero for (i.cost in pizza.costs) { x \u0026lt;- x + i.cost # sum it up } x \u0026lt;- x/n.friends # divide for average per friend print(x) ## [1] 44 Don’t worry if you didn’t understand the code. We’ll get to the structure of a loop soon.\nBack to our case, each friend would pay 44 for the meal. We can check the result against function sum:\nx == sum(pizza.costs)/n.friends ## [1] TRUE The output TRUE shows that the results are equal.\nThe Structure of a Loop Knowing how to use loops can be a powerful ally in a complex data related problem. Let’s talk more about how loops are defined in R. The structure of a loop in R follows:\nfor (i in i.vec){ ... } In the previous code, command for indicates the beginning of a loop. Object i in (i in i.vec) is the iterator of the loop. This iterator will change its value in each iteration, taking each individual value contained in i.vec. Note the loop is encapsulated by curly braces ({}). These are important, as they define where the loop starts and where it ends. The indentation (use of bigger margins) is also important for visual cues, but not necessary. Consider the following practical example:\n# set seq my.seq \u0026lt;- seq(-5,5) # do loop for (i in my.seq){ cat(paste(\u0026#39;\\nThe value of i is\u0026#39;,i)) } ## ## The value of i is -5 ## The value of i is -4 ## The value of i is -3 ## The value of i is -2 ## The value of i is -1 ## The value of i is 0 ## The value of i is 1 ## The value of i is 2 ## The value of i is 3 ## The value of i is 4 ## The value of i is 5 In the code, we created a sequence from -5 to 5 and presented a text for each element with the cat function. Notice how we also broke the prompt line with '\\n'. The loop starts with i=-5, execute command cat(paste('\\nThe value of i is', -5)), proceed to the next iteration by setting i=-4, rerun the cat command, and so on. At its final iteration, the value of i is 5.\nThe iterated sequence in the loop is not exclusive to numerical vectors. Any type of vector or list may be used. See next:\n# set char vec my.char.vec \u0026lt;- letters[1:5] # loop it! for (i.char in my.char.vec){ cat(paste(\u0026#39;\\nThe value of i.char is\u0026#39;, i.char)) } ## ## The value of i.char is a ## The value of i.char is b ## The value of i.char is c ## The value of i.char is d ## The value of i.char is e The same goes for lists:\n# set list my.l \u0026lt;- list(x = 1:5, y = c(\u0026#39;abc\u0026#39;,\u0026#39;dfg\u0026#39;), z = factor(\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;D\u0026#39;)) # loop list for (i.l in my.l){ cat(paste0(\u0026#39;\\nThe class of i.l is \u0026#39;, class(i.l), \u0026#39;. \u0026#39;)) cat(paste0(\u0026#39;The number of elements is \u0026#39;, length(i.l), \u0026#39;.\u0026#39;)) } ## ## The class of i.l is integer. The number of elements is 5. ## The class of i.l is character. The number of elements is 2. ## The class of i.l is factor. The number of elements is 1. In the definition of loops, the iterator does not have to be the only object incremented in each iteration. We can create other objects and increment them using a simple sum operation. See next:\n# set vec and iterators my.vec \u0026lt;- seq(1:5) my.x \u0026lt;- 5 my.z \u0026lt;- 10 for (i in my.vec){ # iterate \u0026quot;manually\u0026quot; my.x \u0026lt;- my.x + 1 my.z \u0026lt;- my.z + 2 cat(\u0026#39;\\nValue of i = \u0026#39;, i, \u0026#39; | Value of my.x = \u0026#39;, my.x, \u0026#39; | Value of my.z = \u0026#39;, my.z) } ## ## Value of i = 1 | Value of my.x = 6 | Value of my.z = 12 ## Value of i = 2 | Value of my.x = 7 | Value of my.z = 14 ## Value of i = 3 | Value of my.x = 8 | Value of my.z = 16 ## Value of i = 4 | Value of my.x = 9 | Value of my.z = 18 ## Value of i = 5 | Value of my.x = 10 | Value of my.z = 20 Using nested loops, that is, a loop inside of another loop is also possible. See the following example, where we present all the elements of a matrix:\n# set matrix my.mat \u0026lt;- matrix(1:9, nrow = 3) # loop all values of matrix for (i in seq(1,nrow(my.mat))){ for (j in seq(1,ncol(my.mat))){ cat(paste0(\u0026#39;\\nElement [\u0026#39;, i, \u0026#39;, \u0026#39;, j, \u0026#39;] = \u0026#39;, my.mat[i,j])) } } ## ## Element [1, 1] = 1 ## Element [1, 2] = 4 ## Element [1, 3] = 7 ## Element [2, 1] = 2 ## Element [2, 2] = 5 ## Element [2, 3] = 8 ## Element [3, 1] = 3 ## Element [3, 2] = 6 ## Element [3, 3] = 9  A Real World Example Now, the computational needs of the real world is far more complex than dividing a dinner expense. A practical example of using loops is processing data according to groups. Using an example from Finance, if we have a return dataset for several stocks and we want to calculate the average return of each stock, we can use a loop for that. In this example, we will use Yahoo Finance data from three stocks: FB, GE and AA. The first step is downloading it with package BatchGetSymbols.\nlibrary(BatchGetSymbols) ## Loading required package: rvest ## Loading required package: xml2 ## Loading required package: dplyr ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union ##  my.tickers \u0026lt;- c(\u0026#39;FB\u0026#39;, \u0026#39;GE\u0026#39;, \u0026#39;AA\u0026#39;) df.stocks \u0026lt;- BatchGetSymbols(tickers = my.tickers, first.date = \u0026#39;2012-01-01\u0026#39;, freq.data = \u0026#39;yearly\u0026#39;)[[2]] ## ## Running BatchGetSymbols for: ## ## tickers =FB, GE, AA ## Downloading data for benchmark ticker ## ^GSPC | yahoo (1|1) | Found cache file ## FB | yahoo (1|3) | Not Cached | Saving cache - Got 95% of valid prices | Nice! ## GE | yahoo (2|3) | Not Cached | Saving cache - Got 100% of valid prices | Good job! ## AA | yahoo (3|3) | Not Cached | Saving cache - Got 100% of valid prices | Good job! It worked fine. Let’s check the contents of the dataframe:\ndplyr::glimpse(df.stocks) ## Rows: 27 ## Columns: 10 ## $ ticker \u0026lt;chr\u0026gt; \u0026quot;AA\u0026quot;, \u0026quot;AA\u0026quot;, \u0026quot;AA\u0026quot;, \u0026quot;AA\u0026quot;, \u0026quot;AA\u0026quot;, \u0026quot;AA\u0026quot;, \u0026quot;AA\u0026quot;, \u0026quot;AA\u0026quot;, \u0026quot;… ## $ ref.date \u0026lt;date\u0026gt; 2012-01-03, 2013-01-02, 2014-01-02, 2015-01-02, … ## $ volume \u0026lt;dbl\u0026gt; 2217410500, 2149575500, 2146821400, 2683551700, 2… ## $ price.open \u0026lt;dbl\u0026gt; 21.482821, 21.338640, 25.303591, 38.135609, 22.87… ## $ price.high \u0026lt;dbl\u0026gt; 25.85628, 25.68807, 42.29280, 41.01921, 32.05000,… ## $ price.low \u0026lt;dbl\u0026gt; 19.272060, 18.503099, 24.270300, 18.791460, 16.19… ## $ price.close \u0026lt;dbl\u0026gt; 22.179689, 21.602970, 25.303591, 38.159641, 23.33… ## $ price.adjusted \u0026lt;dbl\u0026gt; 20.893423, 20.621868, 24.485683, 37.242069, 23.00… ## $ ret.adjusted.prices \u0026lt;dbl\u0026gt; NA, -0.01299715, 0.18736494, 0.52097326, -0.38221… ## $ ret.closing.prices \u0026lt;dbl\u0026gt; NA, -0.02600212, 0.17130149, 0.50807215, -0.38853… All financial data is there. Notice that the return series is available at column ret.adjusted.prices.\nNow we will use a loop to build a table with the mean return of each stock:\n# find unique tickers in column ticker unique.tickers \u0026lt;- unique(df.stocks$ticker) # create empty df tab.out \u0026lt;- data.frame() # loop tickers for (i.ticker in unique.tickers){ # create temp df with ticker i.ticker temp \u0026lt;- df.stocks[df.stocks$ticker==i.ticker, ] # row bind i.ticker and mean.ret tab.out \u0026lt;- rbind(tab.out, data.frame(ticker = i.ticker, mean.ret = mean(temp$ret.adjusted.prices, na.rm = TRUE))) } # print result print(tab.out) ## ticker mean.ret ## 1 AA 0.09646911 ## 2 FB 0.30161873 ## 3 GE 0.05304138 In the code, we used function unique to find out the names of all the tickers in the dataset. Soon after, we create an empty dataframe to save the results and a loop to filter the data of each stock sequentially and average its returns. At the end of the loop, we use function rbind to paste the results of each stock with the results of the main table. As you can see, we can use the data to perform group calculations with loop.\nBy now, I must be forward in saying that the previous loop is by no means the best way of performing the data operation. What we just did by loops is called a split-apply-combine procedure. There are base function in R such as tapply, split and lapply/sapply that can do the same job but with a more intuitive and functional approach. Going further, functions from package tidyverse can do the same procuedure with an even more intuitive approach. In a future post I shall discuss this possibilities further.\nI hope you guys liked the post. Got a question? Just drop it at the comment section.\n  ","date":1539907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539907200,"objectID":"574547876b2619c9939b95f837c25731","permalink":"http://www.msperlin.com/blog/post/2018-10-19-r-and-loops/","publishdate":"2018-10-19T00:00:00Z","relpermalink":"/blog/post/2018-10-19-r-and-loops/","section":"post","summary":"Loops in R First, if you are new to programming, you should know that loops are a way to tell the computer that you want to repeat some operation for a number of times. This is a very common task that can be found in many programming languages. For example, let’s say you invited five friends for dinner at your home and the whole cost of four pizzas will be split evenly. Assume now that you must give instructions to a computer on calculating how much each one will pay at the end of dinner.","tags":["R","loops","tutorial"],"title":"Loops and Pizzas","type":"post"},{"authors":null,"categories":["R","PkgsFromFiles"],"content":"  Its been a while since I develop a CRAN package and this weekend I decided to work on a idea I had some time ago. The result is package PkgsFromFiles.\nWhen working with different computers at home or work, one of the problems I have is installing missing packages across different computers. As an example, a script that works in my work computer may not work in my home computer. This is specially annoying when I have a fresh install of the operating system or R. In this case, I must manually install all packages, case by case. Instead of focusing on the script at hand, I spend considerable time finding and installing missing packages. When using laptops for teaching R, many times I had to wait for the installation of a package before continuing the class. With my new package, PkgsFromFiles, I can scan any folder of my computer and install all necessary packages before using them, as we will soon learn.\nOne of the available solutions to this problem is to use package pacman. It includes function p_load that will check if a package is available and, if not, install it from CRAN. However, for me, I like using library and require as it is consistent with my code format. Also, in a fresh R install, I rather install all my required packages in a single run so that I don’t have to wait later.\nPackage PkgsFromFiles solves this issue by finding and parsing all R related files (.R, .Rmd, .Rnw) from a given folder. It finds all calls to library() and require() and installs all packages that are not available locally.\nInstallation # from cran (soon!) install.packages(\u0026#39;PkgsFromFiles\u0026#39;) # from github if (!require(devtools)) install.packages(\u0026#39;devtools\u0026#39;) devtools::install_github(\u0026#39;msperlin/PkgsFromFiles\u0026#39;)  Usage The main function of the package is pff_find_and_install_pkgs, which will search and install missing packages from R files at a given directory. As an example, we’ll use my research folder from Dropbox. It contains all R scripts I have ever used in my research work. Let’s try it out:\n# Evaluation is disable so it passes CRAN CHECKS, but you should be able to run it in your computer library(PkgsFromFiles) # target folder my.dir \u0026lt;- \u0026#39;~/Dropbox/01-Pesquisa/\u0026#39; df \u0026lt;- pff_find_and_install_pkgs(folder.in = my.dir) ## ## Searching folder ~/Dropbox/01-Pesquisa/ ## Found 74 files in 18 folders ## R Scripts: 72 files ## Rmarkdown files: 2 files ## Sweave files: 0 files ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. ## ## Checking available pkgs from https://cloud.r-project.org ## Checking and installing missing pkgs ## Installing rvest Already installed ## Installing tidyverse Already installed ## Installing furrr Already installed ## Installing XML Already installed ## Installing fst Already installed ## Installing stringr Already installed ## Installing lubridate Already installed ## Installing ggplot2 Already installed ## Installing GetDFPData Already installed ## Installing genderBR Already installed ## Installing purrr Already installed ## Installing xlsx Already installed ## Installing sandwich Already installed ## Installing stargazer Already installed ## Installing Hmisc Already installed ## Installing plm Already installed ## Installing lmtest Already installed ## Installing MatchIt Already installed ## Installing devtools Already installed ## Installing RSelenium Already installed ## Installing GetLattesData Already installed ## Installing xtable Already installed ## Installing httr Already installed ## Installing parallel Installation failed, pkg not in CRAN ## Installing BatchGetSymbols Already installed ## Installing readxl Already installed ## Installing RSQLite Already installed ## Installing pbapply Already installed ## Installing ggmap Already installed ## Installing memoise Already installed ## Installing gganimate Already installed ## Installing texreg Already installed ## Installing pglm Already installed ## Installing estimatr Already installed ## Installing AER Already installed ## Installing quantreg Already installed ## Installing nnet Already installed ## Installing simfinR Already installed ## Installing fGarch Already installed ## Installing MTS Already installed ## Installing DescTools Already installed ## ## Summary: ## Found 40 packages already installed ## Had to install 0 packages ## Installation failed for 1 packages ## 1 due to package not being found in CRAN ## 0 due to missing dependencies or other problems ## ## Check output dataframe for more details about failed packages As you can see, function pff_find_and_install_pkgs will find all R related files recursively in the given folder. In this case, I have all packages locally so no installation was required. A summary in text is shown at the end of execution.\nThe output of the function is a dataframe with the details of the operation. Have a look:\ndplyr::glimpse(df) ## Rows: 41 ## Columns: 3 ## $ pkg \u0026lt;chr\u0026gt; \u0026quot;rvest\u0026quot;, \u0026quot;tidyverse\u0026quot;, \u0026quot;furrr\u0026quot;, \u0026quot;XML\u0026quot;, \u0026quot;fst\u0026quot;, \u0026quot;stringr\u0026quot;… ## $ status.message \u0026lt;chr\u0026gt; \u0026quot;Already installed\u0026quot;, \u0026quot;Already installed\u0026quot;, \u0026quot;Already ins… ## $ installation \u0026lt;lgl\u0026gt; TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, … The package also includes function pff_find_R_files_from_folder, which will find all packages used in R related files from a given folder. It outputs a dataframe with several information about packages used in the found scripts.\ndf.files \u0026lt;- pff_find_R_files_from_folder(folder.in = my.dir) ## ## Searching folder ~/Dropbox/01-Pesquisa/ ## Found 74 files in 18 folders ## R Scripts: 72 files ## Rmarkdown files: 2 files ## Sweave files: 0 files dplyr::glimpse(df.files) ## Rows: 74 ## Columns: 5 ## $ files \u0026lt;chr\u0026gt; \u0026quot;/home/msperlin/Dropbox/01-Pesquisa//01-Working Papers/01-… ## $ file.names \u0026lt;chr\u0026gt; \u0026quot;01-01_S-unzip_affiliation_tables.R\u0026quot;, \u0026quot;01-02_S-read_affili… ## $ extensions \u0026lt;chr\u0026gt; \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;, \u0026quot;R\u0026quot;… ## $ pkgs \u0026lt;chr\u0026gt; \u0026quot;rvest ; tidyverse ; furrr ; XML\u0026quot;, \u0026quot;tidyverse ; furrr ; fs… ## $ n.pkgs \u0026lt;int\u0026gt; 4, 3, 8, 6, 6, 6, 8, 8, 8, 1, 1, 8, 8, 8, 4, 1, 7, 9, 4, 8… I also wrote a simple function for plotting the most used packages for a given folder:\n# target folder my.dir \u0026lt;- \u0026#39;~/Dropbox/01-Pesquisa/\u0026#39; # plot most used pkgs p \u0026lt;- pff_plot_summary_pkgs(folder.in = my.dir) ## ## Searching folder ~/Dropbox/01-Pesquisa/ ## Found 74 files in 18 folders ## R Scripts: 72 files ## Rmarkdown files: 2 files ## Sweave files: 0 files print(p) As you can see, I’m a big fan of the tidyverse!\nHope you guys find the package useful! Fell free to send any question to the comment section of the post or my email (marceloperlin@gmail.com).\n ","date":1539388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539388800,"objectID":"13dfc50bfc6a9ec32f060621a530fac1","permalink":"http://www.msperlin.com/blog/post/2018-10-13-newpackage-pkgsfromfiles/","publishdate":"2018-10-13T00:00:00Z","relpermalink":"/blog/post/2018-10-13-newpackage-pkgsfromfiles/","section":"post","summary":"Its been a while since I develop a CRAN package and this weekend I decided to work on a idea I had some time ago. The result is package PkgsFromFiles.\nWhen working with different computers at home or work, one of the problems I have is installing missing packages across different computers. As an example, a script that works in my work computer may not work in my home computer. This is specially annoying when I have a fresh install of the operating system or R. In this case, I must manually install all packages, case by case.","tags":["R","PkgsFromFiles"],"title":"New package in CRAN: PkgsFromFiles","type":"post"},{"authors":null,"categories":["R","GetLattesData"],"content":"  Last year I released GetLattesData. This package is very handy for anyone that researches bibliometric data of Brazilian scholars. You could easily import the whole academic history of any researcher registered at the platform. More details about Lattes and GetLattesData in the this post.\nHowever, a couple months ago CNPQ introduced a captcha in the webpage. This made it impossible to download the xml files directly, breaking my code. It seems that those changes are now permanent. The update to GetLattesData will address this issue by asking the user to download the files manually and input its location to function gld_get_lattes_data_from_zip. Unfortunately, one can no longer download the files automatically.\nNext I provide an example of usage from the vignette:\nlibrary(GetLattesData) # get files from pkg (you can download from other researchers in lattes website) f.in \u0026lt;- c(system.file(\u0026#39;extdata/3262699324398819.zip\u0026#39;, package = \u0026#39;GetLattesData\u0026#39;), system.file(\u0026#39;extdata/8373564643000623.zip\u0026#39;, package = \u0026#39;GetLattesData\u0026#39;)) # set qualis field.qualis = \u0026#39;ADMINISTRAÇÃO PÚBLICA E DE EMPRESAS, CIÊNCIAS CONTÁBEIS E TURISMO\u0026#39; # get data l.out \u0026lt;- gld_get_lattes_data_from_zip(f.in, field.qualis = field.qualis ) ## ## Reading 3262699324398819.zip - Marcelo Scherer Perlin ## Found 21 published papers ## Found 2 accepted paper(s) ## Found 10 supervisions ## Found 2 published books ## Found 0 book chapters ## Found 17 conference papers ## Reading 8373564643000623.zip - Denis Borenstein ## Found 75 published papers ## Found 2 accepted paper(s) ## Found 97 supervisions ## Found 1 published books ## Found 6 book chapters ## Found 89 conference papers The output my.l is a list with the following dataframes:\nnames(l.out) ## [1] \u0026quot;tpesq\u0026quot; \u0026quot;tpublic.published\u0026quot; \u0026quot;tpublic.accepted\u0026quot; ## [4] \u0026quot;tsupervisions\u0026quot; \u0026quot;tbooks\u0026quot; \u0026quot;tconferences\u0026quot; The first is a dataframe with information about researchers:\ntpesq \u0026lt;- l.out$tpesq str(tpesq) ## tibble [2 × 16] (S3: tbl_df/tbl/data.frame) ## $ name : chr [1:2] \u0026quot;Marcelo Scherer Perlin\u0026quot; \u0026quot;Denis Borenstein\u0026quot; ## $ last.update : Date[1:2], format: \u0026quot;2018-09-24\u0026quot; \u0026quot;2018-08-24\u0026quot; ## $ bsc.institution: chr [1:2] \u0026quot;Universidade Federal de Santa Maria\u0026quot; \u0026quot;Universidade Federal do Rio de Janeiro\u0026quot; ## $ bsc.start.year : chr [1:2] \u0026quot;2001\u0026quot; \u0026quot;1981\u0026quot; ## $ bsc.end.year : chr [1:2] \u0026quot;2005\u0026quot; \u0026quot;1986\u0026quot; ## $ bsc.course : chr [1:2] \u0026quot;Administração de empresas\u0026quot; \u0026quot;Engenharia Naval\u0026quot; ## $ msc.institution: chr [1:2] \u0026quot;Universidade Federal do Rio Grande do Sul\u0026quot; \u0026quot;Universidade Federal do Rio Grande do Sul\u0026quot; ## $ msc.start.year : chr [1:2] \u0026quot;2005\u0026quot; \u0026quot;1989\u0026quot; ## $ msc.end.year : chr [1:2] \u0026quot;2007\u0026quot; \u0026quot;1991\u0026quot; ## $ phd.institution: chr [1:2] \u0026quot;University of Reading\u0026quot; \u0026quot;University of Strathclyde\u0026quot; ## $ phd.start.year : num [1:2] 2007 1991 ## $ phd.end.year : num [1:2] 2010 1995 ## $ country.origin : chr [1:2] \u0026quot;Brasil\u0026quot; \u0026quot;Brasil\u0026quot; ## $ major.field : chr [1:2] \u0026quot;CIENCIAS_SOCIAIS_APLICADAS\u0026quot; \u0026quot;ENGENHARIAS\u0026quot; ## $ minor.field : chr [1:2] \u0026quot;Administração\u0026quot; \u0026quot;Engenharia de Produção\u0026quot; ## $ id.file : chr [1:2] \u0026quot;3262699324398819.zip\u0026quot; \u0026quot;8373564643000623.zip\u0026quot; The second dataframe contains information about all published publications, including Qualis and SJR:\ndplyr::glimpse(l.out$tpublic.published) ## Rows: 96 ## Columns: 13 ## $ id.file \u0026lt;chr\u0026gt; \u0026quot;3262699324398819.zip\u0026quot;, \u0026quot;3262699324398819.zip\u0026quot;, \u0026quot;3… ## $ name \u0026lt;chr\u0026gt; \u0026quot;Marcelo Scherer Perlin\u0026quot;, \u0026quot;Marcelo Scherer Perlin\u0026quot;… ## $ article.title \u0026lt;chr\u0026gt; \u0026quot;Teoria do Caos aplicada aos Contratos de Café no … ## $ year \u0026lt;dbl\u0026gt; 2006, 2009, 2007, 2011, 2013, 2013, 2013, 2013, 20… ## $ language \u0026lt;chr\u0026gt; \u0026quot;Português\u0026quot;, \u0026quot;Inglês\u0026quot;, \u0026quot;Inglês\u0026quot;, \u0026quot;Inglês\u0026quot;, \u0026quot;Portug… ## $ journal.title \u0026lt;chr\u0026gt; \u0026quot;READ - Revista Eletrônica da Administração (UFRGS… ## $ contry.publication \u0026lt;chr\u0026gt; \u0026quot;Brasil\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, … ## $ ISSN \u0026lt;chr\u0026gt; \u0026quot;-\u0026quot;, \u0026quot;1753-9641\u0026quot;, \u0026quot;1413-2311\u0026quot;, \u0026quot;1749-9135\u0026quot;, \u0026quot;1679-… ## $ order.aut \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 3, 1, 3,… ## $ n.authors \u0026lt;dbl\u0026gt; 2, 1, 2, 2, 1, 3, 3, 3, 2, 2, 3, 2, 4, 5, 3, 2, 5,… ## $ qualis \u0026lt;chr\u0026gt; NA, NA, \u0026quot;B1\u0026quot;, NA, \u0026quot;B1\u0026quot;, \u0026quot;A2\u0026quot;, \u0026quot;B1\u0026quot;, \u0026quot;A1\u0026quot;, \u0026quot;B1\u0026quot;, \u0026quot;B… ## $ SJR \u0026lt;dbl\u0026gt; NA, 0.213, NA, NA, NA, 0.886, NA, 0.429, NA, NA, N… ## $ H.SJR \u0026lt;int\u0026gt; NA, 6, NA, NA, NA, 17, NA, 38, NA, NA, NA, NA, 45,… Other dataframes in l.out included information about accepted papers, supervisions, books and conferences.\nAn application of GetLattesData GetLattesData makes it easy to create academic reports for a large number of researchers. See next, where I plot the number of publications for each researcher, conditioning on Qualis ranking.\ntpublic.published \u0026lt;- l.out$tpublic.published library(ggplot2) p \u0026lt;- ggplot(tpublic.published, aes(x = qualis)) + geom_bar(position = \u0026#39;identity\u0026#39;) + facet_wrap(~name) + labs(x = paste0(\u0026#39;Qualis: \u0026#39;, field.qualis)) print(p) We can also use dplyr to do some simple assessment of academic productivity:\nlibrary(dplyr) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union my.tab \u0026lt;- tpublic.published %\u0026gt;% group_by(name) %\u0026gt;% summarise(n.papers = n(), max.SJR = max(SJR, na.rm = T), mean.SJR = mean(SJR, na.rm = T), n.A1.qualis = sum(qualis == \u0026#39;A1\u0026#39;, na.rm = T), n.A2.qualis = sum(qualis == \u0026#39;A2\u0026#39;, na.rm = T), median.authorship = median(as.numeric(order.aut), na.rm = T )) knitr::kable(my.tab)   name n.papers max.SJR mean.SJR n.A1.qualis n.A2.qualis median.authorship    Denis Borenstein 75 3.674 1.2808113 27 16 2  Marcelo Scherer Perlin 21 2.029 0.7204444 3 4 1     ","date":1539216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539216000,"objectID":"8e46c553e7daa43b88471c97d519d3ae","permalink":"http://www.msperlin.com/blog/post/2018-10-11-update-getlattesdata/","publishdate":"2018-10-11T00:00:00Z","relpermalink":"/blog/post/2018-10-11-update-getlattesdata/","section":"post","summary":"Last year I released GetLattesData. This package is very handy for anyone that researches bibliometric data of Brazilian scholars. You could easily import the whole academic history of any researcher registered at the platform. More details about Lattes and GetLattesData in the this post.\nHowever, a couple months ago CNPQ introduced a captcha in the webpage. This made it impossible to download the xml files directly, breaking my code. It seems that those changes are now permanent. The update to GetLattesData will address this issue by asking the user to download the files manually and input its location to function gld_get_lattes_data_from_zip.","tags":["R","GetLattesData"],"title":"Update to GetLattesData","type":"post"},{"authors":null,"categories":["R","BatchGetSymbols","Finance"],"content":"  One of the main requests I get for package BatchGetSymbols is to add the choice of frequency of the financial dataset. Today I finally got some time to work on it. I just posted a new version of BatchGetSymbols in CRAN. The major change is that users can now set the time frequency of the financial data: dailly, weekly, monthly or yearly. Let’s check it out:\nlibrary(BatchGetSymbols) ## Loading required package: rvest ## Loading required package: xml2 ## Loading required package: dplyr ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union ##  library(purrr) ## ## Attaching package: \u0026#39;purrr\u0026#39; ## The following object is masked from \u0026#39;package:rvest\u0026#39;: ## ## pluck library(ggplot2) my.fct \u0026lt;- function(my.freq) { df \u0026lt;- BatchGetSymbols(tickers = c(\u0026#39;GE\u0026#39;), first.date = \u0026#39;2010-01-01\u0026#39;, last.date = Sys.Date(), do.cache = FALSE, freq.data = my.freq)$df.tickers df$freq \u0026lt;- my.freq return(df) } my.possible.freq \u0026lt;- c(\u0026#39;daily\u0026#39;, \u0026#39;weekly\u0026#39;, \u0026#39;monthly\u0026#39;, \u0026#39;yearly\u0026#39;) df.allfreq \u0026lt;- bind_rows(map(.x = my.possible.freq, .f = my.fct)) ## ## Running BatchGetSymbols for: ## ## tickers =GE ## Downloading data for benchmark ticker ## ^GSPC | yahoo (1|1) ## GE | yahoo (1|1) - Got 100% of valid prices | Good job! ## Running BatchGetSymbols for: ## tickers =GE ## Downloading data for benchmark ticker ## ^GSPC | yahoo (1|1) ## GE | yahoo (1|1) - Got 100% of valid prices | You got it! ## Running BatchGetSymbols for: ## tickers =GE ## Downloading data for benchmark ticker ## ^GSPC | yahoo (1|1) ## GE | yahoo (1|1) - Got 100% of valid prices | Feels good! ## Running BatchGetSymbols for: ## tickers =GE ## Downloading data for benchmark ticker ## ^GSPC | yahoo (1|1) ## GE | yahoo (1|1) - Got 100% of valid prices | You got it! p \u0026lt;- ggplot(df.allfreq, aes(x=ref.date, y = price.adjusted)) + geom_point() + geom_line() + facet_grid(freq ~ ticker) print(p) ","date":1539129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539129600,"objectID":"8c33f137a70cd8bd6d896976271cf2ec","permalink":"http://www.msperlin.com/blog/post/2018-10-10-batchgetsymbols-newversion/","publishdate":"2018-10-10T00:00:00Z","relpermalink":"/blog/post/2018-10-10-batchgetsymbols-newversion/","section":"post","summary":"One of the main requests I get for package BatchGetSymbols is to add the choice of frequency of the financial dataset. Today I finally got some time to work on it. I just posted a new version of BatchGetSymbols in CRAN. The major change is that users can now set the time frequency of the financial data: dailly, weekly, monthly or yearly. Let’s check it out:\nlibrary(BatchGetSymbols) ## Loading required package: rvest ## Loading required package: xml2 ## Loading required package: dplyr ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union ##  library(purrr) ## ## Attaching package: \u0026#39;purrr\u0026#39; ## The following object is masked from \u0026#39;package:rvest\u0026#39;: ## ## pluck library(ggplot2) my.","tags":["R","BatchGetSymbols","Finance"],"title":"BatchGetSymbols 2.2","type":"post"},{"authors":["Marcelo S. Perlin","Takeyoshi Imasato","Denis Boreinstein"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"3e5d6970f7dcbf462a22c3a88aaae0f7","permalink":"http://www.msperlin.com/blog/publication/2018_scientometrics-predatory/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/blog/publication/2018_scientometrics-predatory/","section":"publication","summary":"Using a database of potential, possible, or probable predatory scholarly open-access journals, the objective of this research is to study the penetration of predatory publications in the Brazilian academic system and the profile of authors in a cross-section empirical study. Based on a massive amount of publications from Brazilian researchers of all disciplines during the 2000 to 2015 period, we were able to analyze the extent of predatory publications using an econometric modeling. Descriptive statistics indicate that predatory publications represent a small overall proportion, but grew exponentially in the last 5 years. Departing from prior studies, our analysis shows that experienced researchers with a high number of non-indexed publications and PhD obtained locally are more likely to publish in predatory journals. Further analysis shows that once a journal regarded as predatory is listed in the local ranking system, the Qualis, it starts to receive more publications than non-predatory ones.","tags":[],"title":"Is predatory publishing a real threat? Evidence from a large database study","type":"publication"},{"authors":["Sergio da Silva","Marcelo S. Perlin","Raul Matsushita","Takeyoshi Imasato","Denis Boreinstein"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"75532decbbd97e93d56209e96c2c7911","permalink":"http://www.msperlin.com/blog/publication/2018_jis-lotka/","publishdate":"2018-07-01T00:00:00Z","relpermalink":"/blog/publication/2018_jis-lotka/","section":"publication","summary":"Lotka’s law is a power law for the frequency of scholarly publications. We show that Lotka’s law cannot be dismissed after considering a massive sample of the number of publications of Brazilian researchers in journals listed on the SCImago Journal Rank and the Journal Citation Reports. For the SCImago Journal Rank, we found a power law with the Pareto exponent of 0.4 beyond the threshold of 50 papers. This means computing the ‘average number of publications’ of either a researcher or a discipline is of no practical significance.","tags":[],"title":"Lotka’s law for the Brazilian scientific output published in journals","type":"publication"},{"authors":null,"categories":["R","SSD","benchmarking"],"content":"  I recently bought a new computer for home and it came with two drives, one HDD and other SSD. The later is used for the OS and the former stores all of my personal files. From all computers I had, both home and work, this is definitely the fastest. While some of the merits are due to the newer CPUS and RAM, the SSD drive can make all the difference in file operations.\nMy research usually deals with large files from financial markets. Being efficient in reading those files is key to my productivity. Given that, I was very curious in understanding how much I would benefit in speed when reading/writing files in my SSD drive instead of the HDD. For that, I wrote a simple function that will time a particular operation. The function will take as input the number of rows in the data (1..Inf), the type of function used to save the file (rds, csv, fst) and the type of drive (HDD or SSD). See next.\nbench.fct \u0026lt;- function(N = 2500000, type.file = \u0026#39;rds\u0026#39;, type.hd = \u0026#39;HDD\u0026#39;) { # Function for timing read and write operations # # INPUT: N - Number of rows in dataframe to be read and write # type.file - format of output file (rds, csv, fst) # type.hd - where to save (hdd or ssd) # # OUTPUT: A dataframe with results require(tidyverse) require(fst) my.df \u0026lt;- data_frame(x = runif(N), char.vec = sample(letters, size = N, replace = TRUE)) path.file \u0026lt;- switch(type.hd, \u0026#39;SSD\u0026#39; = \u0026#39;~\u0026#39;, \u0026#39;HDD\u0026#39; = \u0026#39;/mnt/HDD/\u0026#39;) my.file \u0026lt;- file.path(path.file, switch (type.file, \u0026#39;rds-base\u0026#39; = \u0026#39;temp_rds.rds\u0026#39;, \u0026#39;rds-readr\u0026#39; = \u0026#39;temp_rds.rds\u0026#39;, \u0026#39;fst\u0026#39; = \u0026#39;temp_fst.fst\u0026#39;, \u0026#39;csv-readr\u0026#39; = \u0026#39;temp_csv.csv\u0026#39;, \u0026#39;csv-base\u0026#39; = \u0026#39;temp_csv.csv\u0026#39;)) if (type.file == \u0026#39;rds-base\u0026#39;) { time.write \u0026lt;- system.time(saveRDS(my.df, my.file, compress = FALSE)) time.read \u0026lt;- system.time(readRDS(my.file)) } else if (type.file == \u0026#39;rds-readr\u0026#39;) { time.write \u0026lt;- system.time(write_rds(x = my.df, path = my.file, compress = \u0026#39;none\u0026#39;)) time.read \u0026lt;- system.time(read_rds(path = my.file )) } else if (type.file == \u0026#39;fst\u0026#39;) { time.write \u0026lt;- system.time(write.fst(x = my.df, path = my.file)) time.read \u0026lt;- system.time(read_fst(my.file)) } else if (type.file == \u0026#39;csv-readr\u0026#39;) { time.write \u0026lt;- system.time(write_csv(x = my.df, path = my.file)) time.read \u0026lt;- system.time(read_csv(file = my.file, col_types = cols(x = col_double(), char.vec = col_character()))) } else if (type.file == \u0026#39;csv-base\u0026#39;) { time.write \u0026lt;- system.time(write.csv(x = my.df, file = my.file)) time.read \u0026lt;- system.time(read.csv(file = my.file)) } # clean up file.remove(my.file) # save output df.out \u0026lt;- data_frame(type.file = type.file, type.hd = type.hd, N = N, type.time = c(\u0026#39;write\u0026#39;, \u0026#39;read\u0026#39;), times = c(time.write[3], time.read[3])) return(df.out) } Now that we have my function, its time to use it for all combinations between number of rows, the formats of the file and type of drive:\nlibrary(purrr) df.grid \u0026lt;- expand.grid(N = seq(1, 500000, by = 50000), type.file = c(\u0026#39;rds-readr\u0026#39;, \u0026#39;rds-base\u0026#39;, \u0026#39;fst\u0026#39;, \u0026#39;csv-readr\u0026#39;, \u0026#39;csv-base\u0026#39;), type.hd = c(\u0026#39;HDD\u0026#39;, \u0026#39;SSD\u0026#39;), stringsAsFactors = F) l.out \u0026lt;- pmap(list(N = df.grid$N, type.file = df.grid$type.file, type.hd = df.grid$type.hd), .f = bench.fct) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. df.res \u0026lt;- do.call(what = bind_rows, args = l.out) Lets check the result in a nice plot:\nlibrary(ggplot2) p \u0026lt;- ggplot(df.res, aes(x = N, y = times, linetype = type.hd)) + geom_line() + facet_grid(type.file ~ type.time) print(p) As you can see, the csv-base format is messing with the y axis. Let’s remove it for better visualization:\nlibrary(ggplot2) p \u0026lt;- ggplot(filter(df.res, !(type.file %in% c(\u0026#39;csv-base\u0026#39;))), aes(x = N, y = times, linetype = type.hd)) + geom_line() + facet_grid(type.file ~ type.time) print(p) When it comes to the file format, we learn:\n By far, the fst format is the best. It takes less time to read and write than the others. However, it’s probably unfair to compare it to csv and rds as it uses the 16 cores of my computer.\n readr is a great package for writing and reading csv files. You can see a large difference of time from using the base functions. This is likely due to the use of low level functions to write and read the text files.\n When using the rds format, the base function do not differ much from the readr functions.\n  As for the effect of using SSD, its clear that it DOES NOT effect the time of reading and writing. The differences between using HDD and SSD looks like noise. Seeking to provide a more robust analysis, let’s formally test this hypothesis using a simple t-test for the means:\ntab \u0026lt;- df.res %\u0026gt;% group_by(type.file, type.time) %\u0026gt;% summarise(mean.HDD = mean(times[type.hd == \u0026#39;HDD\u0026#39;]), mean.SSD = mean(times[type.hd == \u0026#39;SSD\u0026#39;]), p.value = t.test(times[type.hd == \u0026#39;SSD\u0026#39;], times[type.hd == \u0026#39;HDD\u0026#39;])$p.value) print(tab) ## # A tibble: 10 x 5 ## # Groups: type.file [5] ## type.file type.time mean.HDD mean.SSD p.value ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 csv-base read 0.381 0.307 0.562 ## 2 csv-base write 0.457 0.453 0.981 ## 3 csv-readr read 0.148 0.144 0.922 ## 4 csv-readr write 0.0716 0.0732 0.942 ## 5 fst read 0.0108 0.00630 0.343 ## 6 fst write 0.0083 0.00800 0.890 ## 7 rds-base read 0.0362 0.0373 0.921 ## 8 rds-base write 0.0266 0.0278 0.882 ## 9 rds-readr read 0.0375 0.0367 0.943 ## 10 rds-readr write 0.0278 0.0279 0.991 As we can see, the null hypothesis of equal means easily fails to be rejected for almost all types of files and operations at 10%. The exception was for the fst format in a reading operation. In other words, statistically, it does not make any difference in time from using SSD or HDD to read or write files in different formats.\nI am very surprised by this result. Independently of the type of format, I expected a large difference as SSD drives are much faster within an OS. Am I missing something? Is this due to the OS being in the SSD? What you guys think?\n","date":1530230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530230400,"objectID":"020f99e5daaf3fb5c46e35b21f19a64d","permalink":"http://www.msperlin.com/blog/post/2018-06-29-benchmarkingssd/","publishdate":"2018-06-29T00:00:00Z","relpermalink":"/blog/post/2018-06-29-benchmarkingssd/","section":"post","summary":"I recently bought a new computer for home and it came with two drives, one HDD and other SSD. The later is used for the OS and the former stores all of my personal files. From all computers I had, both home and work, this is definitely the fastest. While some of the merits are due to the newer CPUS and RAM, the SSD drive can make all the difference in file operations.\nMy research usually deals with large files from financial markets. Being efficient in reading those files is key to my productivity.","tags":["R","SSD","benchmarking"],"title":"Benchmarking a SSD drive in reading and writing files with R","type":"post"},{"authors":null,"categories":["R","book"],"content":"   IMPORTANT: The third edition of the book was released in 2021 – more details in this link  It is with great pleasure that I announce the second edition of the portuguese version of my book, Processing and Analyzing Financial Data with R. This edition updates the material significantly. The portuguese version is now not only in par with the international version of the book, but much more!\nHere are the main changes:\n The structure of chapters changed towards the stages of a research, from obtaining the raw data, cleaning it, manipulating it and, finally, reporting tables and figures.\n Many new additions of packages for obtaining data, including my own and from other authors.\n Added new chapter for reporting results, exporting tables and also including a whole section about using Rmarkdown.\n Alignment with the tidyverse. I have no doubt that the packages from the tidyverse are here to stay. While the native function are presented, there is an emphasis in using the tidyverse, specially in reading local data, manipulating dataframes and functional programming.\n Exercises are available at the end of each chapter, including hard questions that will challenge your programming ability.\n  You can find the new edition of the book in Amazon. See this section for more details. As usual, an online (and free) version of the book is available at http://www.msperlin.com/padfeR/.\nIt was a lot of work (and fun) to write the new edition. I’m very happy with the result. I hope you enjoy it!\nBest,\n","date":1528761600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1528761600,"objectID":"12f8ed464b3c1d0a940225e1242ecaea","permalink":"http://www.msperlin.com/blog/post/2018-06-12-padfr-ed2/","publishdate":"2018-06-12T00:00:00Z","relpermalink":"/blog/post/2018-06-12-padfr-ed2/","section":"post","summary":"IMPORTANT: The third edition of the book was released in 2021 – more details in this link  It is with great pleasure that I announce the second edition of the portuguese version of my book, Processing and Analyzing Financial Data with R. This edition updates the material significantly. The portuguese version is now not only in par with the international version of the book, but much more!\nHere are the main changes:\n The structure of chapters changed towards the stages of a research, from obtaining the raw data, cleaning it, manipulating it and, finally, reporting tables and figures.","tags":["R","book"],"title":"Second Edition of \"Processamento e Analise de Dados Financeiros e Econômicos com o R\"","type":"post"},{"authors":null,"categories":["R","finance","investing","BatchGetSymbols"],"content":"  I often get asked about how to invest in the stock market. Not surprisingly, this has been a common topic in my classes. Brazil is experiencing a big change in its financial scenario. Historically, fixed income instruments paid a large premium over the stock market and that is no longer the case. Interest rates are low, without the pressure from inflation. This means a more sustainable scenario for low-interest rates in the future. Without the premium in the fixed income market, people turn to the stock market.\nWe can separate investors according to their horizon. Traders try to profit in the short term, usually within a day, and long-term investors buy a stock without the intent to sell it in the near future. This type of investment strategy is called BH (buy and hold). At the extreme, you buy a stock and hold it forever. The most famous spokesperson of BH is Warren Buffet, among many others.\nInvesting in the long run works for me because it doesn’t require much of my time. You just need to keep up with the quarterly and yearly financial reports of companies. You can easily do it as a side activity, parallel to your main job. You don’t need a lot of brain power to do it either, but it does require knowledge of accounting practices to understand all printed material released by companies.\nI read many books before starting to invest and one of the most interesting tables I’ve found portrays the relationship between investment horizon and profitability. The idea is that the more time you hold a stock or index, higher the chance of a profit. The table, originally from Taleb’s Fooled by Randomness, is as follows.\nMy problem with the table is that it seems pretty off. My experience tells me that a 67% chance of positive return every month seems exaggerated. If that was the case, making money in the stock market would be easy. Digging deeper, I found out that the data behind the table is simulated and, therefore, doesn’t really give good an estimate about the improvement in the probability of profits as a function of the investment horizon.\nAs you probably suspect, I decided to tackle the problem using real data and R. I wrote a simple function that will grab data, simulate investments of different horizons many times and plot the results. Let’s try it for the SP500 index:\nsource(\u0026#39;fct_invest_horizon.R\u0026#39;) my.ticker \u0026lt;- \u0026#39;^GSPC\u0026#39; # ticker from yahoo finance max.horizon = 255*50 # 50 years first.date \u0026lt;- \u0026#39;1950-01-01\u0026#39; last.date \u0026lt;- Sys.Date() n.points \u0026lt;- 50 # number of points in figure rf.year \u0026lt;- 0 # risk free return (or inflation) l.out \u0026lt;- get.figs.invest.horizon(ticker.in = my.ticker, first.date = first.date, last.date = last.date, max.horizon = max.horizon, n.points = n.points, rf.year = rf.year) ## Warning: `data_frame()` is deprecated as of tibble 1.1.0. ## Please use `tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_warnings()` to see where this warning was generated. print(l.out$p1) print(l.out$p2) As the investment horizon increases, the chances of a positive return increases. This result suggests that, if you invest for more than 13 years, it is very unlikely that you’ll see a negative return. When looking at the distribution of total returns by the horizon, we find that it increases significantly with time. Someone that invested for 50 years is likely to receive a 2500% return on the investment.\nWith input input rf.year we can also set a desired rate of return. Let’s try it with 5% return per year, with is pretty standard for financial markets.\nmy.ticker \u0026lt;- \u0026#39;^GSPC\u0026#39; # ticker from yahoo finance max.horizon = 255*50 # 50 years first.date \u0026lt;- \u0026#39;1950-01-01\u0026#39; last.date \u0026lt;- Sys.Date() n.points \u0026lt;- 50 # number of points in figure rf.year \u0026lt;- 0.05 # risk free return (or inflation) - yearly l.out \u0026lt;- get.figs.invest.horizon(ticker.in = my.ticker, first.date = first.date, last.date = last.date, max.horizon = max.horizon, n.points = n.points, rf.year = rf.year) print(l.out$p1) As expected, the curve of probabilities has a lower slope, meaning that you need more time investing in the SP500 index to guarantee a return of more than 5% a year.\nNow, let’s try the same setup for Berkshire stock (BRK-A). This is Buffet’s company and looking at its share price we can have a good understanding of how successful Buffet has been as a BH (buy and hold) investor.\nmy.ticker \u0026lt;- \u0026#39;BRK-A\u0026#39; # ticker from yahoo finance max.horizon = 255*25 # 50 years first.date \u0026lt;- \u0026#39;1980-01-01\u0026#39; last.date \u0026lt;- Sys.Date() n.points \u0026lt;- 50 # number of points in figure rf.year \u0026lt;- 0.05 # risk free return (or inflation) - yearly l.out \u0026lt;- get.figs.invest.horizon(ticker.in = my.ticker, first.date = first.date, last.date = last.date, max.horizon = max.horizon, n.points = n.points, rf.year = rf.year) print(l.out$p1) print(l.out$p2) Well, needless to say that, historically, Buffet has done very well in his investments! If you bought the stock and kept it for more 1 year, there is a 70% chance that you got a profit.\nI hope this post convinced you to start investing. The results are clear, its better to start as early as possible.\n","date":1526083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526083200,"objectID":"d342170016a8c2102bdf308c4b99319f","permalink":"http://www.msperlin.com/blog/post/2018-05-12-investing-long-run/","publishdate":"2018-05-12T00:00:00Z","relpermalink":"/blog/post/2018-05-12-investing-long-run/","section":"post","summary":"I often get asked about how to invest in the stock market. Not surprisingly, this has been a common topic in my classes. Brazil is experiencing a big change in its financial scenario. Historically, fixed income instruments paid a large premium over the stock market and that is no longer the case. Interest rates are low, without the pressure from inflation. This means a more sustainable scenario for low-interest rates in the future. Without the premium in the fixed income market, people turn to the stock market.\nWe can separate investors according to their horizon.","tags":["R","finance","investing","BatchGetSymbols"],"title":"Investing for the Long Run","type":"post"},{"authors":null,"categories":["R","Bealls list","predatory journals","scientometrics"],"content":"  My paper about the penetration of predatory journals in Brazil, Is predatory publishing a real threat? Evidence from a large database study, just got published in Scientometrics!. The working paper version is available in SSRN.\nThis is a nice example of a data-intensive scientific work cycle, from gathering data to reporting results. Everything was done in R, using web scrapping algorithms, parallel processing, tidyverse packages and more. This was a special project for me, given its implications in science making in Brazil. It took me nearly one year to produce and execute the whole code. It is also a nice case of the capabilities of package ggplot2 in producing publication-ready figures. As a side output, our database of predatory journals is available as a shiny app.\nMore details about the study itself is available in the paper. Our abstract is as follows:\n Using a database of potential, possible, or probable predatory scholarly open-access journals, the objective of this research is to study the penetration of predatory publications in the Brazilian academic system and the profile of authors in a cross-section empirical study. Based on a massive amount of publications from Brazilian researchers of all disciplines during the 2000–2015 period, we were able to analyze the extent of predatory publications using an econometric modeling. Descriptive statistics indicate that predatory publications represent a small overall proportion, but grew exponentially in the last 5 years. Departing from prior studies, our analysis shows that experienced researchers with a high number of non-indexed publications and PhD obtained locally are more likely to publish in predatory journals. Further analysis shows that once a journal regarded as predatory is listed in the local ranking system, the Qualis, it starts to receive more publications than non-predatory ones.\n ","date":1524355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524355200,"objectID":"1bbc4a75f3888ad29ebce9ad5a64b61c","permalink":"http://www.msperlin.com/blog/post/2018-04-22-predatory-scientometrics/","publishdate":"2018-04-22T00:00:00Z","relpermalink":"/blog/post/2018-04-22-predatory-scientometrics/","section":"post","summary":"My paper about the penetration of predatory journals in Brazil, Is predatory publishing a real threat? Evidence from a large database study, just got published in Scientometrics!. The working paper version is available in SSRN.\nThis is a nice example of a data-intensive scientific work cycle, from gathering data to reporting results. Everything was done in R, using web scrapping algorithms, parallel processing, tidyverse packages and more. This was a special project for me, given its implications in science making in Brazil. It took me nearly one year to produce and execute the whole code.","tags":["R","Bealls list","predatory journals"],"title":"Predatory Journals and R","type":"post"},{"authors":null,"categories":["R","research","writing"],"content":"  Back in 2007 I wrote a Matlab package for estimating regime switching models. I was just starting to learn to code and this project was my way of doing it. After publishing it in FEX (Matlab file exchange site), I got so many repeated questions on my email that eventually I realized it would be easier to write a manual for people to read. Some time and effort would be spend writing it, but less time replying to repeated questions on my email.\nThis manual about the code became, by far, my most cited paper in Google Scholar. It is not even published, just a permanent working paper. When attending conferences and seminars, I was always surprised to hear that, at that time, people knew me as the matlab regime switching guy.\nMoving forward a few years, I stopped using Matlab for R and I continue to invest a lot of time writing papers about packages and publishing them in standard scientific journals. I can testify for a greater contribution and impact for research papers about code. I strongly believe that this format will become more popular in the years to come. The new generation of researchers is far more aware of code than the previous. In that sense, nothing beats R and CRAN at the diversity and depth of packages.\nIn this subject, I frequently review papers in the same topic and I see common mistakes that researchers do when writing their papers. Here’s some tips for those that wish to pursue such a type of publication:\n A problem must be clearly stated: Every paper is a solution to a problem. This is also true for a paper about code. Identify it and make it painfully clear how the code solves it. Simply put, do your homework.\n The paper is NOT an extended manual: Don’t write a paper simply showing its functions. We have that from CRAN or other repository of code.\n Make sure you know what’s available: How people did it before? Is there a competing package? How does your code improves it?\n A bibliometric study is mandatory: Same as the previous point. Looking at the previous published research papers, can you find out how they handled the problem your code solves?\n Not everyone uses R, so make it easier for people to use you software: Make sure you keep a simple and accessible code. Explain what is R and why you should use it. Case in point, not everyone know what a tibble is.\n Think about your example of usage: You should always add a reproducible example of usage. This is what everyone will try! Make sure it is a simple example, not too deep in the literature. Something everyone can understand. Your code should also be accessible and reproducible.\n  It is a lot of work to publish a research paper about code. But, it is all worth it! The impact is much greater than a standard research paper. Your academic career will certainly move forward with it.\n","date":1521158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521158400,"objectID":"6cbb303d035ee3a53e9a8307ac6cb266","permalink":"http://www.msperlin.com/blog/post/2018-03-16-writing_papers_about_pkgs/","publishdate":"2018-03-16T00:00:00Z","relpermalink":"/blog/post/2018-03-16-writing_papers_about_pkgs/","section":"post","summary":"Back in 2007 I wrote a Matlab package for estimating regime switching models. I was just starting to learn to code and this project was my way of doing it. After publishing it in FEX (Matlab file exchange site), I got so many repeated questions on my email that eventually I realized it would be easier to write a manual for people to read. Some time and effort would be spend writing it, but less time replying to repeated questions on my email.\nThis manual about the code became, by far, my most cited paper in Google Scholar.","tags":["R","research"],"title":"Writing papers about packages","type":"post"},{"authors":null,"categories":["R","BatchGetSymbols","Yahoo finance"],"content":"  I just released a long due update to package BatchGetSymbols. The files are under review in CRAN and you should get the update soon. Meanwhile, you can install the new version from Github:\nif (!require(devtools)) install.packages(\u0026#39;devtools\u0026#39;) devtools::install_github(\u0026#39;msperlin/BatchGetSymbols\u0026#39;) The main innovations are:\n Clever cache system: By default, every new download of data will be saved in a local file located in a directory chosen by user. Every new request of data is compared to the available local information. If data is missing, the function only downloads the piece of data that is missing. This make the call to function BatchGetSymbols a lot faster! When updating an existing dataset of prices, the function only downloads the missing part of the data.\n Returns calculation: Function now returns a return vector in df.tickers. Returns are used a lot more than prices in research. No reason why they should be keep out of the output.\n Wide format: Added function for converting data to the wide format. In some situations, such as portfolio analysis, the wide format makes a lot of sense and is required for some methodologies.\n Ibovespa composition: Added function for downloading current Ibovespa composition directly from Bovespa website.\n  In the next chunks of code I show some of the innovations:\nlibrary(BatchGetSymbols) ## Loading required package: rvest ## Loading required package: xml2 ## Loading required package: dplyr ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union ##  # download Ibovespa stocks my.tickers \u0026lt;- GetSP500Stocks()$Tickers[1:5] # lets keep it light # set dates first.date \u0026lt;- \u0026#39;2017-01-01\u0026#39; last.date \u0026lt;- \u0026#39;2019-01-01\u0026#39; # set folder for cache system my.temp.cache.folder \u0026lt;- \u0026#39;BGS_CACHE\u0026#39; # get data and time it time.nocache \u0026lt;- system.time({ my.l \u0026lt;- BatchGetSymbols(tickers = my.tickers, first.date, last.date, cache.folder = my.temp.cache.folder, do.cache = FALSE) }) ## ## Running BatchGetSymbols for: ## ## tickers =MMM, ABT, ABBV, ABMD, ACN ## Downloading data for benchmark ticker ## ^GSPC | yahoo (1|1) ## MMM | yahoo (1|5) - Got 100% of valid prices | Well done! ## ABT | yahoo (2|5) - Got 100% of valid prices | Good job! ## ABBV | yahoo (3|5) - Got 100% of valid prices | Youre doing good! ## ABMD | yahoo (4|5) - Got 100% of valid prices | Youre doing good! ## ACN | yahoo (5|5) - Got 100% of valid prices | You got it! time.withcache \u0026lt;- system.time({ my.l \u0026lt;- BatchGetSymbols(tickers = my.tickers, first.date, last.date, cache.folder = my.temp.cache.folder, do.cache = TRUE) }) ## ## Running BatchGetSymbols for: ## tickers =MMM, ABT, ABBV, ABMD, ACN ## Downloading data for benchmark ticker ## ^GSPC | yahoo (1|1) | Not Cached | Saving cache ## MMM | yahoo (1|5) | Not Cached | Saving cache - Got 100% of valid prices | Looking good! ## ABT | yahoo (2|5) | Not Cached | Saving cache - Got 100% of valid prices | Good stuff! ## ABBV | yahoo (3|5) | Not Cached | Saving cache - Got 100% of valid prices | Got it! ## ABMD | yahoo (4|5) | Not Cached | Saving cache - Got 100% of valid prices | Looking good! ## ACN | yahoo (5|5) | Not Cached | Saving cache - Got 100% of valid prices | Good stuff! cat(\u0026#39;\\nTime with no cache:\u0026#39;, time.nocache[\u0026#39;elapsed\u0026#39;]) ## ## Time with no cache: 4.094 cat(\u0026#39;\\nTime with cache:\u0026#39;, time.withcache[\u0026#39;elapsed\u0026#39;]) ## ## Time with cache: 2.386 Now let’s check the default output with data in the long format:\ndplyr::glimpse(my.l) ## List of 2 ## $ df.control: tibble [5 × 6] (S3: tbl_df/tbl/data.frame) ## ..$ ticker : chr [1:5] \u0026quot;MMM\u0026quot; \u0026quot;ABT\u0026quot; \u0026quot;ABBV\u0026quot; \u0026quot;ABMD\u0026quot; ... ## ..$ src : chr [1:5] \u0026quot;yahoo\u0026quot; \u0026quot;yahoo\u0026quot; \u0026quot;yahoo\u0026quot; \u0026quot;yahoo\u0026quot; ... ## ..$ download.status : chr [1:5] \u0026quot;OK\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;OK\u0026quot; \u0026quot;OK\u0026quot; ... ## ..$ total.obs : int [1:5] 502 502 502 502 502 ## ..$ perc.benchmark.dates: num [1:5] 1 1 1 1 1 ## ..$ threshold.decision : chr [1:5] \u0026quot;KEEP\u0026quot; \u0026quot;KEEP\u0026quot; \u0026quot;KEEP\u0026quot; \u0026quot;KEEP\u0026quot; ... ## $ df.tickers:\u0026#39;data.frame\u0026#39;: 2510 obs. of 10 variables: ## ..$ price.open : num [1:2510] 179 178 178 177 178 ... ## ..$ price.high : num [1:2510] 180 179 179 179 178 ... ## ..$ price.low : num [1:2510] 177 178 177 176 177 ... ## ..$ price.close : num [1:2510] 178 178 178 178 177 ... ## ..$ volume : num [1:2510] 2509300 1542000 1447800 1625000 1622600 ... ## ..$ price.adjusted : num [1:2510] 162 163 162 163 162 ... ## ..$ ref.date : Date[1:2510], format: \u0026quot;2017-01-03\u0026quot; \u0026quot;2017-01-04\u0026quot; ... ## ..$ ticker : chr [1:2510] \u0026quot;MMM\u0026quot; \u0026quot;MMM\u0026quot; \u0026quot;MMM\u0026quot; \u0026quot;MMM\u0026quot; ... ## ..$ ret.adjusted.prices: num [1:2510] NA 0.00152 -0.00342 0.00293 -0.00539 ... ## ..$ ret.closing.prices : num [1:2510] NA 0.00152 -0.00342 0.00293 -0.00539 ... And change the format of the long dataframe to wide:\nl.wide \u0026lt;- reshape.wide(my.l$df.tickers)  Now we check the matrix of prices:\nprint(head(l.wide$price.adjusted)) ## ref.date ABBV ABMD ABT ACN MMM ## 1 2017-01-03 52.55166 112.36 36.55642 109.8161 162.4736 ## 2 2017-01-04 53.29267 115.74 36.84662 110.0802 162.7200 ## 3 2017-01-05 53.69685 114.81 37.16491 108.4300 162.1634 ## 4 2017-01-06 53.71369 115.42 38.17595 109.6653 162.6379 ## 5 2017-01-09 54.06735 117.11 38.13851 108.4394 161.7619 ## 6 2017-01-10 53.94946 112.24 38.65339 108.4960 161.1322 and matrix of returns:\nprint(head(l.wide$ret.adjusted.prices)) ## ref.date ABBV ABMD ABT ACN ## 1 2017-01-03 NA NA NA NA ## 2 2017-01-04 0.0141005055 0.030081853 0.0079383861 0.0024043005 ## 3 2017-01-05 0.0075841391 -0.008035252 0.0086381596 -0.0149906200 ## 4 2017-01-06 0.0003136497 0.005313126 0.0272041565 0.0113923084 ## 5 2017-01-09 0.0065841132 0.014642203 -0.0009806436 -0.0111779967 ## 6 2017-01-10 -0.0021804289 -0.041584860 0.0135001858 0.0005216922 ## MMM ## 1 NA ## 2 0.001516547 ## 3 -0.003420851 ## 4 0.002926172 ## 5 -0.005386333 ## 6 -0.003892474 ","date":1516579200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516579200,"objectID":"457d5f7492ac37875f284d9eb909d9fe","permalink":"http://www.msperlin.com/blog/post/2018-01-22-update-batchgetsymbols/","publishdate":"2018-01-22T00:00:00Z","relpermalink":"/blog/post/2018-01-22-update-batchgetsymbols/","section":"post","summary":"I just released a long due update to package BatchGetSymbols. The files are under review in CRAN and you should get the update soon. Meanwhile, you can install the new version from Github:\nif (!require(devtools)) install.packages(\u0026#39;devtools\u0026#39;) devtools::install_github(\u0026#39;msperlin/BatchGetSymbols\u0026#39;) The main innovations are:\n Clever cache system: By default, every new download of data will be saved in a local file located in a directory chosen by user. Every new request of data is compared to the available local information. If data is missing, the function only downloads the piece of data that is missing.","tags":["R","BatchGetSymbols","Yahoo finance"],"title":"Major update to BatchGetSymbols","type":"post"},{"authors":null,"categories":["R","blog","jekyll"],"content":"  My blog in 2017 As we come close to the end of 2017, its time to look back. This has been a great year for me in many ways. This blog started as a way to write short pieces about using R for finance and promote my book in an organic way. Today, I’m very happy with my decision. Discovering and trying new writing styles keeps my interest very much alive. Academic research is very strict on what you can write and publish. It is satisfying to see that I can promote my work and have an impact in different ways, not only through the publication of academic papers.\nMy blog is build using a Jekyll template, meaning the whole site, including individual posts, is built and controlled with editable text files and Github. All files related to posts follow the same structure, meaning I can easily gather the textual data and organize it in a nice tibble. Let’s first have a look in all post files:\npost.folder \u0026lt;- \u0026#39;~/GitRepo/msperlin.github.io/_posts/\u0026#39; my.f.posts \u0026lt;- list.files(post.folder, full.names = TRUE) my.f.posts ## character(0) I posted 0 posts during 2017. Notice how all dates are in the beginning of the file name. I can easily convert that to a Date object using as.Date. Let’s organize it all in a nice tibble.\nlibrary(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ── ## ✔ ggplot2 3.3.3 ✔ purrr 0.3.4 ## ✔ tibble 3.1.0 ✔ dplyr 1.0.4 ## ✔ tidyr 1.1.2 ✔ stringr 1.4.0 ## ✔ readr 1.4.0 ✔ forcats 0.5.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() df.posts \u0026lt;- tibble(ref.date = as.Date(basename(my.f.posts)), ref.month = format(ref.date, \u0026#39;%m\u0026#39;), content = sapply(my.f.posts, function(x) paste0(readLines(x), collapse = \u0026#39;\\n\u0026#39;) ), char.length = nchar(content)) %\u0026gt;% # includes output code in length calculation.. filter(ref.date \u0026gt; as.Date(\u0026#39;2017-01-01\u0026#39;) | ref.date \u0026lt; as.Date(\u0026#39;2018-01-01\u0026#39;) ) # not really necessary but keep it for future glimpse(df.posts) ## Rows: 0 ## Columns: 4 ## $ ref.date \u0026lt;date\u0026gt; ## $ ref.month \u0026lt;chr\u0026gt; ## $ content \u0026lt;named list\u0026gt; [] ## $ char.length \u0026lt;int\u0026gt; Fist, let’s look at the frequency of posts by month:\nprint( ggplot(df.posts, aes(x = ref.month)) + geom_histogram(stat=\u0026#39;count\u0026#39;))  ## Warning: Ignoring unknown parameters: binwidth, bins, pad It is not accidental that january was the month with the highest number of posts. This is when I had material reserved for the book. June and July (0!) were the worst months as I traveled a lot. In June I attended R and Finance in Chicago, SER in Rio de Janeiro and in July I was visiting Goethe University in Germany for the whole month. On average, I created 0 posts per month overall, which fells quite alright. I hope I can keep that pace for the upcoming years.\nAs for the length of posts, below we can see a nice pattern for its distribution conditional on the months of the year.\nprint(ggplot(df.posts, aes(x=ref.month, y = char.length)) + geom_boxplot()) I was not very productive from may to august, writing a few and short posts, when comparing to other months. This was probably due to my travels.\n Plans for 2018 Despite the usual effort in research and teaching, my plans for 2018 are:\n Work on the second edition of the portuguese book. It significantly lags the english version in content and this need to be fixed. I already have some ideas laid out for new chapters and new packages to cover. I’ll write more about this update as soon as I have it figured out.\n Start a portal for financial data in Brazil. I want to make it easy for people to visualize and download organized financial data, specially those without programming experience. It will include the usual datasets such as prices in equity/bond/derivative markets for various frequencies, historical yield curves, financial statements of companies, and so on. The idea is to offer the datasets in various file formats, facilitating its use in research.\n  That’s it. If you got this far, happy new year! Enjoy your family and the holidays!\n ","date":1514592000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514592000,"objectID":"f4e795e914a80f82a355611aafd24151","permalink":"http://www.msperlin.com/blog/post/2017-12-30-looking-back-2017/","publishdate":"2017-12-30T00:00:00Z","relpermalink":"/blog/post/2017-12-30-looking-back-2017/","section":"post","summary":"My blog in 2017 As we come close to the end of 2017, its time to look back. This has been a great year for me in many ways. This blog started as a way to write short pieces about using R for finance and promote my book in an organic way. Today, I’m very happy with my decision. Discovering and trying new writing styles keeps my interest very much alive. Academic research is very strict on what you can write and publish. It is satisfying to see that I can promote my work and have an impact in different ways, not only through the publication of academic papers.","tags":["R","blog","jekyll"],"title":"Looking back in 2017 and plans for 2018","type":"post"},{"authors":null,"categories":["R","shiny","webserver","digital ocean"],"content":"  In this post I’ll share my experience in setting up my own virtual server for hosting shiny applications in Digital Ocean. First, context. I’m working in a academic project where we build a package for accessing financial data and corporate events directly from B3, the Brazilian financial exchange. The objective is to set a reproducible standard and facilite data acquisition of a large, and very interesting, dataset. The result is GetDFPData. Since many researchers and students in Brazil are not knowledgeable in R, we needed to make it easier for people to use the software. A shiny app hosted in the internet is perfect for that. The app is available at http://www.msperlin.com/shiny/GetDFPData/.\nYou can host your own shiny app for free in www.shiny.io, but that comes with some usage limitations. While searching for alternatives, I’ve found this great post by Dean Attali that clearly explains the steps for setting up a web server in a virtual machine from Digital Ocean. Despite being a 2015 post, it works perfectly. The best thing is that a server only costs $5 per month, with the first two months for free.\nOnce the server is up and running, I can control it using ssh (terminal), send/retrieve files with github/dropbox or rsync, and run code with Rstudio server, which is basically a Rstudio session in a browser. Now I have my own corner in the internet, where I can host all my shiny apps with full control. I’m not only using the server for hosting web applications, but also running CRON jobs for periodically gather data for another project, which has to run a R script every day. No longer I have to worry or remember to turn on my computer every day. I’m sure I’ll find many more uses to it in the future.\nI’m very happy in choosing the longer, more difficult path in publishing a shiny app in the internet. I learned a lot along the way. At first it felt overwhelming to configure every aspect of the server. But, if you know a bit of Linux, setting up your own webserver is not that difficult. I recommend everyone to give it a try.\n","date":1513123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513123200,"objectID":"48bce746377192c1f0527a28b0e2e664","permalink":"http://www.msperlin.com/blog/post/2017-12-13-serving-shiny-apps-internet/","publishdate":"2017-12-13T00:00:00Z","relpermalink":"/blog/post/2017-12-13-serving-shiny-apps-internet/","section":"post","summary":"In this post I’ll share my experience in setting up my own virtual server for hosting shiny applications in Digital Ocean. First, context. I’m working in a academic project where we build a package for accessing financial data and corporate events directly from B3, the Brazilian financial exchange. The objective is to set a reproducible standard and facilite data acquisition of a large, and very interesting, dataset. The result is GetDFPData. Since many researchers and students in Brazil are not knowledgeable in R, we needed to make it easier for people to use the software.","tags":["R","shiny","webserver","digital ocean"],"title":"Serving shiny apps in the internet with your own server","type":"post"},{"authors":null,"categories":["R","GetDFPData","B3"],"content":"   Package GetDFPData is being substituted by GetDFPData2. See this blog post for details.  Financial statements of companies traded at B3 (formerly Bovespa), the Brazilian stock exchange, are available in its website. Accessing the data for a single company is straightforward. In the website one can find a simple interface for accessing this dataset. An example is given here. However, gathering and organizing the data for a large scale research, with many companies and many dates, is painful. Financial reports must be downloaded or copied individually and later aggregated. Changes in the accounting format thoughout time can make this process slow, unreliable and irreproducible.\nPackage GetDFPData provides a R interface to all annual financial statements available in the website and more. It not only downloads the data but also organizes it in a tabular format and allows the use of inflation indexes. Users can select companies and a time period to download all available data. Several information about current companies, such as sector and available quarters are also at reach. The main purpose of the package is to make it easy to access financial statements in large scale research, facilitating the reproducibility of corporate finance studies with B3 data.\nThe positive aspects of GetDFDData are:\n Easy and simple R and web interface Changes in accounting format are internally handled by the software Access to corporate events in the FRE system such as dividend payments, changes in stock holder composition, changes in governance listings, board composition and compensation, debt composition, and a lot more! The output data is automatically organized using tidy data principles (long format) A cache system is employed for fast data acquisition Completely free and open source!  Installation The package is available in CRAN (release version) and in Github (development version). You can install any of those with the following code:\n# Release version in CRAN install.packages(\u0026#39;GetDFPData\u0026#39;) # not in CRAN yet # Development version in Github devtools::install_github(\u0026#39;msperlin/GetDFPData\u0026#39;)  Shinny interface The web interface of GetDFPData is available at http://www.msperlin.com/shiny/GetDFPData/.\n How to use GetDFPData The starting point of GetDFPData is to find the official names of companies in B3. Function gdfpd.search.company serves this purpose. Given a string (text), it will search for a partial matches in companies names. As an example, let’s find the official name of Petrobras, one of the largest companies in Brazil:\nlibrary(GetDFPData) library(tibble) gdfpd.search.company(\u0026#39;petrobras\u0026#39;,cache.folder = tempdir()) Its official name in Bovespa records is PETRÓLEO BRASILEIRO S.A. - PETROBRAS. Data for quarterly and annual statements are available from 1998 to 2017. The situation of the company, active or canceled, is also given. This helps verifying the availability of data.\nThe content of all available financial statements can be accessed with function gdfpd.get.info.companies. It will read and parse a .csv file from my github repository. This will be periodically updated for new information. Let’s try it out:\ndf.info \u0026lt;- gdfpd.get.info.companies(type.data = \u0026#39;companies\u0026#39;, cache.folder = tempdir()) glimpse(df.info) This file includes several information that are gathered from Bovespa: names of companies, official numeric ids, listing segment, sectors, traded tickers and, most importantly, the available dates. The resulting dataframe can be used to filter and gather information for large scale research such as downloading financial data for a specific sector.\nDownloading financial information for ONE company All you need to download financial data with GetDFPData are the official names of companies, which can be found with gdfpd.search.company, the desired starting and ending dates and the type of financial information (individual or consolidated). Let’s try it for PETROBRAS:\nname.companies \u0026lt;- \u0026#39;PETRÓLEO BRASILEIRO S.A. - PETROBRAS\u0026#39; first.date \u0026lt;- \u0026#39;2015-01-01\u0026#39; last.date \u0026lt;- \u0026#39;2016-01-01\u0026#39; df.reports \u0026lt;- gdfpd.GetDFPData(name.companies = name.companies, first.date = first.date, last.date = last.date, cache.folder = tempdir()) The resulting object is a tibble, a data.frame type of object that allows for list columns. Let’s have a look in its content:\nglimpse(df.reports) Object df.reports only has one row since we only asked for data of one company. The number of rows increases with the number of companies, as we will soon learn with the next example. All financial statements for the different years are available within df.reports. For example, the assets statements for all desired years of PETROBRAS are:\ndf.income.long \u0026lt;- df.reports$fr.income[[1]] glimpse(df.income.long) The resulting dataframe is in the long format, ready for processing. In the long format, financial statements of different years are stacked. In the wide format, we have the year as columns of the table.\nIf you want the wide format, which is the most common way that financial reports are presented, you can use function gdfpd.convert.to.wide. See an example next:\ndf.income.wide \u0026lt;- gdfpd.convert.to.wide(df.income.long) knitr::kable(df.income.wide )  Downloading financial information for SEVERAL companies If you are doing serious research, it is likely that you need financial statements for more than one company. Package GetDFPData is specially designed for handling large scale download of data. Let’s build a case with two selected companies:\nmy.companies \u0026lt;- c(\u0026#39;PETRÓLEO BRASILEIRO S.A. - PETROBRAS\u0026#39;, \u0026#39;BANCO DO ESTADO DO RIO GRANDE DO SUL SA\u0026#39;) first.date \u0026lt;- \u0026#39;2016-01-01\u0026#39; last.date \u0026lt;- \u0026#39;2017-01-01\u0026#39; type.statements \u0026lt;- \u0026#39;individual\u0026#39; df.reports \u0026lt;- gdfpd.GetDFPData(name.companies = my.companies, first.date = first.date, last.date = last.date, cache.folder = tempdir()) And now we can check the resulting tibble:\nglimpse(df.reports) Every row of df.reports will provide information for one company. Metadata about the corresponding dataframes such as min/max dates is available in the first columns. Keeping a tabular structure facilitates the organization and future processing of all financial data. We can use tibble df.reports for creating other dataframes in the long format containing data for all companies. See next, where we create dataframes with the assets and liabilities of all companies:\ndf.assets \u0026lt;- do.call(what = rbind, args = df.reports$fr.assets) df.liabilities \u0026lt;- do.call(what = rbind, args = df.reports$fr.liabilities) df.assets.liabilities \u0026lt;- rbind(df.assets, df.liabilities) As an example, let’s use the resulting dataframe for calculating and analyzing a simple liquidity index of a company, the total of current (liquid) assets (Ativo circulante) divided by the total of current short term liabilities (Passivo Circulante), over time.\nlibrary(dplyr) my.tab \u0026lt;- df.assets.liabilities %\u0026gt;% group_by(name.company, ref.date) %\u0026gt;% summarise(Liq.Index = acc.value[acc.number == \u0026#39;1.01\u0026#39;]/ acc.value[acc.number == \u0026#39;2.01\u0026#39;]) my.tab Now we can visualize the information using ggplot2:\nlibrary(ggplot2) p \u0026lt;- ggplot(my.tab, aes(x = ref.date, y = Liq.Index, fill = name.company)) + geom_col(position = \u0026#39;dodge\u0026#39; ) print(p)  Exporting financial data The package includes function gdfpd.export.DFP.data for exporting the financial data to an Excel or zipped csv files. See next:\nmy.basename \u0026lt;- \u0026#39;MyExcelData\u0026#39; my.format \u0026lt;- \u0026#39;csv\u0026#39; # only supported so far gdfpd.export.DFP.data(df.reports = df.reports, base.file.name = my.basename, type.export = my.format) The resulting Excel file contains all data available in df.reports.\n  ","date":1512518400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512518400,"objectID":"9d32c81bbaba8c6ff1ae26b3c83cd332","permalink":"http://www.msperlin.com/blog/post/2017-12-06-package-getdfpdata/","publishdate":"2017-12-06T00:00:00Z","relpermalink":"/blog/post/2017-12-06-package-getdfpdata/","section":"post","summary":"Package GetDFPData is being substituted by GetDFPData2. See this blog post for details.  Financial statements of companies traded at B3 (formerly Bovespa), the Brazilian stock exchange, are available in its website. Accessing the data for a single company is straightforward. In the website one can find a simple interface for accessing this dataset. An example is given here. However, gathering and organizing the data for a large scale research, with many companies and many dates, is painful. Financial reports must be downloaded or copied individually and later aggregated.","tags":["R","GetDFPData","corporate events","financial reports"],"title":"Package GetDFPData","type":"post"},{"authors":null,"categories":["R","yield curve","GetTDData"],"content":" The latest version of GetTDData offers function get.yield.curve to download the current Brazilian yield curve directly from Anbima. The yield curve is a financial tool that, based on current prices of fixed income instruments, shows how the market perceives the future real, nominal and inflation returns. You can find more details regarding the use and definition of a yield curve in Investopedia.\nUnfortunately, function get.yield.curve only downloads the current yield curve from the website. Data for historical curves over five business days are not available in Anbima website.\nThe new version of GetTDData is available in github and CRAN:\n#from CRAN install.packages(\u0026#39;GetTDData\u0026#39;) # From github devtools::install_github(\u0026#39;msperlin/GetTDData\u0026#39;) The current Brazilian yield curve Downloading the yield curve is easy, all you need is to di us call function get.yield.curve without any argument:\nlibrary(GetTDData) df.yield \u0026lt;- get.yield.curve() str(df.yield) ## \u0026#39;data.frame\u0026#39;: 111 obs. of 5 variables: ## $ n.biz.days : num 126 252 378 504 630 ... ## $ type : chr \u0026quot;real_return\u0026quot; \u0026quot;real_return\u0026quot; \u0026quot;real_return\u0026quot; \u0026quot;real_return\u0026quot; ... ## $ value : num 3.89 2.05 1.98 2.28 2.6 ... ## $ ref.date : Date, format: \u0026quot;2020-08-31\u0026quot; \u0026quot;2021-01-04\u0026quot; ... ## $ current.date: Date, format: \u0026quot;2020-04-27\u0026quot; \u0026quot;2020-04-27\u0026quot; ... The result is a dataframe in the long format containing data for the yield curve of real, nominal and inflation returns. Let’s plot it!\nlibrary(ggplot2) p \u0026lt;- ggplot(df.yield, aes(x=ref.date, y = value) ) + geom_line(size=1) + geom_point() + facet_grid(~type, scales = \u0026#39;free\u0026#39;) + labs(title = paste0(\u0026#39;The current Brazilian Yield Curve \u0026#39;), subtitle = paste0(\u0026#39;Date: \u0026#39;, df.yield$current.date[1])) print(p)  The expected inflation in Brazil seems to be stable. Market expectation is for an inflation around 5% a year in 2024. This level is quite low when compared to our history. As for future nominal interest rate, market expects another drop in the interest rate level in 2019. This is in line with the latest report from COPOM, the Brazilian comittee of monetary policy. Real returns also seems to be stable and low, around 5%. Again, this is one of the lowest levels of real returns in our economy.\nI’m very optimistic (and very biased as I love my country!) regarding the future of the Brazilian economy. I hope we can keep these low levels of interest rate and inflation in order to foment comsumption, jobs and overall economic well-being.\n ","date":1505347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505347200,"objectID":"31eb59cdeff4466709f8d56035736a1f","permalink":"http://www.msperlin.com/blog/post/2017-09-14-brazilian-yield-curve/","publishdate":"2017-09-14T00:00:00Z","relpermalink":"/blog/post/2017-09-14-brazilian-yield-curve/","section":"post","summary":"The latest version of GetTDData offers function get.yield.curve to download the current Brazilian yield curve directly from Anbima. The yield curve is a financial tool that, based on current prices of fixed income instruments, shows how the market perceives the future real, nominal and inflation returns. You can find more details regarding the use and definition of a yield curve in Investopedia.\nUnfortunately, function get.yield.curve only downloads the current yield curve from the website. Data for historical curves over five business days are not available in Anbima website.\nThe new version of GetTDData is available in github and CRAN:","tags":["R","yield curve","GetTDData"],"title":"The Brazilian Yield Curve","type":"post"},{"authors":null,"categories":["Linux"],"content":"  Its been 8 months since I switched from Windows 10 to Linux Mint. In this post I’ll talk about my experience as a scholar and R user in the transition.\nMy work is, simply put, to communicate ideas. A typical day of work is a mixture of writing research papers, creating classroom material and admin tasks. Most of my work happens in front of a computer.\nWithout a doubt, my greatest asset are my computer files. I reuse them over and over. Every new research script is a new version of something I’ve done in the past. The same for class slides and exercises. At each iteration, I add new material and update information. Said that, one problem in using Windows software is the proprietary format of Office packages. Once my Office license expired late 2016, I quick realized that I was not able to work on my files the same way I did before. While there are alternatives to Office products and online platforms, they don’t work out of the box in all cases. As an example, if you have a ppt presentation with many figures, it is likely that you’ll have to resize them individually when opening it in Libreoffice. So, by using Office, my work was trapped in that format.\nAnother problem with Windows is the amount of resources it needs to run. I have an old laptop that is just unworkable in Windows 10 due to its low specs. My desktop computer at work was also becaming slower. By switching to a less resource demanding operational system, I am able to work with low end computers and use them for a few more years. Maintaining windows systems is also a hassle. Every other year I needed to format, reinstall all software, download all dropbox files. If you have three or more computer to maintain, it become a burden to carry.\nOn the other side, I have been extremely impressed by what open source communities can build. After embracing R from a background in Matlab, writing a book about R, I’m constantly overwhelmed by the high quality work that open source communities do. If you do not know, Linux Mint and others are all free operations systems, developed by a large group of people that don’t necessarily make a living out of it.\nAfter some research, I decided to go with Linux Mint due to its large ammount of users and my previous positive experience. Others distributions worth looking are Debian and Ubuntu. There are many more. You can find a lot of information about available Linux distributions and its popularity in Distro Watch.\nThe positive side  My R code is the same. The only change I had to make in the code is pathwise. While in Windows you use C:/Documents, in Linux is /home/msperlin/Documents.\n External R packages dependencies are easy to install. Some R package that depend on external software can be installed using bash command apt. For example, package readxl can be installed with terminal command sudo apt install r-cran-readxl. Any external dependency to readxl that is missing in the computer will also be installed. This is very handy when you have R packages with heavy dependencies.\n Linux Mint is blazing fast ! My boot up is very quick and I’m able to start working fast.\n It looks beautiful. I know this is personal opinion, but I fell the Cinnamon desktop looks better than Windows 10.\n The system is very stable. I never had any system problem such as the blue screen of Windows. I’m used to leave my linux based computer running code 5 to 10 hours straight without any problem.\n Everything can be done from the terminal, if you want to. One of the good things about Linux Mint is that everything can be changed from the terminal as the whole configuration of the system are stored in editable text files. As an example, I wrote a simple bash script that installs all software that I need for work. You can write a bash script to configure the computer anyway you want. If you buy a new computer, just run the bash script and it will be exactly how you want it.\n Software is always up to date. It is simple to keep all software running the latest version using apt tools and the terminal. All you need to do is to call sudo apt update and sudo apt upgrade in the terminal. If you have many computers, it is very easy to keep then running the same version of software. The closest you can find for Windows is Chocolatey, which, unfortunately, did not work well for me.\n   The negative side  Collaborating with Office users. Using LibreOffice I can, most of the time, open and edit Word/Excel files. But, if you have a more complicated file, there are going to be problems. My solution to this is to always be charmingly annoying and convince others to use Latex or LibreOffice. If they don’t want to, well, we will not collaborate. I’m aware of the luck I have in this option. Most people don’t get to chose who they work with.\n Relearn all software. I used Windows for so many years that I knew by heart how to use its software. I had to relearn many things. I’ve found a good website, AlternativeTo that can help a lot. So far, I can’t name a Windows software that is not replaceable in Linux. Next I describe what I use in Linux to do most of my work:\n  Writing research articles: Latex (textudio + texlive) Writing other documents: Writer (libreoffice), kate, Atom Spreadsheets: Calc (libreoffice) Slides: Impress (libreoffice), Latex beamer and RMarkdown Data Analysis: R+RStudio and Python+Spyder (ocasionally), Data storage: SQLITE, csv files, rds and fst Remote access: RealVnc Browser: Chrome\nSmall number of games. Occasionally, I used to play some computer games in the weekend. That, is no longer the case. While there has been an advance in the game front with Linux+Steam, most titles simply don’t come to Linux.\n Conclusion I’m very happy with my choice and advise everyone to make the switch. I’m more productive in Linux than I was in Windows. But, the transition is not easy. You’ll need to learn many new things. Be confident that the positive payoff vastly outweigh the costs.\nI hope this post gives some pointers for those considering making the change. You can find many tutorials in the internet about how to install and use Linux Mint.\nA couple of good ideas from Eric Jacobsen (comments section) before you start out:\n You can use VirtualBox to easily try out Linux Mint or other distributions before settling on one and investing the time to set it up.\n You can install XFCE desktop environment on the Cinnamon distribution, for lighter resource usage. (Cinnamon and XFCE are both desktop environments. Try on virtualbox first if desired.) The login page allows you to specify which of the installed desktop environment to use, so it is easy to switch between them.\n Note that Mint is derived from Ubuntu, which is derived from Debian, thus Mint uses the Debian package manager. This is pretty important because you may be using it a lot, and Mint benefits from packages created for Ubuntu and Debian.\n An alternative to installing TeX packages is using ShareLaTex or Overleaf to use TeX online.\n   ","date":1502496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502496000,"objectID":"e09fe28662f483f03ccac67737009216","permalink":"http://www.msperlin.com/blog/post/2017-08-12-switching-to-linux/","publishdate":"2017-08-12T00:00:00Z","relpermalink":"/blog/post/2017-08-12-switching-to-linux/","section":"post","summary":"Its been 8 months since I switched from Windows 10 to Linux Mint. In this post I’ll talk about my experience as a scholar and R user in the transition.\nMy work is, simply put, to communicate ideas. A typical day of work is a mixture of writing research papers, creating classroom material and admin tasks. Most of my work happens in front of a computer.\nWithout a doubt, my greatest asset are my computer files. I reuse them over and over. Every new research script is a new version of something I’ve done in the past.","tags":["Linux"],"title":"My experience in switching from Windows 10 to Linux Mint","type":"post"},{"authors":["Martin Pontuschka","Marcelo S. Perlin"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"59f782586da1045ad6f8a00e7d57bcd1","permalink":"http://www.msperlin.com/blog/publication/2017_rbfin-contegrationbrazilargentina/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/blog/publication/2017_rbfin-contegrationbrazilargentina/","section":"publication","summary":"Neste artigo buscamos verificar a dinâmica da integração financeira entre as séries temporais do índice acionário brasileiro e argentino. Para isto, estimamos um modelo de estado de espaço através do filtro de Kalman, que permite a observação da dinâmica da integração financeira ao longo do tempo. Seguimos a proposta de Haldane e Hall (1991), que sugerem um modelo de espaço de estado para observar convergência entre diferentes séries temporais. Ao estimar o modelo conseguimos observar momentos de convergência e momentos de divergência entre o mercado acionário brasileiro e o argentino ao longo dos anos entre 1987 e 2014. Uma importante evidência foi a observação de divergência entre os índices acionários regionais em momentos de crise nos mercados internacionais. Ou seja, nos momentos de maior estresse nos mercados financeiros, o índice acionário brasileiro apresentou maior convergência em relação aos mercados internacionais do que em relação ao mercado argentino. Esta evidência está de acordo com Manning (2002), o qual verificou o mesmo efeito nos mercados acionários asiáticos durante a crise de 1997.","tags":[],"title":"Análise de Integração Financeira entre o Mercado Acionário Brasileiro e o Argentino: Uma Abordagem Dinâmica","type":"publication"},{"authors":["Takeyoshi Imasato","Marcelo S. Perlin","Denis Borenstein"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"67a628447e34e205166e34c1dff15186","permalink":"http://www.msperlin.com/blog/publication/2017_rac-perfilacademicos/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/blog/publication/2017_rac-perfilacademicos/","section":"publication","summary":"A produção da ciência tem sido foco de diversas discussões, particularmente ao que se refere à produtividade científica. Este artigo tem como objetivo analisar o perfil de acadêmicos doutores que atuam na área de Administração no Brasil e as suas respectivas publicações científicas, de modo a identificar fatores que expliquem esse aspecto da produtividade científica. Utilizando estatística descritiva e modelos estatísticos, com base em dados provenientes de 1976 pesquisadores doutores registrados na Plataforma Lattes, a pesquisa identificou fatores ligados à formação e experiência que influem na produtividade científica dos acadêmicos em Administração. Os resultados da pesquisa também indicam que houve importantes modificações nos padrões de formação acadêmica e de produtividade no país. Considerando a centralidade do tema da produtividade científica, o trabalho sugere novas pesquisas para que se ampliem as discussões sobre os rumos da produção intelectual e da pós-graduação em Administração no Brasil.","tags":[],"title":"Análise do Perfil dos Acadêmicos e de suas Publicações Científicas em Administração","type":"publication"},{"authors":["Marcelo S. Perlin","João F. Caldeira","André A. P. Santos","Martin Pontuschka"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"a16df2475f7571e62fe3fac61b0c3e5e","permalink":"http://www.msperlin.com/blog/publication/2017_jf-gtrends/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/blog/publication/2017_jf-gtrends/","section":"publication","summary":"We study the case of mispricing in the odd lots equity market in Brazil. Contrary to expectation, odd lot investors are paying higher prices than round lot investors. The pricing difference between markets is affected by market returns, volatility and spreads. Our main hypothesis is that; once the assets traded in the odd lot market are more illiquid than their counterparts, the mispricing is driven by liquidity factors. Additionally, we show that the mispricing yields an arbitrage opportunity that is not being traded away in the Brazilian market. Therefore, we propose regulators to review the market design for odd lots in Brazil. We argue that reducing the minimal trading unit in the round lots market would benefit investors.","tags":[],"title":"Can we predict the financial markets based on Google's search queries?","type":"publication"},{"authors":["Henrique P. Ramos","Marcelo S. Perlin","Marcelo B. Righi"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"c454997f645baa3a7a9873fa5ea70717","permalink":"http://www.msperlin.com/blog/publication/2017_najef-mispricing-odd-lots/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/blog/publication/2017_najef-mispricing-odd-lots/","section":"publication","summary":"We study the case of mispricing in the odd lots equity market in Brazil. Contrary to expectation, odd lot investors are paying higher prices than round lot investors. The pricing difference between markets is affected by market returns, volatility and spreads. Our main hypothesis is that; once the assets traded in the odd lot market are more illiquid than their counterparts, the mispricing is driven by liquidity factors. Additionally, we show that the mispricing yields an arbitrage opportunity that is not being traded away in the Brazilian market. Therefore, we propose regulators to review the market design for odd lots in Brazil. We argue that reducing the minimal trading unit in the round lots market would benefit investors.","tags":[],"title":"Mispricing in the odd lots market in Brazil","type":"publication"},{"authors":["Henrique Pinto Ramos","Kadja Katherine Mendes Ribeiro","Marcelo S. Perlin"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"ae61cece9bd42e34c39cfb6b080df544","permalink":"http://www.msperlin.com/blog/publication/2017_ram-pesquisasgoogle/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/blog/publication/2017_ram-pesquisasgoogle/","section":"publication","summary":"Entender a capacidade preditiva de pesquisas no Google sobre o mercado financeiro brasileiro. Apesar de uma crescente literatura estrangeira utilizando dados sobre pesquisas oriundas no Google, no Brasil não se tem conhecimento de trabalhos desta natureza. A aplicação no mercado financeiro evidencia novas fontes de informação acerca do movimento dos mercados e pode contribuir para pesquisadores e praticantes entenderem melhor esta dinâmica. Principais aspectos metodológicos: Seguindo a literatura, foram estimados modelos VAR para investigar os efeitos em três variáveis dos mercados de renda acionário e de renda fixa: volume, retorno e volatilidade. Além disso, foi estudado o impacto da procura por termos relacionados à gripe no mercado de renda variável. Foram usados dados semanais de pesquisas do Google Trends e dos mercados financeiros entre o período de 2007 a 2014. Síntese dos principais resultados: Evidencia-se a existência de um efeito preditivo entre os níveis de pesquisas e as variáveis financeiras, principalmente no mercado de renda variável. Todavia, este resultado não foi robusto em todos os casos analisados. Destaca-se que, para a relação inversa, isto é, o mercado financeiro impactando as pesquisas no Google, encontrou-se forte evidência de uma relação causal. O uso de uma estratégia de negociação baseada neste tipo de dados gerou retornos maiores do que os bechmarkings definidos. O estudo revelou uma relação significativa entre o nível de pesquisas no Google e o mercado financeiro. Os resultados oferecem uma nova fonte de informação que afeta o mercado financeiro do Brasil.","tags":[],"title":"O poder preditivo de pesquisas na internet sobre o mercado financeiro brasileiro","type":"publication"},{"authors":["João F. Caldeira","Guilherme V. Moura","Marcelo S. Perlin","André A. P. Santos"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"11f95b4d21ce987adb9069a8a011fc06","permalink":"http://www.msperlin.com/blog/publication/2017_economia-portfoliomanagement/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/blog/publication/2017_economia-portfoliomanagement/","section":"publication","summary":"It is often argued that intraday returns can be used to construct covariance estimates that are more accurate than those based on daily returns. However, it is still unclear whether high frequency data provide more precise covariance estimates in markets more contaminated from microstructure noise such as higher bid-ask spreads and lower liquidity. We address this question by investigating the benefits of using high frequency data in the Brazilian equities market to construct optimal minimum variance portfolios. We implement alternative realized covariance estimators based on intraday returns sampled at alternative frequencies and obtain their dynamic versions using a multivariate GARCH framework. Our evidence based on a high-dimensional data set suggests that realized covariance estimators performed significantly better from an economic point of view in comparison to standard estimators based on low-frequency (close-to-close) data as they delivered less risky portfolios.","tags":[],"title":"Portfolio management using realized covariances: Evidence from Brazil","type":"publication"},{"authors":["Marcelo S. Perlin","André A. P. Santos","Takeyoshi Imasato","Denis Borenstein","Sergio da Silva"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"173893b8742ea79a880eae349e18b7e2","permalink":"http://www.msperlin.com/blog/publication/2017_ji-brazilianscientificoutput/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/blog/publication/2017_ji-brazilianscientificoutput/","section":"publication","summary":"We assemble a massive sample of 180,000 CVs of Brazilian academic researchers of all disciplines from the Lattes platform. From the CVs we gather information on key variables that possibly explain the quantity and impact of their output published in journals: gender, PhD origin (domestic or foreign), time taken for finishing a PhD program, and number of years involved in research since PhD completion. Researcher productivity is gauged by three metrics: average SCImago Journal Rank impact factor, average points from Qualis (which is a Brazilian domestic metrics that includes domestic journals), and average number of journal publications per year after PhD completion. Taken together, we find males are more productive in terms of quantity of publications, but the effect of gender in terms of quality is mixed for individual groups of subject areas. Holding a PhD from abroad increases the chance for a researcher to publish in journals of higher impact, whereas domestic PhDs publish more articles, but in journals of less impact. Thus, there is a trade-off between research impact and quantity. We also find that the more years a researcher takes to finish his or her doctorate, the more likely he or she will publish less thereafter. The data also support the existence of an inverted U-shaped function relating research age and productivity.","tags":[],"title":"The Brazilian scientific output published in journals: A study based on a large CV database","type":"publication"},{"authors":null,"categories":null,"content":" I’m looking forward to attending R in Finance conference in Chicago, next friday (2017-05-09). The program looks great! I am really happy, and a bit surprised, to see so many presentations related to market microstructure in the conference.\nI will talk about my package GetHFData in the first session. This is a package for downloading and aggregating trade data from Bovespa, the Brazilian exchange. More details are available in RBFIN.\nMy slides are ready and available.\n","date":1494806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494806400,"objectID":"b36bfd2378e22686aef084181afeeeb5","permalink":"http://www.msperlin.com/blog/post/2017-05-15-r-finance/","publishdate":"2017-05-15T00:00:00Z","relpermalink":"/blog/post/2017-05-15-r-finance/","section":"post","summary":"I’m looking forward to attending R in Finance conference in Chicago, next friday (2017-05-09). The program looks great! I am really happy, and a bit surprised, to see so many presentations related to market microstructure in the conference.\nI will talk about my package GetHFData in the first session. This is a package for downloading and aggregating trade data from Bovespa, the Brazilian exchange. More details are available in RBFIN.\nMy slides are ready and available.","tags":["R","R in finance"],"title":"Looking forward to RFinance - Chicago","type":"post"},{"authors":null,"categories":["R","CRAN","package names"],"content":" Setting a name for a CRAN package is an intimate process. Out of an infinite range of possibilities, an idea comes for a package and you spend at least a couple of days writing up and testing your code before submitting to CRAN. Once you set the name of the package, you cannot change it. Your choice index your effort and, it shouldn’t be a surprise that the name of the package can improve its impact.\nLooking at package names, one strategy that I commonly observe is to use small words, a verb or noun, and add the letter R to it. A good example is dplyr. Letter d stands for dataframe, ply is just a tool, and R is, well, you know. In a conventional sense, the name of this popular tool is informative and easy to remember. As always, the extremes are never good. A couple of bad examples of package naming are A3, AF, BB and so on. Googling the package name is definitely not helpful. On the other end, package samplesizelogisticcasecontrol provides a lot of information but it is plain unattractive!\nAnother strategy that I also find interesting is developers using names that, on first sight, are completely unrelated to the purpose of the package. But, there is a not so obvious link. One example is package sandwich. At first sight, I challenge anyone to figure out what it does. This is an econometric package that computes robust standard errors in a regression model. These robust estimates are also called sandwich estimators because the formula looks like a sandwich. But, you only know that if you studied a bit of econometric theory. This strategy works because it is easier to remember things that surprise us. Another great example is package janitor. I’m sure the you already suspects that it has something do to with data cleaning. And you are right! The message of the name is effortless and it works! The author even got the privilege of using letter R in the name.\nWhile I can always hand pick good and bad examples, let’s dig deeper. In this post, we will study the names of packages available in CRAN by comparing them to the whole English vocabulary. We are going use the following datasets:\n List of CRAN package, available with function available.packages(). List of English words, available at WordNet database. This is a comprehensive database of English words that I once used in a paper. It contains several tables, including all possible words from the English language.  First, let’s have a look at the distribution of size (number of characters) for all packages available in CRAN:\nlibrary(dplyr) library(ggplot2) # get data df.pkgs \u0026lt;- as.data.frame(available.packages(repos = \u0026#39;https://cloud.r-project.org/\u0026#39;)) %\u0026gt;% mutate(Package = as.character(Package), n.char = nchar(Package)) %\u0026gt;% rename(pkg = Package) %\u0026gt;% select(pkg, n.char) # plot it! p \u0026lt;- ggplot(df.pkgs, aes(x=n.char)) + geom_histogram(binwidth = 1) print(p) As I suspected, the names of CRAN packages are usually small, with an average of 5-6 characters. We have a couple of packages with more than 25 characters. Let’s see their names:\ndf.pkgs$pkg[df.pkgs$n.char\u0026gt;25] ## [1] \u0026quot;AnglerCreelSurveySimulation\u0026quot; \u0026quot;BipartiteModularityMaximization\u0026quot; ## [3] \u0026quot;BoutrosLab.plotting.general\u0026quot; \u0026quot;easyDifferentialGeneCoexpression\u0026quot; ## [5] \u0026quot;factset.analyticsapi.engines\u0026quot; \u0026quot;factset.protobuf.stachextensions\u0026quot; ## [7] \u0026quot;FractalParameterEstimation\u0026quot; \u0026quot;GeneralisedCovarianceMeasure\u0026quot; ## [9] \u0026quot;GreedyExperimentalDesignJARs\u0026quot; \u0026quot;ig.vancouver.2014.topcolour\u0026quot; ## [11] \u0026quot;image.CornerDetectionHarris\u0026quot; \u0026quot;MulvariateRandomForestVarImp\u0026quot; ## [13] \u0026quot;NegativeControlOutcomeAdjustment\u0026quot; \u0026quot;particle.swarm.optimisation\u0026quot; ## [15] \u0026quot;paws.application.integration\u0026quot; \u0026quot;RcmdrPlugin.sutteForecastR\u0026quot; ## [17] \u0026quot;ResidentialEnergyConsumption\u0026quot; \u0026quot;RoughSetKnowledgeReduction\u0026quot; ## [19] \u0026quot;samplesizelogisticcasecontrol\u0026quot; \u0026quot;sarp.snowprofile.alignment\u0026quot; ## [21] \u0026quot;SuperpixelImageSegmentation\u0026quot; \u0026quot;wyz.code.offensiveProgramming\u0026quot; I am sorry for the authors, but, in my opinion, I’m sure we could find better names. I am also sorry for those who are using these packages but do not use the autocomplete tool of RStudio and need to type the loooooooooong names.\nAs for my hypothesis that CRAN package have short names, let’s compare the distribution of package names against all words in the English language. For that, let’s load the WordNet database and do some calculations:\nlibrary(RSQLite) library(stringr) # get data conn \u0026lt;- dbConnect(drv = SQLite(), \u0026#39;wordnet/sqlite-31_snapshot.db\u0026#39;) words \u0026lt;- dbReadTable(conn, \u0026#39;wordsXsensesXsynsets\u0026#39;) %\u0026gt;% select(lemma) # some are duplicate (same word, different types) words \u0026lt;- unique(words) words$nchar \u0026lt;- nchar(words$lemma) # set df to plot df.to.plot \u0026lt;- data.frame(n.char = c(df.pkgs$n.char, words$nchar), source.char = c(rep(\u0026#39;CRAN pkgs\u0026#39;, nrow(df.pkgs)), rep(\u0026#39;English Vocabulary\u0026#39;, nrow(words)))) p \u0026lt;- ggplot(df.to.plot, aes(x=n.char, color=source.char )) + geom_density(size=1) + coord_cartesian(xlim=c(0, 40)) print(p) As I suspected, the distributions are very different. There is no need to apply a formal test as the visual evidence is clear: CRAN package have a tendency for shorter names.\nNow, let’s look at the distribution of used letters in relative terms:\nlibrary(scales) temp \u0026lt;- str_split(str_to_upper(df.pkgs$pkg), \u0026#39;\u0026#39;) all.chars \u0026lt;- do.call(what = c,args = temp) char.counts.pkg \u0026lt;- table(all.chars) temp \u0026lt;- str_split(str_to_upper(words$lemma), \u0026#39;\u0026#39;) all.chars \u0026lt;- do.call(what = c,args = temp) char.counts.words \u0026lt;- table(all.chars) df.to.plot \u0026lt;- data.frame(perc.count = c(char.counts.pkg/sum(char.counts.pkg), char.counts.words/sum(char.counts.words)), char = c(names(char.counts.pkg), names(char.counts.words)), source.char = c(rep(\u0026#39;CRAN pkgs\u0026#39;, length(char.counts.pkg)), rep(\u0026#39;WordNet\u0026#39;, length(char.counts.words)))) # only keep LETTERS idx \u0026lt;- df.to.plot$char %in% LETTERS df.to.plot \u0026lt;- df.to.plot[idx, ] p \u0026lt;- ggplot(df.to.plot, aes(x=char, y = perc.count, fill=source.char,width=.5)) + geom_col(position = \u0026#39;dodge\u0026#39;) + scale_y_continuous(labels = percent_format()) print(p) The result is really interesting! I was expecting far more differences in the relative use of characters. Not surprisingly, letter R is more used in package naming than in the English vocabulary. Still, the difference is not that large. Given that R is the name of the programming language, I was expecting a much greater proportion of R characters. My intuition was clearly wrong! In comparison, letters P and M have more difference in relative terms than letter R. I’m really not sure why that is. Overall, it is pretty clear the use of characters in the names of packages follow the distribution of words in the English language.\nWhile the distribution of letter is similar, we find just a few package with names exactly as in the English language. For all 18699 packages found in CRAN, only 1260 are an exact match of all 146625 unique words in the English vocabulary.\nSumming up, our data analysis shows that the names of packages are usually shorter than the words in the English language. However, when looking at distribution of used characters and editing distances, it is pretty clear that the names are based on the English language, usually with a few modifications of a base word.\nI hope you enjoyed this post.\n","date":1494288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494288000,"objectID":"8af2a341f1a91ab537ef38b7ea262e5e","permalink":"http://www.msperlin.com/blog/post/2017-05-09-studying-pkg-names/","publishdate":"2017-05-09T00:00:00Z","relpermalink":"/blog/post/2017-05-09-studying-pkg-names/","section":"post","summary":"Setting a name for a CRAN package is an intimate process. Out of an infinite range of possibilities, an idea comes for a package and you spend at least a couple of days writing up and testing your code before submitting to CRAN. Once you set the name of the package, you cannot change it. Your choice index your effort and, it shouldn’t be a surprise that the name of the package can improve its impact.\nLooking at package names, one strategy that I commonly observe is to use small words, a verb or noun, and add the letter R to it.","tags":["R","CRAN","package names"],"title":"Studying CRAN package names","type":"post"},{"authors":null,"categories":["R","book"],"content":" I am very please to announce that my book,Processing and Analyzing Financial Data with R, is finally out! This book is an english version of my previous title in portuguese. This is a long term project that I plan to keep on working over the years.\nYou can find it in Amazon. Following great titles about R, I decided to also publish an online version with full content here. More details about the book, including table of contents, is availabe in its webpage.\n","date":1493856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493856000,"objectID":"f7fc0d46dd6ef9f115ed6193c6a630c8","permalink":"http://www.msperlin.com/blog/post/2017-05-04-pafdr-is-out/","publishdate":"2017-05-04T00:00:00Z","relpermalink":"/blog/post/2017-05-04-pafdr-is-out/","section":"post","summary":"I am very please to announce that my book,Processing and Analyzing Financial Data with R, is finally out! This book is an english version of my previous title in portuguese. This is a long term project that I plan to keep on working over the years.\nYou can find it in Amazon. Following great titles about R, I decided to also publish an online version with full content here. More details about the book, including table of contents, is availabe in its webpage.","tags":["R","book"],"title":"My Book about using R in Finance","type":"post"},{"authors":null,"categories":["R","prophet","BatchGetSymbols"],"content":" Facebook recently released a API package allowing access to its forecasting model called prophet. According to the underling post:\nIt\u0026#39;s not your traditional ARIMA-style time series model. It\u0026#39;s closer in spirit to a Bayesian-influenced generalized additive model, a regression of smooth terms. The model is resistant to the effects of outliers, and supports data collected over an irregular time scale (ingliding presence of missing data) without the need for interpolation. The underlying calculation engine is Stan; the R and Python packages simply provide a convenient interface.  After reading it, I got really curious about the predictive performance of this method for stock prices. That is, can we predict stock price movements based on prophet? In this post I will investigate this research question using a database of prices for the SP500 components.\nBefore describing the code and results, it is noteworthy to point out that forecasting stock returns is really hard! There is a significant body of literature trying to forecast prices and to prove (or not) that financial markets are efficient in pricing publicly available information, including historical prices. This is the so called efficient market hypothesis. I have studied it, tried to trade for myself for a while when I was a Msc student, advised several graduate students on it, and the results are mostly the same: it is very difficult to find a trade signal that works well and is sustainable in real life.\nThis means that most of the variation in prices is due to random factors that cannot be anticipated. The explanation is simple, prices move according to investor’s expectation from available information. Every time that new (random) information, true or not, reaches the market, investor’s update their beliefs and trade accordingly. So, unless, new information or market expectation have a particular pattern, price changes will be mostly random.\nEven with a body of evidence against our research, it is still interesting to see how we could apply prophet in a trading setup.\nThe data First, let’s download stock prices for some components of the SP500 index since 2010.\nlibrary(BatchGetSymbols) ## Loading required package: rvest ## Loading required package: xml2 ## Loading required package: dplyr ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union ##  set.seed(100) all.stocks \u0026lt;- GetSP500Stocks()$Ticker my.stocks \u0026lt;- sample(all.stocks, 20) first.date \u0026lt;- as.Date(\u0026#39;2015-01-01\u0026#39;) last.date \u0026lt;- as.Date(\u0026#39;2019-01-01\u0026#39;) df.stocks \u0026lt;- BatchGetSymbols(my.stocks, first.date = first.date, last.date = last.date)[[2]] ## ## Running BatchGetSymbols for: ## ## tickers =FTV, ZBH, OXY, C, XLNX, VZ, BEN, WY, ROL, VTR, TSN, ABMD, MKC, MDLZ, CNP, PVH, ADBE, EXPD, L, UNM ## Downloading data for benchmark ticker ## ^GSPC | yahoo (1|1) | Found cache file ## FTV | yahoo (1|20) | Found cache file - Got 62% of valid prices | OUT: not enough data (thresh.bad.data = 75%) ## ZBH | yahoo (2|20) | Found cache file - Got 100% of valid prices | OK! ## OXY | yahoo (3|20) | Found cache file - Got 100% of valid prices | Got it! ## C | yahoo (4|20) | Found cache file - Got 100% of valid prices | You got it! ## XLNX | yahoo (5|20) | Found cache file - Got 100% of valid prices | OK! ## VZ | yahoo (6|20) | Found cache file - Got 100% of valid prices | Got it! ## BEN | yahoo (7|20) | Found cache file - Got 100% of valid prices | Feels good! ## WY | yahoo (8|20) | Found cache file - Got 100% of valid prices | Looking good! ## ROL | yahoo (9|20) | Found cache file - Got 100% of valid prices | Got it! ## VTR | yahoo (10|20) | Found cache file - Got 100% of valid prices | OK! ## TSN | yahoo (11|20) | Found cache file - Got 100% of valid prices | Feels good! ## ABMD | yahoo (12|20) | Found cache file - Got 100% of valid prices | Feels good! ## MKC | yahoo (13|20) | Found cache file - Got 100% of valid prices | Got it! ## MDLZ | yahoo (14|20) | Found cache file - Got 100% of valid prices | Feels good! ## CNP | yahoo (15|20) | Found cache file - Got 100% of valid prices | Got it! ## PVH | yahoo (16|20) | Found cache file - Got 100% of valid prices | Looking good! ## ADBE | yahoo (17|20) | Found cache file - Got 100% of valid prices | OK! ## EXPD | yahoo (18|20) | Found cache file - Got 100% of valid prices | You got it! ## L | yahoo (19|20) | Found cache file - Got 100% of valid prices | Good job! ## UNM | yahoo (20|20) | Found cache file - Got 100% of valid prices | Looking good! Now, let’s understand how prophet works. I was happy to see that the interface is quite simple, you offer a time series with input y and a date vector with ds. If no further custom option is set, you are good to go. My only complain with prophet is that that the function outputs lots of messages. They really should add a quiet option, so that the user doesn’t have to use capture.output to silent it. Have a look in the next example with a dummy series:\nlibrary(prophet) ## Loading required package: Rcpp ## Loading required package: rlang ## ## Attaching package: \u0026#39;rlang\u0026#39; ## The following object is masked from \u0026#39;package:xml2\u0026#39;: ## ## as_list df.est \u0026lt;- data.frame(y = rnorm(100), ds = Sys.Date() + 1:100) m \u0026lt;- prophet(df = df.est) ## Disabling yearly seasonality. Run prophet with yearly.seasonality=TRUE to override this. ## Disabling daily seasonality. Run prophet with daily.seasonality=TRUE to override this. The next step is to think about how to structure a function for our research problem. Our study has two steps, first we will set a training (in-sample) period, estimate the model and make forecasts. After that, we use the out-of-sample data to test the accuracy of the model.\nThe whole procedure of estimating and forecasting will be encapsulated in a single R function. This is not the best way of doing it but, for our simple example, it will suffice. My function will take as input a dataframe and the number of out-of-sample forecasts. Based on the adjusted closing prices, we calculate returns and feed 1:(nrow(df)-nfor) rows for the estimation. The last nfor rows are used for testing the accuracy of the model. For example, if I have a vector with 1000 returns and nfor=5, I use observations from 1:995 for estimating the model and 996:1000 for testing the forecasts. The function returns a dataframe with the predictions for each horizon, its error, among other things. Here’s the function definition:\nest.model.and.forecast \u0026lt;- function(df.in, nfor=5){ # Estimated model using prophet and forecast it # # Args: # df.in - A dataframe with columns price.adjusted and ref.date # nfor - Number of out-of-sample forecasts # # Returns: # A dataframe with forecasts and errors for each horizon. require(prophet) require(dplyr) my.ticker \u0026lt;- df.in$ticker[1] #cat(\u0026#39;\\nProcessing \u0026#39;, my.ticker) df.in \u0026lt;- df.in %\u0026gt;% select(ref.date, ret.adjusted.prices) names(df.in) \u0026lt;- c(\u0026#39;ds\u0026#39;, \u0026#39;y\u0026#39;) idx \u0026lt;- nrow(df.in) - nfor df.est \u0026lt;- df.in[1:idx, ] df.for \u0026lt;- df.in[(idx + 1):nrow(df.in), ] capture.output( m \u0026lt;- prophet(df = df.est) ) # forecast 50 days ahead (it also includes non trading days) df.pred \u0026lt;- predict(m, make_future_dataframe(m, periods = nfor + 50)) df.pred$ds \u0026lt;- as.Date(df.pred$ds) df.for \u0026lt;- merge(df.for, df.pred, by = \u0026#39;ds\u0026#39;) df.for \u0026lt;- select(df.for, ds, y, yhat) # forecast statistics df.for$eps \u0026lt;- with(df.for,y - yhat) df.for$abs.eps \u0026lt;- with(df.for,abs(y - yhat)) df.for$perc.eps \u0026lt;- with(df.for,(y - yhat)/y) df.for$nfor \u0026lt;- 1:nrow(df.for) df.for$ticker \u0026lt;- my.ticker return(df.for) } Let’s try it out using the by function to apply it for each stock in our sample. All results are later combined in a single dataframe with function do.call.\nout.l \u0026lt;- by(data = df.stocks, INDICES = df.stocks$ticker, FUN = est.model.and.forecast, nfor = 5) my.result \u0026lt;- do.call(rbind, out.l) Lets have a look in the resulting dataframe:\nhead(my.result) ## ds y yhat eps abs.eps perc.eps ## ABMD.1 2018-12-24 -0.031726969 -0.005299687 -0.02642728 0.02642728 0.8329596 ## ABMD.2 2018-12-26 0.093781188 -0.002497698 0.09627889 0.09627889 1.0266333 ## ABMD.3 2018-12-27 0.026769487 -0.005186820 0.03195631 0.03195631 1.1937587 ## ABMD.4 2018-12-28 0.007919663 -0.001131558 0.00905122 0.00905122 1.1428795 ## ABMD.5 2018-12-31 0.021592217 -0.003278144 0.02487036 0.02487036 1.1518206 ## ADBE.1 2018-12-24 -0.017432945 -0.005505203 -0.01192774 0.01192774 0.6842070 ## nfor ticker ## ABMD.1 1 ABMD ## ABMD.2 2 ABMD ## ABMD.3 3 ABMD ## ABMD.4 4 ABMD ## ABMD.5 5 ABMD ## ADBE.1 1 ADBE In this object you’ll find the forecasts (yhat), the actual values (y), the absolute and normalized error (abs.eps, perc.eps).\nFor ou first analysis, let’s have a look on the effect of the forecasting horizon over the absolute error distribution.\nlibrary(ggplot2) p \u0026lt;- ggplot(my.result, aes(x=factor(nfor), y=abs.eps)) p \u0026lt;- p + geom_boxplot() print(p) We do find some positive dependency. As the horizon increases, the forecasting algorithm makes more mistakes. Surprisingly, this pattern is not found for nfor=5 and nfor=4. It might be interesting to add more data and check if this effect is robust.\n Encopassing test A simple and powerful test for verifying the accuracy of a prediction algorithm is the encompassing test. The idea is to estimate the following linear model with the real returns (\\(R_t\\)) and its predictions (\\(\\hat{R} _t\\)).\n\\[y_t = \\alpha + \\beta\\hat{y_t} + \\epsilon _t\\]\nIf the model provides good forecasts, we can expect that \\(\\alpha\\) is equal to zero (no bias) and \\(\\beta\\) is equal to 1. If both conditions are true, we have that \\(R_t = \\hat{R} _t + \\epsilon _t\\)$, meaning that our forecasting model provides an unbiased estimator of the predicted variable. In a formal research, we could use a Wald test to verify this hypothesis jointly.\nFirst, lets find the result of the encompassing test for all forecasts.\nlm.model \u0026lt;- lm(formula = y ~yhat, data = my.result) summary(lm.model) ## ## Call: ## lm(formula = y ~ yhat, data = my.result) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.055237 -0.012307 -0.000287 0.008360 0.086257 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.007954 0.004063 1.957 0.0533 . ## yhat 0.171995 1.132177 0.152 0.8796 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.02675 on 93 degrees of freedom ## Multiple R-squared: 0.0002481, Adjusted R-squared: -0.0105 ## F-statistic: 0.02308 on 1 and 93 DF, p-value: 0.8796 As you can see, it didn’t work very well. The constant is significant, which indicates a bias. The value of 0.1719945 is not very close to 1. But, it could be the case that the different horizon have different results. A longer horizon, with bad forecasts, will be affecting short horizons with good forecasts. Lets use dplyr to separate our model according to nfor.\nmodels \u0026lt;- my.result %\u0026gt;% group_by(nfor) %\u0026gt;% do(ols.model = lm(data = ., formula = y ~ yhat )) We report the results with texreg::screenreg.\ntexreg::screenreg(models$ols.model) ## ## ============================================================== ## Model 1 Model 2 Model 3 Model 4 Model 5 ## -------------------------------------------------------------- ## (Intercept) -0.04 *** 0.05 *** 0.01 -0.00 0.01 *** ## (0.00) (0.01) (0.00) (0.00) (0.00) ## yhat -2.32 ** -0.14 -1.01 -0.19 -0.22 ## (0.77) (2.82) (0.87) (0.51) (0.72) ## -------------------------------------------------------------- ## R^2 0.35 0.00 0.07 0.01 0.01 ## Adj. R^2 0.31 -0.06 0.02 -0.05 -0.05 ## Num. obs. 19 19 19 19 19 ## RMSE 0.01 0.02 0.01 0.01 0.01 ## ============================================================== ## *** p \u0026lt; 0.001, ** p \u0026lt; 0.01, * p \u0026lt; 0.05 Well, the R2 shows some evidence that shorter horizons have better results in the encompassing test. But, we got some negative betas! This means that, for some horizons, it might be better to take the opposite suggestion of the forecast!\n Trading based on forecasts In a practical trading applications, it might not be of interest to forecast actual returns. If you are trading according to these forecasts, you are probably more worried about the direction of the forecasts and not its nominal error. A model can have bad nominal forecasts, but be good in predicting the sign of the next price movement. If this is the case, you can still make money even though your model fails in the encompassing test.\nLet’s try it out with a simple trading strategy for all different horizons:\n buy in end of day t if forecast in t+1 is positive and sell at the end of t+1 short-sell in the end of day t when forecast for t+1 is negative and buy it back in the end of t+1  The total profit will be given by:\nmy.profit \u0026lt;- sum(with(my.result, (yhat\u0026gt;0)*y + (yhat\u0026lt;0)*-y)) print(my.profit) ## [1] -0.7584576 Not bad! Doesn’t look like much, but remember that we have a few trading days and this return might be due to a sistematic effect in the market. Let’s see how this result compares to random trading signals.\nn.sim \u0026lt;- 10000 monkey.ret \u0026lt;- numeric(length = n.sim) for (i in seq(n.sim)) { rnd.vec \u0026lt;- rnorm(length(my.result$y)) monkey.ret[i] \u0026lt;- sum( (rnd.vec\u0026gt;0)*my.result$y + (rnd.vec\u0026lt;0)*-my.result$y ) } temp.df \u0026lt;- data.frame(monkey.ret, my.profit) p \u0026lt;- ggplot(temp.df, aes(monkey.ret)) p \u0026lt;- p + geom_histogram() p \u0026lt;- p + geom_vline(aes(xintercept = my.profit),size=2) p \u0026lt;- p + labs(x=\u0026#39;Returns from random trading signals\u0026#39;) print(p) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The previous histogram shows the total return from randomnly generated signals in 10^{4} simulations. The vertical line is the result from using prophet. As you can see, it is a bit higher than the average of the distribution. The total return from prophet is lower than the return of the naive strategy in 99.72 percent of the simulations. This is not a bad result. But, notice that we didnt add trading or liquidity costs to the analysis, which will make the total returns worse.\nThe main results of this simple study are clear: prophet is bad at point forecasts for returns, specially for longer horizons, but does quite better in directional predictions. It might be interesting to test it further, with more data, adding trading costs, other forecasting setups, and see if the results hold.\n ","date":1488672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488672000,"objectID":"85558e42cdfc69fda78d2f817209d691","permalink":"http://www.msperlin.com/blog/post/2017-03-05-prophet-and_stock-market/","publishdate":"2017-03-05T00:00:00Z","relpermalink":"/blog/post/2017-03-05-prophet-and_stock-market/","section":"post","summary":"Facebook recently released a API package allowing access to its forecasting model called prophet. According to the underling post:\nIt\u0026#39;s not your traditional ARIMA-style time series model. It\u0026#39;s closer in spirit to a Bayesian-influenced generalized additive model, a regression of smooth terms. The model is resistant to the effects of outliers, and supports data collected over an irregular time scale (ingliding presence of missing data) without the need for interpolation. The underlying calculation engine is Stan; the R and Python packages simply provide a convenient interface.  After reading it, I got really curious about the predictive performance of this method for stock prices.","tags":["R","prophet","BatchGetSymbols"],"title":"Can we predict stock prices with Prophet?","type":"post"},{"authors":null,"categories":["R","book","self-publish"],"content":"  Many people, including my university colleagues and friends, have asked me about the process of writing a book and self publishing it in Amazon. You can find the details about the english version of the book here and here. Given so much interest, I’m going to report the whole process in this post.\nFirst, motivation. Why did I write a book?\nI am a university professor. Writing is a major part of my work and I really enjoy it. Think about it, it is a magical process. I press a specific and long combination of strokes in my keyboard and that translates into information distributed all over the world. This information helps people in a particular problem, just as I have been helped in the past by reading the work of other people.\nA longer answer, I wrote the book because I simply felt like studying and learning more about R. Writing a book is an excellent opportunity for doing so as it forces you to think about a topic and communicate it clearly and objectively. After reading the book from Grangrud, I got some inspiration for writing my own book about R, reproducible research and Finance. My experience in the academia tells me that people often learn programming by themselves, without any consideration for the structure and re-usability of the code. You can see examples of it when you download code from other researchers. In most cases, it is a big mess and I often found it easier (and less risky) to rewrite it all from scratch. So, writing a book is a way to show to people how to use R for data analysis in finance and how to organize code that can be used later, saving lots of time of development. As I mentioned in a previous post, another argument for writing the book is that, even if I didn’t publish it, I would still have very good class notes for my graduate students or perhaps a pdf to host in my website.\nOff course you may wonder about the financial side. After all, I am a finance professor and finance is the science of money. While you can make money selling books, the financial incentives for writing a new technical book are quite low. In financial terms, writing a technical book is a project with negative expected Net Present Value (NPV). This means that, from the ex ante point of view of a potential writer, working in a technical book simply does not create financial value. There is a high production cost in terms of how much time you spend writing and formatting the book and a high uncertainty about the future royalties. If you look at it from the financial side, it is just a bad gamble. This is why you should only write a book about something you love and want to learn more about. Otherwise, it can become a big frustration.\nThe writing process The first step in writing my book was to set out a summary, that is, the names of the chapters. The first part of it was pretty obvious, it covered the basics of using R: packages, objects, functions, loops and so on. The rest of the topics were all based on my experience. I asked the question What are the most important data tasks that a finance student should learn?. Once you have a TOC (table of contents) ready, it is all about filling up the book with content.\nI really enjoyed the process of writing the book. Taking a long term project such as a book requires discipline. One strategy that really helped me was to work very early in the morning. As soon as the sun rised at 0630, I’m up and in my way to the university. From 07:00 am to 0900 am, nothing really happens at the university, giving me plenty of time to write peacefully. Some days I could write all morning, if I felt like it and had a free schedule. One tip for anyone that is thinking about writing a book (or article), only write when you fell like writing. It is really difficult to write well if your mind is not into it. Not having a deadline for this project meant that I could take my time in developing it.\nAs for software, the first version of the book was written in TexStudio with the content in Sweave files (.Rnw), and a simple hack to call Sweave from R and compile the resulting tex file. I’m used to work with latex, so the choice for textudio was obvious. I like this latex editor as it integrates nicely with grammar checking tools and latex compilation. Why not knitr? Well, at the time I was very comfortable with Sweave. It offered everything I needed. I didn’t see a reason for change. Clearly, I was wrong. Once I started to investigate how to format better the code in the book, it became clear that I should be using knitr, which has all these extra options that are not available in Sweave. I switched it as soon as I realized how much I would gain there was.\nAfter 6 months of work, I had a first readable version of the book. This is when I started to investigate how to turn a latex file into an ebook file. To my surprise, this is not easy! It could be the case that I had lots of code and figures from the compilation. I tried all the existing software and, to my frustration, it just didn’t work. When I didn’t got an error message that stopped the conversion, the resulting epub file look awful in the kindle reader. The warning and error messages from the latex2ebook compiler also didn’t help a lot.\nBy looking for solutions in the internet and with a bit of luck, I’ve found that Yihui Xie had just published his bookdown package. I tried out the first chapters and it worked perfectly! The great thing about bookdown is that you can output to pdf, html or epub with the same files. I was very happy that, finally, I would be able to get all formats that I needed in a single platform. I’m not going to go into the details about how to use bookdown. You can find the official tutorial here. It is worth to point out that the written content is just RMarkdown so, if you know latex, you’ll fell right at home. So, here I go again, switching a whole book from latex to Rmarkdown.\nThe only feature in bookdown that didn’t work out of the box was the equation in the kindle format. The formulas just didn’t print well. The weird part is that they looked fine in any other ebook reader, but not in Kindle. Apparently, there is no solution for this problem besides using figure files for the equations. What I did to solve it was to use R switches within the book so that the epub compilation used a figure file created in codecogs and the latex/html compilation used the normal code for equations. See here an example of code for the first equation in chapter 9.\nAs you can see, I used a switch in the knitr command as in include=identical(knitr:::pandoc_to(), 'epub3') to run each code conditional to the type of output. As for inline equations, I just used straight text to indicate coefficients such as beta, alpha. I know I could have used the same strategy, but I didn’t fell the need since there were so few inline equations in the text.\nAfter switching to bookdown, I added a couple more chapters in the following months. A comment here is that I tried writing in RStudio but it didn’t worked out for me. Perhaps it was due to the large files I was handling, but it felt really clunky, slow and not reactive. Also, the grammar check system in Rstudio needs a lot of work. I tried going back to Texstudio but it didn’t had support for markdown highlighting. I then switched software by doing the writing in notepad++ + markdown extension and only used RStudio to compile the book. If you haven’t tried notepad++, give it a go. I really like it and I find myself using it more and more.\n Publishing in Amazon Around october of 2016, the content of the book started to take a good shape and it was time to investigate where and how to publish it. I got in touch with a major local publisher here in Brazil and, after one month, they informed me that they were not interested in the book. While I could try other publishers, I really didn’t felt like going through another month of evaluation. I studied for while and decided to self-publish the book.\nOne positive aspect of self publishing is that big publishers downsized over the years and good professionals are now in the market for hire. This means that a large range of good editors and designers are available to the independent author. In my case, I fought the urge to do the cover myself and searched for a cover designer. After a lot of search, I hired Capista to do the book cover. I really liked his portfolio and, he offers a discount for self-publish authors, which is very nice of him.\nI also hired professionals to check the text for mistakes and grammar. This is particular important as grammar mistakes are normal and expected in large documents. But, selecting a reviewer was an interactive process. I searched the web and hired tree different reviewers and offered them three different chapters of the book. Once I had all revisions, I selected the reviewer the offered the best result and sent him the rest of the chapters. You can find his site here. I’m glad I did this way, It would be a nightmare if I hired the wrong person to do this job.\nThe main advantage of self publication is that the royalties for the author are higher and the cost to publish are lower. This setup translates into a lower price of the book and more sells. Specific to Brazil, where the exchange rate of dollar to Real (the Brazilian currency) is very unfavourable, it makes sense to offer a lower priced book. This way, it is easier for students and instructor to acquire and use the book.\nSince ebooks is a growing market, you can find lots of tutorials about this topic in the internet, like this one and this one. The problem here is that they are mostly about self-publishing text ebooks, as in fiction stories. The only article I’ve found about self-publishing technical books is this one, where the author describe their rather good experience in publishing a software-engineering textbook.\nI checked Lulu, Smashwords and Amazon KDP. What made my choice towards Amazon was that it was a established platform, where almost everyone is registered. Buying an Ebook in Amazon is just one click away. Also, I’ve found that the KDP (self publish) program of Amazon is quite good. If the ebook sells for less than 9 dollars and is enrolled in the Amazon unlimited program, the author gets 70 percent of the sticker price in royalties, which is a nice percentage! A bit of information, the normal royalty rate for an author of a publishing company is around 5-10 percent of the book price. Just a quick comparison, for each ebook sold at 9 USD, the author gets 6.3 USD (0.70 times 9). In other scenario, assuming an author has a publisher that offered 10 percent royalty, you would get the same royalty for a a book priced at 63 USD (6.3/0.1). The big difference here is that a 9 USD ebook sells a lot more than a 63 USD ebook, resulting in higher royalties and higher impact. More people read your book and you make more money. My only disappointment with Amazon is that the printed books are shipped from the USA. A buyer of the printed book in Brazil will need to wait at least 10-15 days to get the book. But, overall, I find this to be a small cost to bear.\nWhen the final version of the content of the book was ready, I started to organize everything I needed in order to publish the book. This is the list:\nA ISBN number - A unique id that indexes your book, one for the ebook and another for the print version. In order to get an ISBN, you need to find the representation of ISBN in your country. Amazon also offers their own ISBN, but I don’t advise to use it. Having your own ISBN gives you more control. Just get one yourself. It is easy and cheap. In my case, I sent the request from the website by filling up forms in less than 10 minutes and got the number in five days.\n Cover - The cover is processed as a figure file (.jpg). An important information here is that you need to define the size of you book and the number of pages at this stage. In my case, I used a 7x11 inches configuration for 200+ pages. It looks quite nice. Have a look:\n  My Book and my spanish Iguana!\n Beautiful cover! Isn’t it? Once you have the figure file, you just add it to the book creator system in amazon. At this stage, you will also need a summary about the book and its author and a picture for the back cover of the printed version.\nRegistration in Amazon. The site asks you a lot of information. It took me at least half an hour to fill up the forms, which also includes tax details for us and non-us residents.\n Description of the book. This is the text that goes in the webpage of the book in Amazon. In my case, I just used the same text as the back cover.\n A book site. I wanted to distribute the code from the book and also exercises over the web. I used Google Sites, but I’m sure you have plenty of options here. On a side note, I’m also considering writing a CRAN package for distributing this content.\n  For the ebook, the next step is easy. Just upload the cover, the .epub file from bookdown, write the description and other details and you are good to go. With everything ready to go, you can check the ebook page by page with the online viewer. I didn’t have any problems at this stage. As for the print version, it is far more complicated. In my case, I used the pdf file to build the printed version. Once you upload the pdf, you can check the result in a online viewer. This is the final proof of the book and shows how it would look like when printed. There are lots of error checking from the system and, if you got an error, you cannot publish the book. I’ve spent a lot of time formatting everything so that the book comes out perfectly. The problem is that, each attempt to solve errors demanded processing time. At least 10 minutes to reach the whole cycle of compiling the pdf, uploading and checking the result. So, every time I tried to solve an error, I got the result 10 minutes later. I repeated this process over and over until I had a perfect book.\n The publication The book was launched in the sixth of February 2017. I advertised it in here, my blog, Facebook and email. I was very surprised with the strong reception. In the first and second day, it became the best seller book in the section of finance in the Brazilian Amazon. I am really happy for the success of the book. Right now, the sales have gone down, which is normal and expected, but I’m getting all sorts of good responses. What surprised me is that, even though the book was published primarily for Brazil, I’m receiving lots of replies (and sales) of people from other Portuguese speaking countries such as Cabo Verde and Portugal!\n Some advices If I could go back in time and advice myself about the book, this is what I would have said. I hope these advices are helpful for others as well.\n From the first version, make up your mind about book size and letter size. This decision will impact everything else. Problems are easier to detect and solve if you stick to a single format.\n RStudio and bookdown are your friends. I’m not aware of any other free platform that offers anything like it for developing a technical book.\n When possible, use tidy=TRUE in the chunks of code. This simple command forces the code within the boundaries of the pdf. Sometimes it doesn’t look good so you always need to mannualy check the chunks with lots of code.\n Avoid using function str in the content. For some reason, it does not respect the boundaries of the page. There is a solution but it would not look good in the book.\n Use a dark theme for writing. Your eyes will hurt less and you will be able to write more.\n Talk to people that understands the material you are writing about. Listen to their advices and make changes when necessary. Don’t over commit to sections that you are not sure will be included.\n For all figure files in your book, always get the highest possible resolution (at least 300 dpi). Amazon will not let you publish figures with low resolution.\n Don’t even try to do yourself the book cover or grammar check. Hire someone to do it for you. Your future self will thank you later!\n   Whats next? I really enjoyed the experience of self-publishing a book and I strongly advise for anyone to give it a try. A cautionary note, I never worked with a established publisher and, therefore, I cannot talk about how it is or compare it to self-publish. If someone has had that experience, fell free to use the comments to discuss it.\nThe impact of a technical book is uncertain. Only time will tell if the book is successful or not. I have high hopes though. The feedback from the community has been strongly positive. I also have been working on extra material such as exercises and slides that are going to be distributed for free. This should motivate professors to use the book in class and increase distribution.\nI hope this post was helpful and enlightening for potential authors. Comments or suggestions, please use the comment section or drop me an email at marceloperlin@gmail.com.\nBest, Marcelo.\n ","date":1487203200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1487203200,"objectID":"b8a0284b99f38e4b1b1ade008183676f","permalink":"http://www.msperlin.com/blog/post/2017-02-16-writing-a-book/","publishdate":"2017-02-16T00:00:00Z","relpermalink":"/blog/post/2017-02-16-writing-a-book/","section":"post","summary":"Many people, including my university colleagues and friends, have asked me about the process of writing a book and self publishing it in Amazon. You can find the details about the english version of the book here and here. Given so much interest, I’m going to report the whole process in this post.\nFirst, motivation. Why did I write a book?\nI am a university professor. Writing is a major part of my work and I really enjoy it. Think about it, it is a magical process. I press a specific and long combination of strokes in my keyboard and that translates into information distributed all over the world.","tags":["R","book","self-publish"],"title":"Writing a R book and self-publishing it in Amazon","type":"post"},{"authors":null,"categories":["R","tennis"],"content":" In the previous post about tennis, we studied how changes in ball’s composition in hard and grass courts affected the game back in 2000. In this post, we will analyse a different dataset from the same repository and look at the players winning records in ATP matches.\nThe data I’m again using the great repository of tennis data of Jeff Sackmann. In this case, however, I’m using the ATP repository that contains ATP match data since 1968 until today. Again, I thank Jeff Sackmann for making this dataset publicly available.\nFirst, let’s download and unzip the dataset. This is a large file with 35MB and it takes some time.\nzip.file \u0026lt;- \u0026#39;TennisData_ATP.zip\u0026#39; # download file if (!file.exists(zip.file)){ download.file(\u0026#39;https://github.com/JeffSackmann/tennis_atp/archive/master.zip\u0026#39;, destfile = zip.file) } # unzip it dir.out \u0026lt;- \u0026#39;tennis_atp-master/\u0026#39; if (!dir.exists(dir.out)) unzip(zip.file) Lets have a look at the contents of the zip file.\nmy.f \u0026lt;- list.files(dir.out, pattern = \u0026#39;*.csv\u0026#39;, full.names = T) print(my.f) ## [1] \u0026quot;tennis_atp-master//atp_matches_1968.csv\u0026quot; ## [2] \u0026quot;tennis_atp-master//atp_matches_1969.csv\u0026quot; ## [3] \u0026quot;tennis_atp-master//atp_matches_1970.csv\u0026quot; ## [4] \u0026quot;tennis_atp-master//atp_matches_1971.csv\u0026quot; ## [5] \u0026quot;tennis_atp-master//atp_matches_1972.csv\u0026quot; ## [6] \u0026quot;tennis_atp-master//atp_matches_1973.csv\u0026quot; ## [7] \u0026quot;tennis_atp-master//atp_matches_1974.csv\u0026quot; ## [8] \u0026quot;tennis_atp-master//atp_matches_1975.csv\u0026quot; ## [9] \u0026quot;tennis_atp-master//atp_matches_1976.csv\u0026quot; ## [10] \u0026quot;tennis_atp-master//atp_matches_1977.csv\u0026quot; ## [11] \u0026quot;tennis_atp-master//atp_matches_1978.csv\u0026quot; ## [12] \u0026quot;tennis_atp-master//atp_matches_1979.csv\u0026quot; ## [13] \u0026quot;tennis_atp-master//atp_matches_1980.csv\u0026quot; ## [14] \u0026quot;tennis_atp-master//atp_matches_1981.csv\u0026quot; ## [15] \u0026quot;tennis_atp-master//atp_matches_1982.csv\u0026quot; ## [16] \u0026quot;tennis_atp-master//atp_matches_1983.csv\u0026quot; ## [17] \u0026quot;tennis_atp-master//atp_matches_1984.csv\u0026quot; ## [18] \u0026quot;tennis_atp-master//atp_matches_1985.csv\u0026quot; ## [19] \u0026quot;tennis_atp-master//atp_matches_1986.csv\u0026quot; ## [20] \u0026quot;tennis_atp-master//atp_matches_1987.csv\u0026quot; ## [21] \u0026quot;tennis_atp-master//atp_matches_1988.csv\u0026quot; ## [22] \u0026quot;tennis_atp-master//atp_matches_1989.csv\u0026quot; ## [23] \u0026quot;tennis_atp-master//atp_matches_1990.csv\u0026quot; ## [24] \u0026quot;tennis_atp-master//atp_matches_1991.csv\u0026quot; ## [25] \u0026quot;tennis_atp-master//atp_matches_1992.csv\u0026quot; ## [26] \u0026quot;tennis_atp-master//atp_matches_1993.csv\u0026quot; ## [27] \u0026quot;tennis_atp-master//atp_matches_1994.csv\u0026quot; ## [28] \u0026quot;tennis_atp-master//atp_matches_1995.csv\u0026quot; ## [29] \u0026quot;tennis_atp-master//atp_matches_1996.csv\u0026quot; ## [30] \u0026quot;tennis_atp-master//atp_matches_1997.csv\u0026quot; ## [31] \u0026quot;tennis_atp-master//atp_matches_1998.csv\u0026quot; ## [32] \u0026quot;tennis_atp-master//atp_matches_1999.csv\u0026quot; ## [33] \u0026quot;tennis_atp-master//atp_matches_2000.csv\u0026quot; ## [34] \u0026quot;tennis_atp-master//atp_matches_2001.csv\u0026quot; ## [35] \u0026quot;tennis_atp-master//atp_matches_2002.csv\u0026quot; ## [36] \u0026quot;tennis_atp-master//atp_matches_2003.csv\u0026quot; ## [37] \u0026quot;tennis_atp-master//atp_matches_2004.csv\u0026quot; ## [38] \u0026quot;tennis_atp-master//atp_matches_2005.csv\u0026quot; ## [39] \u0026quot;tennis_atp-master//atp_matches_2006.csv\u0026quot; ## [40] \u0026quot;tennis_atp-master//atp_matches_2007.csv\u0026quot; ## [41] \u0026quot;tennis_atp-master//atp_matches_2008.csv\u0026quot; ## [42] \u0026quot;tennis_atp-master//atp_matches_2009.csv\u0026quot; ## [43] \u0026quot;tennis_atp-master//atp_matches_2010.csv\u0026quot; ## [44] \u0026quot;tennis_atp-master//atp_matches_2011.csv\u0026quot; ## [45] \u0026quot;tennis_atp-master//atp_matches_2012.csv\u0026quot; ## [46] \u0026quot;tennis_atp-master//atp_matches_2013.csv\u0026quot; ## [47] \u0026quot;tennis_atp-master//atp_matches_2014.csv\u0026quot; ## [48] \u0026quot;tennis_atp-master//atp_matches_2015.csv\u0026quot; ## [49] \u0026quot;tennis_atp-master//atp_matches_2016.csv\u0026quot; ## [50] \u0026quot;tennis_atp-master//atp_matches_2017.csv\u0026quot; ## [51] \u0026quot;tennis_atp-master//atp_matches_2018.csv\u0026quot; ## [52] \u0026quot;tennis_atp-master//atp_matches_2019.csv\u0026quot; ## [53] \u0026quot;tennis_atp-master//atp_matches_futures_1991.csv\u0026quot; ## [54] \u0026quot;tennis_atp-master//atp_matches_futures_1992.csv\u0026quot; ## [55] \u0026quot;tennis_atp-master//atp_matches_futures_1993.csv\u0026quot; ## [56] \u0026quot;tennis_atp-master//atp_matches_futures_1994.csv\u0026quot; ## [57] \u0026quot;tennis_atp-master//atp_matches_futures_1995.csv\u0026quot; ## [58] \u0026quot;tennis_atp-master//atp_matches_futures_1996.csv\u0026quot; ## [59] \u0026quot;tennis_atp-master//atp_matches_futures_1997.csv\u0026quot; ## [60] \u0026quot;tennis_atp-master//atp_matches_futures_1998.csv\u0026quot; ## [61] \u0026quot;tennis_atp-master//atp_matches_futures_1999.csv\u0026quot; ## [62] \u0026quot;tennis_atp-master//atp_matches_futures_2000.csv\u0026quot; ## [63] \u0026quot;tennis_atp-master//atp_matches_futures_2001.csv\u0026quot; ## [64] \u0026quot;tennis_atp-master//atp_matches_futures_2002.csv\u0026quot; ## [65] \u0026quot;tennis_atp-master//atp_matches_futures_2003.csv\u0026quot; ## [66] \u0026quot;tennis_atp-master//atp_matches_futures_2004.csv\u0026quot; ## [67] \u0026quot;tennis_atp-master//atp_matches_futures_2005.csv\u0026quot; ## [68] \u0026quot;tennis_atp-master//atp_matches_futures_2006.csv\u0026quot; ## [69] \u0026quot;tennis_atp-master//atp_matches_futures_2007.csv\u0026quot; ## [70] \u0026quot;tennis_atp-master//atp_matches_futures_2008.csv\u0026quot; ## [71] \u0026quot;tennis_atp-master//atp_matches_futures_2009.csv\u0026quot; ## [72] \u0026quot;tennis_atp-master//atp_matches_futures_2010.csv\u0026quot; ## [73] \u0026quot;tennis_atp-master//atp_matches_futures_2011.csv\u0026quot; ## [74] \u0026quot;tennis_atp-master//atp_matches_futures_2012.csv\u0026quot; ## [75] \u0026quot;tennis_atp-master//atp_matches_futures_2013.csv\u0026quot; ## [76] \u0026quot;tennis_atp-master//atp_matches_futures_2014.csv\u0026quot; ## [77] \u0026quot;tennis_atp-master//atp_matches_futures_2015.csv\u0026quot; ## [78] \u0026quot;tennis_atp-master//atp_matches_futures_2016.csv\u0026quot; ## [79] \u0026quot;tennis_atp-master//atp_matches_futures_2017.csv\u0026quot; ## [80] \u0026quot;tennis_atp-master//atp_matches_futures_2018.csv\u0026quot; ## [81] \u0026quot;tennis_atp-master//atp_matches_futures_2019.csv\u0026quot; ## [82] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1978.csv\u0026quot; ## [83] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1979.csv\u0026quot; ## [84] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1980.csv\u0026quot; ## [85] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1981.csv\u0026quot; ## [86] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1982.csv\u0026quot; ## [87] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1983.csv\u0026quot; ## [88] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1984.csv\u0026quot; ## [89] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1985.csv\u0026quot; ## [90] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1986.csv\u0026quot; ## [91] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1987.csv\u0026quot; ## [92] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1988.csv\u0026quot; ## [93] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1989.csv\u0026quot; ## [94] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1990.csv\u0026quot; ## [95] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1991.csv\u0026quot; ## [96] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1992.csv\u0026quot; ## [97] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1993.csv\u0026quot; ## [98] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1994.csv\u0026quot; ## [99] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1995.csv\u0026quot; ## [100] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1996.csv\u0026quot; ## [101] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1997.csv\u0026quot; ## [102] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1998.csv\u0026quot; ## [103] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1999.csv\u0026quot; ## [104] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2000.csv\u0026quot; ## [105] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2001.csv\u0026quot; ## [106] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2002.csv\u0026quot; ## [107] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2003.csv\u0026quot; ## [108] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2004.csv\u0026quot; ## [109] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2005.csv\u0026quot; ## [110] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2006.csv\u0026quot; ## [111] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2007.csv\u0026quot; ## [112] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2008.csv\u0026quot; ## [113] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2009.csv\u0026quot; ## [114] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2010.csv\u0026quot; ## [115] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2011.csv\u0026quot; ## [116] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2012.csv\u0026quot; ## [117] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2013.csv\u0026quot; ## [118] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2014.csv\u0026quot; ## [119] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2015.csv\u0026quot; ## [120] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2016.csv\u0026quot; ## [121] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2017.csv\u0026quot; ## [122] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2018.csv\u0026quot; ## [123] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2019.csv\u0026quot; ## [124] \u0026quot;tennis_atp-master//atp_players.csv\u0026quot; ## [125] \u0026quot;tennis_atp-master//atp_rankings_00s.csv\u0026quot; ## [126] \u0026quot;tennis_atp-master//atp_rankings_10s.csv\u0026quot; ## [127] \u0026quot;tennis_atp-master//atp_rankings_70s.csv\u0026quot; ## [128] \u0026quot;tennis_atp-master//atp_rankings_80s.csv\u0026quot; ## [129] \u0026quot;tennis_atp-master//atp_rankings_90s.csv\u0026quot; ## [130] \u0026quot;tennis_atp-master//atp_rankings_current.csv\u0026quot; Again, same as with the other post, we see a lot of files. The names are quite suggesting and it is clear that not all files contains matches data. Let’s restrict the analysis just for the files with the string atp_matches. This includes main matches, qualifications and futures games.\nlibrary(stringr) # restrict just for main atp matches my.f \u0026lt;- my.f[str_detect(my.f, \u0026#39;matches\u0026#39;)] my.f ## [1] \u0026quot;tennis_atp-master//atp_matches_1968.csv\u0026quot; ## [2] \u0026quot;tennis_atp-master//atp_matches_1969.csv\u0026quot; ## [3] \u0026quot;tennis_atp-master//atp_matches_1970.csv\u0026quot; ## [4] \u0026quot;tennis_atp-master//atp_matches_1971.csv\u0026quot; ## [5] \u0026quot;tennis_atp-master//atp_matches_1972.csv\u0026quot; ## [6] \u0026quot;tennis_atp-master//atp_matches_1973.csv\u0026quot; ## [7] \u0026quot;tennis_atp-master//atp_matches_1974.csv\u0026quot; ## [8] \u0026quot;tennis_atp-master//atp_matches_1975.csv\u0026quot; ## [9] \u0026quot;tennis_atp-master//atp_matches_1976.csv\u0026quot; ## [10] \u0026quot;tennis_atp-master//atp_matches_1977.csv\u0026quot; ## [11] \u0026quot;tennis_atp-master//atp_matches_1978.csv\u0026quot; ## [12] \u0026quot;tennis_atp-master//atp_matches_1979.csv\u0026quot; ## [13] \u0026quot;tennis_atp-master//atp_matches_1980.csv\u0026quot; ## [14] \u0026quot;tennis_atp-master//atp_matches_1981.csv\u0026quot; ## [15] \u0026quot;tennis_atp-master//atp_matches_1982.csv\u0026quot; ## [16] \u0026quot;tennis_atp-master//atp_matches_1983.csv\u0026quot; ## [17] \u0026quot;tennis_atp-master//atp_matches_1984.csv\u0026quot; ## [18] \u0026quot;tennis_atp-master//atp_matches_1985.csv\u0026quot; ## [19] \u0026quot;tennis_atp-master//atp_matches_1986.csv\u0026quot; ## [20] \u0026quot;tennis_atp-master//atp_matches_1987.csv\u0026quot; ## [21] \u0026quot;tennis_atp-master//atp_matches_1988.csv\u0026quot; ## [22] \u0026quot;tennis_atp-master//atp_matches_1989.csv\u0026quot; ## [23] \u0026quot;tennis_atp-master//atp_matches_1990.csv\u0026quot; ## [24] \u0026quot;tennis_atp-master//atp_matches_1991.csv\u0026quot; ## [25] \u0026quot;tennis_atp-master//atp_matches_1992.csv\u0026quot; ## [26] \u0026quot;tennis_atp-master//atp_matches_1993.csv\u0026quot; ## [27] \u0026quot;tennis_atp-master//atp_matches_1994.csv\u0026quot; ## [28] \u0026quot;tennis_atp-master//atp_matches_1995.csv\u0026quot; ## [29] \u0026quot;tennis_atp-master//atp_matches_1996.csv\u0026quot; ## [30] \u0026quot;tennis_atp-master//atp_matches_1997.csv\u0026quot; ## [31] \u0026quot;tennis_atp-master//atp_matches_1998.csv\u0026quot; ## [32] \u0026quot;tennis_atp-master//atp_matches_1999.csv\u0026quot; ## [33] \u0026quot;tennis_atp-master//atp_matches_2000.csv\u0026quot; ## [34] \u0026quot;tennis_atp-master//atp_matches_2001.csv\u0026quot; ## [35] \u0026quot;tennis_atp-master//atp_matches_2002.csv\u0026quot; ## [36] \u0026quot;tennis_atp-master//atp_matches_2003.csv\u0026quot; ## [37] \u0026quot;tennis_atp-master//atp_matches_2004.csv\u0026quot; ## [38] \u0026quot;tennis_atp-master//atp_matches_2005.csv\u0026quot; ## [39] \u0026quot;tennis_atp-master//atp_matches_2006.csv\u0026quot; ## [40] \u0026quot;tennis_atp-master//atp_matches_2007.csv\u0026quot; ## [41] \u0026quot;tennis_atp-master//atp_matches_2008.csv\u0026quot; ## [42] \u0026quot;tennis_atp-master//atp_matches_2009.csv\u0026quot; ## [43] \u0026quot;tennis_atp-master//atp_matches_2010.csv\u0026quot; ## [44] \u0026quot;tennis_atp-master//atp_matches_2011.csv\u0026quot; ## [45] \u0026quot;tennis_atp-master//atp_matches_2012.csv\u0026quot; ## [46] \u0026quot;tennis_atp-master//atp_matches_2013.csv\u0026quot; ## [47] \u0026quot;tennis_atp-master//atp_matches_2014.csv\u0026quot; ## [48] \u0026quot;tennis_atp-master//atp_matches_2015.csv\u0026quot; ## [49] \u0026quot;tennis_atp-master//atp_matches_2016.csv\u0026quot; ## [50] \u0026quot;tennis_atp-master//atp_matches_2017.csv\u0026quot; ## [51] \u0026quot;tennis_atp-master//atp_matches_2018.csv\u0026quot; ## [52] \u0026quot;tennis_atp-master//atp_matches_2019.csv\u0026quot; ## [53] \u0026quot;tennis_atp-master//atp_matches_futures_1991.csv\u0026quot; ## [54] \u0026quot;tennis_atp-master//atp_matches_futures_1992.csv\u0026quot; ## [55] \u0026quot;tennis_atp-master//atp_matches_futures_1993.csv\u0026quot; ## [56] \u0026quot;tennis_atp-master//atp_matches_futures_1994.csv\u0026quot; ## [57] \u0026quot;tennis_atp-master//atp_matches_futures_1995.csv\u0026quot; ## [58] \u0026quot;tennis_atp-master//atp_matches_futures_1996.csv\u0026quot; ## [59] \u0026quot;tennis_atp-master//atp_matches_futures_1997.csv\u0026quot; ## [60] \u0026quot;tennis_atp-master//atp_matches_futures_1998.csv\u0026quot; ## [61] \u0026quot;tennis_atp-master//atp_matches_futures_1999.csv\u0026quot; ## [62] \u0026quot;tennis_atp-master//atp_matches_futures_2000.csv\u0026quot; ## [63] \u0026quot;tennis_atp-master//atp_matches_futures_2001.csv\u0026quot; ## [64] \u0026quot;tennis_atp-master//atp_matches_futures_2002.csv\u0026quot; ## [65] \u0026quot;tennis_atp-master//atp_matches_futures_2003.csv\u0026quot; ## [66] \u0026quot;tennis_atp-master//atp_matches_futures_2004.csv\u0026quot; ## [67] \u0026quot;tennis_atp-master//atp_matches_futures_2005.csv\u0026quot; ## [68] \u0026quot;tennis_atp-master//atp_matches_futures_2006.csv\u0026quot; ## [69] \u0026quot;tennis_atp-master//atp_matches_futures_2007.csv\u0026quot; ## [70] \u0026quot;tennis_atp-master//atp_matches_futures_2008.csv\u0026quot; ## [71] \u0026quot;tennis_atp-master//atp_matches_futures_2009.csv\u0026quot; ## [72] \u0026quot;tennis_atp-master//atp_matches_futures_2010.csv\u0026quot; ## [73] \u0026quot;tennis_atp-master//atp_matches_futures_2011.csv\u0026quot; ## [74] \u0026quot;tennis_atp-master//atp_matches_futures_2012.csv\u0026quot; ## [75] \u0026quot;tennis_atp-master//atp_matches_futures_2013.csv\u0026quot; ## [76] \u0026quot;tennis_atp-master//atp_matches_futures_2014.csv\u0026quot; ## [77] \u0026quot;tennis_atp-master//atp_matches_futures_2015.csv\u0026quot; ## [78] \u0026quot;tennis_atp-master//atp_matches_futures_2016.csv\u0026quot; ## [79] \u0026quot;tennis_atp-master//atp_matches_futures_2017.csv\u0026quot; ## [80] \u0026quot;tennis_atp-master//atp_matches_futures_2018.csv\u0026quot; ## [81] \u0026quot;tennis_atp-master//atp_matches_futures_2019.csv\u0026quot; ## [82] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1978.csv\u0026quot; ## [83] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1979.csv\u0026quot; ## [84] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1980.csv\u0026quot; ## [85] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1981.csv\u0026quot; ## [86] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1982.csv\u0026quot; ## [87] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1983.csv\u0026quot; ## [88] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1984.csv\u0026quot; ## [89] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1985.csv\u0026quot; ## [90] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1986.csv\u0026quot; ## [91] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1987.csv\u0026quot; ## [92] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1988.csv\u0026quot; ## [93] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1989.csv\u0026quot; ## [94] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1990.csv\u0026quot; ## [95] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1991.csv\u0026quot; ## [96] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1992.csv\u0026quot; ## [97] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1993.csv\u0026quot; ## [98] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1994.csv\u0026quot; ## [99] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1995.csv\u0026quot; ## [100] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1996.csv\u0026quot; ## [101] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1997.csv\u0026quot; ## [102] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1998.csv\u0026quot; ## [103] \u0026quot;tennis_atp-master//atp_matches_qual_chall_1999.csv\u0026quot; ## [104] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2000.csv\u0026quot; ## [105] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2001.csv\u0026quot; ## [106] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2002.csv\u0026quot; ## [107] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2003.csv\u0026quot; ## [108] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2004.csv\u0026quot; ## [109] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2005.csv\u0026quot; ## [110] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2006.csv\u0026quot; ## [111] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2007.csv\u0026quot; ## [112] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2008.csv\u0026quot; ## [113] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2009.csv\u0026quot; ## [114] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2010.csv\u0026quot; ## [115] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2011.csv\u0026quot; ## [116] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2012.csv\u0026quot; ## [117] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2013.csv\u0026quot; ## [118] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2014.csv\u0026quot; ## [119] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2015.csv\u0026quot; ## [120] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2016.csv\u0026quot; ## [121] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2017.csv\u0026quot; ## [122] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2018.csv\u0026quot; ## [123] \u0026quot;tennis_atp-master//atp_matches_qual_chall_2019.csv\u0026quot; As you can see, I used string matching function str_detect from stringr to find and keep just the csv files with the string matches in its name. Now, let’s load all files into a single dataframe.\nlibrary(readr) library(dplyr) # set cols (missing some) my.cols \u0026lt;- cols( .default = col_integer(), tourney_id = col_character(), tourney_name = col_character(), surface = col_character(), tourney_level = col_character(), winner_entry = col_character(), winner_name = col_character(), winner_hand = col_character(), winner_ioc = col_character(), winner_age = col_double(), loser_entry = col_character(), loser_name = col_character(), loser_hand = col_character(), loser_ioc = col_character(), loser_age = col_double(), score = col_character(), round = col_character() ) # load all files with lapply and do.call (some cols don\u0026#39;t match in all files) df.matches \u0026lt;- do.call(bind_rows,lapply(my.f, read_csv, col_types = my.cols)) # create year column df.matches$Date \u0026lt;- as.Date(as.character(df.matches$tourney_date),\u0026#39;%Y%m%d\u0026#39;) df.matches$Year \u0026lt;- format(df.matches$Date,\u0026#39;%Y\u0026#39;) Let’s see what the data offers.\nstr(df.matches) ## tibble [752,552 × 51] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ tourney_id : chr [1:752552] \u0026quot;1968-580\u0026quot; \u0026quot;1968-580\u0026quot; \u0026quot;1968-580\u0026quot; \u0026quot;1968-580\u0026quot; ... ## $ tourney_name : chr [1:752552] \u0026quot;Australian Chps.\u0026quot; \u0026quot;Australian Chps.\u0026quot; \u0026quot;Australian Chps.\u0026quot; \u0026quot;Australian Chps.\u0026quot; ... ## $ surface : chr [1:752552] \u0026quot;Grass\u0026quot; \u0026quot;Grass\u0026quot; \u0026quot;Grass\u0026quot; \u0026quot;Grass\u0026quot; ... ## $ draw_size : int [1:752552] 64 64 64 64 64 64 64 64 64 64 ... ## $ tourney_level : chr [1:752552] \u0026quot;G\u0026quot; \u0026quot;G\u0026quot; \u0026quot;G\u0026quot; \u0026quot;G\u0026quot; ... ## $ tourney_date : int [1:752552] 19680119 19680119 19680119 19680119 19680119 19680119 19680119 19680119 19680119 19680119 ... ## $ match_num : int [1:752552] 1 2 3 4 5 6 7 8 9 10 ... ## $ winner_id : int [1:752552] 110023 109803 100257 100105 109966 107759 100101 100025 108519 109799 ... ## $ winner_seed : int [1:752552] NA NA NA 5 NA NA 12 3 NA NA ... ## $ winner_entry : chr [1:752552] NA NA NA NA ... ## $ winner_name : chr [1:752552] \u0026quot;Richard Coulthard\u0026quot; \u0026quot;John Brown\u0026quot; \u0026quot;Ross Case\u0026quot; \u0026quot;Allan Stone\u0026quot; ... ## $ winner_hand : chr [1:752552] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; ... ## $ winner_ht : int [1:752552] NA NA NA NA NA NA NA 173 NA NA ... ## $ winner_ioc : chr [1:752552] \u0026quot;AUS\u0026quot; \u0026quot;AUS\u0026quot; \u0026quot;AUS\u0026quot; \u0026quot;AUS\u0026quot; ... ## $ winner_age : num [1:752552] NA 27.5 16.2 22.3 29.9 ... ## $ winner_rank : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ winner_rank_points: int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ loser_id : int [1:752552] 107760 106964 110024 110025 110026 110027 110028 108430 110029 110030 ... ## $ loser_seed : int [1:752552] NA NA 15 NA NA NA NA NA NA NA ... ## $ loser_entry : chr [1:752552] NA NA NA NA ... ## $ loser_name : chr [1:752552] \u0026quot;Max Senior\u0026quot; \u0026quot;Ernie Mccabe\u0026quot; \u0026quot;Gondo Widjojo\u0026quot; \u0026quot;Robert Layton\u0026quot; ... ## $ loser_hand : chr [1:752552] \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; \u0026quot;R\u0026quot; ... ## $ loser_ht : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ loser_ioc : chr [1:752552] \u0026quot;AUS\u0026quot; \u0026quot;AUS\u0026quot; \u0026quot;INA\u0026quot; \u0026quot;AUS\u0026quot; ... ## $ loser_age : num [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ loser_rank : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ loser_rank_points : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ score : chr [1:752552] \u0026quot;12-10 7-5 4-6 7-5\u0026quot; \u0026quot;6-3 6-2 6-4\u0026quot; \u0026quot;6-4 3-6 6-3 7-5\u0026quot; \u0026quot;6-4 6-2 6-1\u0026quot; ... ## $ best_of : int [1:752552] 5 5 5 5 5 5 5 5 5 5 ... ## $ round : chr [1:752552] \u0026quot;R64\u0026quot; \u0026quot;R64\u0026quot; \u0026quot;R64\u0026quot; \u0026quot;R64\u0026quot; ... ## $ minutes : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ w_ace : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ w_df : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ w_svpt : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ w_1stIn : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ w_1stWon : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ w_2ndWon : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ w_SvGms : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ w_bpSaved : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ w_bpFaced : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ l_ace : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ l_df : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ l_svpt : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ l_1stIn : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ l_1stWon : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ l_2ndWon : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ l_SvGms : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ l_bpSaved : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ l_bpFaced : int [1:752552] NA NA NA NA NA NA NA NA NA NA ... ## $ Date : Date[1:752552], format: \u0026quot;1968-01-19\u0026quot; \u0026quot;1968-01-19\u0026quot; ... ## $ Year : chr [1:752552] \u0026quot;1968\u0026quot; \u0026quot;1968\u0026quot; \u0026quot;1968\u0026quot; \u0026quot;1968\u0026quot; ... Again, lots of information. Looks like the games are separated by rows, where the columns have a lot of information about the matches. This dataset is not as detailed as the last one with point by point data, but still quite impressive.\n Looking at top players winning percentages For our first analysis, lets look at the winning percentages of the top 10 players from the ATP rankings. For that, I will use package rvest to scrape the information about the top ATP players from the ATP website.\nlibrary(rvest) ## Loading required package: xml2 ## ## Attaching package: \u0026#39;rvest\u0026#39; ## The following object is masked from \u0026#39;package:readr\u0026#39;: ## ## guess_encoding library(stringr) n.players \u0026lt;- 10 my.url \u0026lt;- \u0026#39;http://www.atpworldtour.com/en/rankings/singles\u0026#39; atp.players \u0026lt;- read_html(my.url) %\u0026gt;% html_nodes(\u0026quot;.player-cell\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_replace_all(\u0026#39;\\\\n\u0026#39;,\u0026#39;\u0026#39;) %\u0026gt;% str_trim() name.player \u0026lt;- atp.players[1:n.players] print(name.player) ## [1] \u0026quot;Novak Djokovic\u0026quot; \u0026quot;Rafael Nadal\u0026quot; \u0026quot;Dominic Thiem\u0026quot; ## [4] \u0026quot;Roger Federer\u0026quot; \u0026quot;Daniil Medvedev\u0026quot; \u0026quot;Stefanos Tsitsipas\u0026quot; ## [7] \u0026quot;Alexander Zverev\u0026quot; \u0026quot;Matteo Berrettini\u0026quot; \u0026quot;Gael Monfils\u0026quot; ## [10] \u0026quot;David Goffin\u0026quot; As you can see, we have to usual suspects. Novak Djokovic is now leading in the first position, which was held previously by Djokovic for a long period of time. I really like to see Dominic Thiem in the top ten as he is the youngest with 23 year, a solid game, and lots of potential. Federer is the oldest of the list, with 35 years old, but the most talented. It is amazing how he can still stay competitive given the age difference.\nBack to the R analysis, a problem with this approach of scraping the names from the ATP website is that they don’t necessarily match the names in the ATP files. As an example, the swiss player Stan Wawrinka is named Stanislas Wawrinka in the csv files, while in the ATP website is Stan Wawrinka. This means that directly trying to match the names in name.player with the names in df.matches may not work. This is classic case of data from difference sources that don’t share the same unique identifiers.\nI usually solve this problem using a lookup table built manually in a csv file, where I store the matching identifiers in the different datasets. If the number of unique cases is small, it is quite doable. For this analysis, however, we have 13491 players, which is definitely too much. Manually creating a lookup table would be very demanding.\nThe solution I often in this case is to make the computer look for the best imperfect match among the different identifiers (names). This is also known as calculating string distances - the number of require edits so that a string matches other, and can be accomplished in many different ways. In this example I’ll use function amatch from package stringdist, with the basic options.\nlibrary(stringdist) # get unique names from losers and winners unique.players \u0026lt;- unique(c(df.matches$winner_name, df.matches$loser_name )) # find the index of the closest string for each name in name.player idx \u0026lt;- amatch(name.player, unique.players, maxDist = 5) Lets look at the result:\nprint(data.frame(name.from.web = name.player, name.from.atp = unique.players[idx] )) ## name.from.web name.from.atp ## 1 Novak Djokovic Novak Djokovic ## 2 Rafael Nadal Rafael Nadal ## 3 Dominic Thiem Dominic Thiem ## 4 Roger Federer Roger Federer ## 5 Daniil Medvedev Daniil Medvedev ## 6 Stefanos Tsitsipas Stefanos Tsitsipas ## 7 Alexander Zverev Alexander Zverev ## 8 Matteo Berrettini Matteo Berrettini ## 9 Gael Monfils Gael Monfils ## 10 David Goffin David Goffin Beautiful! For our case of just 10 players, it worked perfectly. The name of Stan Wawrinka_ was matched correctly. Be aware, however, that this procedure of string matching is not guaranteed to work 100% of the cases. You could also set NA values using the options in amatch but it is still not guaranteed to work well. You should always manually check the result.\nGoing forward, let’s copy the players name from one to the other and use a loop and dplyr to calculate yearly winning percentages of each player, also separating by the type of surface. Using a loop here might look odd, but this was the simplest solution I’ve found as you cannot use dplyr directly. The dataframe df.matches is unique in the sense that it has two informations in each row, you have a win for someone and a loss to another player (see columns winner_name and loser_name). If I use dplyr directly, I would miss the games where two players in the list played each other. There is probably a way to adjust the dataframe, but I just find it easier to loop over the players.\n# copy players name name.player \u0026lt;- unique.players[idx] # create table with players my.tab \u0026lt;- tibble() for (i.player in name.player) { temp \u0026lt;- filter(df.matches, (winner_name==i.player)|(loser_name==i.player)) temp$Name \u0026lt;- i.player temp.tab \u0026lt;- temp %\u0026gt;% group_by(Name, Year,surface) %\u0026gt;% summarise(`Percentage of wins` = sum(winner_name == Name)/n(), `Number of matches` = n()) %\u0026gt;% filter(`Number of matches` \u0026gt; 0 ) %\u0026gt;% filter(surface %in% c(\u0026#39;Hard\u0026#39;,\u0026#39;Clay\u0026#39;)) my.tab \u0026lt;- bind_rows(my.tab, temp.tab) } # use atp ranking in names e.g. Andy Murray (1) my.tab$Name \u0026lt;- paste0(my.tab$Name, \u0026#39; (\u0026#39;, match(my.tab$Name, name.player), \u0026#39;)\u0026#39; ) my.tab \u0026lt;- my.tab[complete.cases(my.tab), ] Lets plot it!\nlibrary(ggplot2) p \u0026lt;- ggplot(my.tab, aes(x = as.numeric(Year), y = `Percentage of wins`, color=surface)) p \u0026lt;- p + geom_point() + geom_smooth() p \u0026lt;- p + facet_grid(surface~Name) p \u0026lt;- p + ylim(c(0.25,1)) + xlim(c(2005,2016)) print(p) We can see a few patterns. First, the winning consistency of top ten player is higher in hard courts. For example, look at Murray and Djokovic. It is easier for them to keep the same proportion of wins in hard courts than it is in clay. My best guess is that hard courts are more predictable and the ball is faster and bounces lower, making it easier for top players to hit winners and other point-finishing strokes such as drop shots. Be aware that this could also be due to the different number of matches in each court. We could test the difference of volatility statistically but I rather keep the analysis simple and visual.\nAlso, not surprisingly, Nadal, the king of clay, presents the highest winning percentage in clay, before 2012. His game strategy uses strength and heavy top spin balls that are difficult to counter. See here for a more detailed analysis on why he is so dominant on clay. The bad side of his strategy and type of play is that it is based heavily on his muscular strength. As his age increases, it is difficult to maintain the same level. This explains the increased proportion of defeats after 2012.\nThe effect of age is also perceived for Roger Federer. But, given that his style of play is not based just on strength, I think that he can still beat a lot of players. Just look at the his recent title win at the 2017 Australian open.\nNow, look how Dojokovic is consistent in clay and hard courts. He is definitely the player with the best stats. I predict that, if he works for it, he will have no problem in taking the #1 spot once again from Murray.\n Looking at Brazilian tennis players In this section we will analyse the stats of Brazilian tennis players in ATP rankings. The following code can be easily executed for any country as the data is scraped from the ATP website. Just change the object my.country for your case.\nmy.country \u0026lt;- \u0026#39;BRA\u0026#39; n.players \u0026lt;- 5 my.url \u0026lt;- paste0(\u0026#39;http://www.atpworldtour.com/en/rankings/singles?rankDate=2017-02-06\u0026amp;rankRange=0-500\u0026amp;countryCode=\u0026#39;,my.country) atp.players \u0026lt;- read_html(my.url) %\u0026gt;% html_nodes(\u0026quot;.player-cell\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_replace_all(\u0026#39;\\\\n\u0026#39;,\u0026#39;\u0026#39;) %\u0026gt;% str_trim() name.player \u0026lt;- atp.players[1:n.players] name.player ## [1] \u0026quot;Thiago Monteiro\u0026quot; \u0026quot;Rogerio Dutra Silva\u0026quot; \u0026quot;Thomaz Bellucci\u0026quot; ## [4] \u0026quot;Joao Souza\u0026quot; \u0026quot;Andre Ghem\u0026quot; The top five Brazilian players in ATP are Thiago Monteiro, Rogerio Dutra Silva, Thomas Belluci, João Souza and Andre Ghem, in that order. We should be proud of all of them. I can only imagine the dedication and effort necessary to be a professional tennis player. You can see more details about each here.\nAt this point, it makes sense to create a function to analyse the dataframe, as we will use it again later.\nplot.winning.perc \u0026lt;- function(name.player, df.matches, min.games.per.year = 0, min.year = 2005, max.year = 2016){ require(stringdist) require(stringr) require(dplyr) require(ggplot2) # create year column df.matches$Date \u0026lt;- as.Date(as.character(df.matches$tourney_date),\u0026#39;%Y%m%d\u0026#39;) df.matches$Year \u0026lt;- format(df.matches$Date,\u0026#39;%Y\u0026#39;) # get unique names from losers and winners unique.players \u0026lt;- unique(c(df.matches$winner_name, df.matches$loser_name )) # find the index of the closest string for each name in name.player idx \u0026lt;- amatch(name.player, unique.players, maxDist = 5) #print(data.frame(name.from.web = name.player, # name.from.atp = unique.players[idx] )) my.tab \u0026lt;- tibble() for (i.player in name.player) { temp \u0026lt;- filter(df.matches, (winner_name==i.player)|(loser_name==i.player)) temp$Name \u0026lt;- i.player temp.tab \u0026lt;- temp %\u0026gt;% group_by(Name, Year,surface) %\u0026gt;% summarise(`Percentage of wins` = sum(winner_name == Name)/n(), `Number of matches` = n()) %\u0026gt;% filter(`Number of matches` \u0026gt; min.games.per.year ) %\u0026gt;% filter(surface %in% c(\u0026#39;Hard\u0026#39;,\u0026#39;Clay\u0026#39;)) my.tab \u0026lt;- bind_rows(my.tab, temp.tab) } p \u0026lt;- ggplot(my.tab, aes(x = as.numeric(Year), y = `Percentage of wins`, color=surface)) p \u0026lt;- p + geom_point() + geom_smooth() p \u0026lt;- p + facet_grid(surface~Name) p \u0026lt;- p + ylim(c(0.25,1)) + xlim(c(min.year,max.year)) return(p) } Now, we use the previous function for the new names of players.\n# plot winnign percentages p \u0026lt;- plot.winning.perc(name.player, df.matches) print(p) From this list, the youngest Brazilian player, and with most potential in my opinion, is Thiago Monteiro. He is 22 and has lots of time to improve his game. In 2016 he beated Tsonga in a great match here in Brazil.\nThe records of Guga (Gustavo Kurten) For last, lets look at stats of the the best and most cherished singles player from Brazil, Gustavo Kuerten - Guga. Guga is internationally famous and reached #1 in the ATP ranking in 2000. A very charismatic player, with a fantastic backhand. It is not far fetched to say that Guga brought Tennis to Brazil, making it a more popular sport.\nGuga had a hip injury back in 2001. He tried surgery, but it didn’t worked out and he wasn’t able to maintain the higher level of tennis that is necessary in the professional circuit. He retired in february of 2008. His story is very inspiring, growing up in the state of Santa Catarina, in a time where tennis was not a popular sport in Brazil and funding opportunities were scarce. His biography (in portuguese) is an excellent read.\nLet’s have a look in the data.\nname.player \u0026lt;- \u0026#39;Gustavo Kuerten\u0026#39; p \u0026lt;- plot.winning.perc(name.player, df.matches, min.year = 1990) print(p) As expected, a winning journey until 2001, when losses start to accumulate.\n Conlusion The data repository of Jeff Sackmann is great. I really only scratched the surface with some simple graphical analysis. I think there is a lot of space for modeling as well, that is, creating a model that can predict the winning records of a player. I’m thinking about an econometric model using historical statistics of players such as past winning records, forehand/backhand winners, and so on. But I’m sure you can use other approaches as well.\nFrom the computational side, it would be interesting to create a CRAN package to analyse this dataset. Just like in this post, all data could be fetched from the ATP website and the repository of Jeff. One could check the winning records of players with just a simple function call. I just checked and there is nothing like it in CRAN. A shiny app would also be interesting. I’ll keep it in mind for the future.\nI hope you liked this post. Any comments or suggestions, please share in the comments section.\n  ","date":1486944000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1486944000,"objectID":"6b8b2b11b5003222f0cc2a26d06089aa","permalink":"http://www.msperlin.com/blog/post/2017-02-13-r-and-tennis-players/","publishdate":"2017-02-13T00:00:00Z","relpermalink":"/blog/post/2017-02-13-r-and-tennis-players/","section":"post","summary":"In the previous post about tennis, we studied how changes in ball’s composition in hard and grass courts affected the game back in 2000. In this post, we will analyse a different dataset from the same repository and look at the players winning records in ATP matches.\nThe data I’m again using the great repository of tennis data of Jeff Sackmann. In this case, however, I’m using the ATP repository that contains ATP match data since 1968 until today. Again, I thank Jeff Sackmann for making this dataset publicly available.","tags":["R","tennis"],"title":"Using R to study tennis players","type":"post"},{"authors":null,"categories":["R","exams","RndTexExams"],"content":" Part of my job as a researcher and teacher is to periodically apply and grade exams in my classroom. Being constantly in the shoes of an examiner, you soon quickly realize that students are clever in finding ways to do well in an exam without effort. These days, photos and pdf versions of past exams and exercises are shared online in facebook, whatsapp groups, instagram and what not. As weird as it may sound, the distribution of information in the digital era creates a problem for examiners. If you use the same exam from past year, it is likely that students will simply memorize the answers from a digital record. Moreover, some students will also cheat by looking for answers during the test. Either way, keeping the same exam over time and across students, is not advisable.\nThis issue really bothered me. For large classes, there isn’t a way to evaluate the work of students as cost effective as online or printed exams. I’m strongly in favor of meritocracy in academia and I think that a grade in an exam should, on average, be good indicator of the knowledge that the students retained during coursework. Otherwise, what’s the point of doing all of it?\nIn the past, I manually created different versions of questions and wrote new ones in order to avoid cheating and memorization of questions. But, year after year, it became clear to me that this was a time consuming task that took more energy than what I would like to invest. Besides teaching, I also do research and work on administrative issues within my department. Sometimes, specially around deadlines, you simply don’t have the time and mental energy to come up with different versions of an existing exam.\nBack in 2016 I decided to invest some to time to automatize this process and try to come up with an elegant solution. Since I had all my exams in a latex template called examdesign, I wrote package RndTexExams that took as input a .tex file and created n versions of exams by randomly defining the order of questions, the answer list and textual content based on a simple markup language. If you know latex, it is basically a problem of finding regex patterns and restructuring a character object that is later saved in a new and compilable latex file.\nThe package I wrote worked pretty well for me but, as with any first version of a software, it had missing features. The output was only a pdf file based on a template, it did not work with standard academic platforms such as Blackboard and Moodle and, the most problematic in my opinion, it was not designed to run embedded R code that could be parsed by knitr, like in a Rmarkdown file.\nThis is when I tried out the package exams. While my solution with RndTexExams was alright for a latex user, package exams is much better at solving the problem of dynamic content in exams. Using the knitr and sweave engines, the level of randomization and creation of dynamic content is really amazing. By combining R code (and all the capabilities of CRAN packages), you can do do anything your want in an exam. You can get information on the web, use completely different datasets for each exam and so on. The limit is set by your imagination.\nAn example of exam with dynamic content As a quick example, I am going to show one question from the exercise chapter of my book. When it is ready, I will be serving the exercises with a web based shiny app, meaning that the reader will download a pdf file with unique questions that is processed in a shiny server.\nIn this example questions, I’m asking the reader to use R to solve the following problem:\nHow many packages you can find today (2020-04-28) in CRAN? Use repository https://cloud.r-project.org/ for the solution. The solution is pretty simple, all you need to do is to ask for the number of rows for the object output from a call to available.packages(). The reader can find the solution with the command nrow(available.packages(repos='https://cloud.r-project.org/')).\nNow, lets build the content of this simple question in a separate file. You can either use .Rnw or .Rmd files with exam. I will choose the later just to keep it simple. Here are the contents of a file called Question.Rmd, available here.\nmy.q.file \u0026lt;- \u0026#39;~/Dropbox/11-My Website/www.msperlin.com-blog/static/files/Question.Rmd\u0026#39; cat(paste0(readLines(my.q.file), collapse = \u0026#39;\\n\u0026#39;)) ## ```{r data generation, echo = FALSE, results = \u0026quot;hide\u0026quot;} ## #possible.repo \u0026lt;- getCRANmirrors()$URL # doenst work well for all repos ## ## possible.repo \u0026lt;- c(\u0026#39;https://cloud.r-project.org/\u0026#39;, ## \u0026#39;http://mirror.fcaglp.unlp.edu.ar/CRAN/\u0026#39;, ## \u0026#39;http://cran-r.c3sl.ufpr.br/\u0026#39;, ## \u0026#39;http://cran.stat.sfu.ca/\u0026#39;, ## \u0026#39;https://mirrors.dotsrc.org/cran/\u0026#39;, ## \u0026#39;https://mirrors.cicku.me/CRAN/\u0026#39;, ## \u0026#39;https://cran.ism.ac.jp/\u0026#39;) ## ## my.repo \u0026lt;- sample(possible.repo,1) ## ## n.pkgs \u0026lt;- nrow(available.packages(repos = my.repo)) ## ## sol.q \u0026lt;- n.pkgs ## rnd.vec \u0026lt;- c(0, sample(-5000:-1,4)) ## ## my.answers \u0026lt;- paste0(sol.q+rnd.vec, \u0026#39; packages\u0026#39;) ## ``` ## ## Question ## ======== ## ## How many packages you can find today (`r Sys.Date()`) in CRAN? ## ## Use repository `r my.repo` for the solution. ## ## ```{r questionlist, echo = FALSE, results = \u0026quot;asis\u0026quot;} ## exams::answerlist(my.answers, markup = \u0026quot;markdown\u0026quot;) ## ``` ## ## Meta-information ## ================ ## extype: schoice ## exsolution: 10000 ## exname: numbero of cran pkgs ## exshuffle: TRUE For the last piece of code, notice that I’ve set the solution of the question in object sol.q. Later, in object my.answers, I use it together with a random vector of integers to create five alternative answers to the questions, where the first one is the correct. This operation results in the following objects:\nmy.repo \u0026lt;- \u0026#39;https://cloud.r-project.org/\u0026#39; n.pkgs \u0026lt;- nrow(available.packages(repos = my.repo)) sol.q \u0026lt;- n.pkgs rnd.vec \u0026lt;- c(0, sample(-5000:-1,4)) my.answers \u0026lt;- paste0(sol.q+rnd.vec, \u0026#39; packages\u0026#39;) print(my.answers) ## [1] \u0026quot;15557 packages\u0026quot; \u0026quot;12845 packages\u0026quot; \u0026quot;13153 packages\u0026quot; \u0026quot;13913 packages\u0026quot; ## [5] \u0026quot;13148 packages\u0026quot; To conclude the question, I simply use Sys.Date() to get the system’s date and later set the correct answers using function answerlist. Some metadata is also inserted at the last section of Question.Rmd. The line exshuffle: TRUE sets a random order of possible answers in each exam for this questions. Do notice that the solution is registered in line exsolution: 10000, where the 1 in 10000 means correct answer in the first element of my.answers and the 0s represent incorrect answers.\nNow that the file with content of the question is finished, let’s set some options and build the exam with exams. For simplicity, we will repeate the same question five times.\nlibrary(exams) my.f \u0026lt;- my.q.file n.ver \u0026lt;- 1 name.exam \u0026lt;- \u0026#39;exam_sample\u0026#39; my.dir \u0026lt;- paste0(\u0026#39;exam-out/\u0026#39;) my.exam \u0026lt;- exams2pdf(file = rep(my.f,5), n = n.ver, dir = my.dir, name = name.exam, verbose = TRUE) ## Exams generation initialized. ## ## Output directory: /home/msperlin/Desktop/testing/content/post/exam-out ## Exercise directory: /home/msperlin/Desktop/testing/content/post ## Supplement directory: /tmp/RtmpJtbGrM/file9e855ddbb636 ## Temporary directory: /tmp/RtmpJtbGrM/file9e85555c5ce9 ## Exercises: ~/Dropbox/11-My Website/www.msperlin.com-blog/static/files/Question, ~/Dropbox/11-My Website/www.msperlin.com-blog/static/files/Question, ~/Dropbox/11-My Website/www.msperlin.com-blog/static/files/Question, ~/Dropbox/11-My Website/www.msperlin.com-blog/static/files/Question, ~/Dropbox/11-My Website/www.msperlin.com-blog/static/files/Question ## ## Generation of individual exams. ## Exam 1: _Dropbox_11-My Website_www.msperlin.com-blog_static_files_Question (srt) _Dropbox_11-My Website_www.msperlin.com-blog_static_files_Question_1 (srt) _Dropbox_11-My Website_www.msperlin.com-blog_static_files_Question_2 (srt) _Dropbox_11-My Website_www.msperlin.com-blog_static_files_Question_3 (srt) _Dropbox_11-My Website_www.msperlin.com-blog_static_files_Question_4 (srt) ... w ## Loading required namespace: tinytex ## ... done. f.out \u0026lt;- paste0(my.dir,name.exam,\u0026#39;1\u0026#39;,\u0026#39;.pdf\u0026#39;) file.exists(f.out) ## [1] TRUE The result of the previous code is a pdf file pdf file with the exam content.\nOne interesting information from this post is that you can find a small difference in the number of packages in between the CRAN mirrors. My best guess is that they synchronize with the master server in different times of the day/week.\nLooking at the contents of the pdf file, clearly some things are missing from the exam, such as the title page and the instructions. You can add all the bells and whistles with the inputs of function exams2pdf or change it directly in the different file templates. One quick tip for new users is that the answer sheet can be found by looping over the values of the output from exams2pdf:\ndf.answer.key \u0026lt;- data.frame() n.q \u0026lt;- 5 # number of questions for (i.ver in seq(n.ver)){ exam.now \u0026lt;- my.exam[[i.ver]] for (i.q in seq(n.q)){ sol.now \u0026lt;- letters[which(exam.now[[i.q]]$metainfo$solution)] temp \u0026lt;- data.frame(i.ver = i.ver, i.q = i.q, solution = sol.now) df.answer.key \u0026lt;- rbind(df.answer.key, temp) } } df.answer.key.wide \u0026lt;- tidyr::spread(df.answer.key, key = i.q, value = solution) df.answer.key.wide ## i.ver 1 2 3 4 5 ## 1 1 a a a a a By using package exams2pdf, I can code different questions in the exams format and not worry whether someone is going to copy it over and distribute it in the internet. Students may know the content of each question, but they will have to learn how to get to the correct answer in order to solve it for their exam. Cheating is also impossible since each student will have different versions and different answer sheets. If I have a class of 100 students, I will build 100 different exams, each one with unique answers.\nAs for maintainability, the time value of my exam questions increases significantly. I can use them over and over, now that I can effortlessly create as many versions of it as I need. Since it is all based in R code, I can use the code from the class material in my exams. Going further, I can also automatically grade the exams using the internet (see the vignette of RndTexExams for information on how to do that with Google spreadsheets.)\nIn this post I only scratched the surface of exams. Adding to the description of its capabilities, you can export exams to standard academic systems such as Moodle, Blackboard and others. You can also print the exam in pdf, nops (a pdf that allows easy scanning), or html. If you know a bit of latex or html, it is easy to customize the templates to the needs of your particular exam.\nAs with all technical things, not everything is perfect. In my oppinio, the main issue with the exams template is that requires some knowledge of R and Knitr. While this is Ok for most people reading this blog, it is not the case for the average professor. It may sound surprising to the quantitative inclined people, but the great majority of professors still use .docx and .xlsx files to write academic work such as articles and exams. Why they don’t use or learn better tools? Well, this is a long answer, best suited for another post.\nPackage exams had a big and positive impact on how I do my work. Based on a large database of questions that I’ve built, I can create a new exam in 5 minutes and grade it for a large class in less than 1 minute. I am very thankful to its authors and this is one of the reasons why I love posting packages in CRAN. It is my way of giving it back to the community.\nConcluding, package exams is great and I believe that every examiner and professor should be using it. Thinking about the future, the template of questions in exams has the potential of setting the language of exams, a structure that could allow the user to output questions in any format he wants, just as you can use Markdown to output latex or word.\nSharing questions in a collaborative platform, such as Quora, should be something for the developers (or R community) to think of. Questions could be ranked according to popular vote. Users could contribute by posting question files for other to use. Users would get a feedback on their work and, at the same time, be able to use other people questions. Students could also have access to it and independently study to a particular topic, by building custom made exams with randomized content.\nSumming up, if you are a teacher or examiner, I hope that this post convinces you to try out package exams.\n ","date":1485734400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485734400,"objectID":"c3dce7b1323ea1709bcfec7bc1e1aad5","permalink":"http://www.msperlin.com/blog/post/2017-01-30-exams-with-dynamic-content/","publishdate":"2017-01-30T00:00:00Z","relpermalink":"/blog/post/2017-01-30-exams-with-dynamic-content/","section":"post","summary":"Part of my job as a researcher and teacher is to periodically apply and grade exams in my classroom. Being constantly in the shoes of an examiner, you soon quickly realize that students are clever in finding ways to do well in an exam without effort. These days, photos and pdf versions of past exams and exercises are shared online in facebook, whatsapp groups, instagram and what not. As weird as it may sound, the distribution of information in the digital era creates a problem for examiners. If you use the same exam from past year, it is likely that students will simply memorize the answers from a digital record.","tags":["R","exams","RndTexExams"],"title":"Building and maintaining exams with dynamic content","type":"post"},{"authors":null,"categories":["R","stock market","beta","linear regression"],"content":" One of the first examples about using linear regression models in finance is the calculation of betas, the so called market model. Coefficient beta is a measure of systematic risk and it is calculated by estimating a linear model where the dependent variable is the return vector of a stock and the explanatory variable is the return vector of a diversified local market index, such as SP500 (US), FTSE (UK), Ibovespa (Brazil), or any other.\nFrom the academic side, the calculation of beta is part of a famous asset pricing model, CAPM - Capital Asset Pricing Model, that relates expected return and systematic risk. One can reach the market model equation by assuming several conditions such as Normal distributed returns, rational investors and frictionless market. Summing up, the CAPM model predicts that betas have a linear relationship to expected returns, that is, stocks with higher betas should present, collectively, higher average of historical returns.\nIn the quantitative side, we can formulate the market model as:\n\\(R_t = \\alpha + \\beta R_{M,t} + \\epsilon _t\\)\nwhere \\(R_{t}\\) is the return of the stock at time \\(t\\), \\(R_{M,t}\\) is the return of the market index, \\(\\alpha\\) is the constant (also called Jensen’s alphas) and, finally, \\(\\beta\\) is the measure of systematic risk for the stock. The values of \\(\\alpha\\) and \\(\\beta\\) are found by minimizing the sum of squared errors of the model. So, if you have a vector of prices for a stock and another vector of prices for the local market index, you can easily find the stock’s beta by calculating the daily returns and estimating the market model by OLS.\nThe problem here is that, usually, you don’t want the beta of a single stock. You want to calculate the systematic risk for a large number of stocks. This is where students usually have problems, as they only learned in class how to estimate one model. In order to do the same procedure for more than one stock, some programming is needed. This is where R really shines in comparison to simpler programs such as Excel.\nIn this post I will download some data from the US market, make some adjustments to the resulting dataframe and discuss three ways to calculate the betas of several stocks. These are:\nUsing a loop Using function by Using package dplyr  But first, lets load the data.\nLoading the data and preparing it I’m a bit biased, but I really like using package BatchGetSymbols to download financial data from yahoo finance. In this example we will download data for 10 stocks selected randomly from the SP500 index. I will also add the ticker ^GSPC, which belongs to the SP500 index. We will need it to calculate the betas. In order for the code to be reproducible, I will set random.seed(100). This means that anyone that runs the code available here will get the exact same results.\nlibrary(BatchGetSymbols) ## Loading required package: rvest ## Loading required package: xml2 ## Loading required package: dplyr ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union ##  set.seed(100) ticker.MktIdx \u0026lt;- \u0026#39;^GSPC\u0026#39; first.date \u0026lt;- as.Date(\u0026#39;2015-01-01\u0026#39;) last.date \u0026lt;- as.Date(\u0026#39;2019-01-01\u0026#39;) n.chosen.stocks \u0026lt;- 10 # can\u0026#39;t be higher than 505 # get random stocks my.tickers \u0026lt;- c(sample(GetSP500Stocks()$Tickers,n.chosen.stocks), ticker.MktIdx) l.out \u0026lt;- BatchGetSymbols(tickers = my.tickers, first.date = first.date, last.date = last.date) ## ## Running BatchGetSymbols for: ## ## tickers =FTV, ZBH, OXY, C, XLNX, VZ, BEN, WY, ROL, VTR, ^GSPC ## Downloading data for benchmark ticker ## ^GSPC | yahoo (1|1) | Found cache file ## FTV | yahoo (1|11) | Found cache file - Got 62% of valid prices | OUT: not enough data (thresh.bad.data = 75%) ## ZBH | yahoo (2|11) | Found cache file - Got 100% of valid prices | Youre doing good! ## OXY | yahoo (3|11) | Found cache file - Got 100% of valid prices | Good stuff! ## C | yahoo (4|11) | Found cache file - Got 100% of valid prices | Looking good! ## XLNX | yahoo (5|11) | Found cache file - Got 100% of valid prices | Youre doing good! ## VZ | yahoo (6|11) | Found cache file - Got 100% of valid prices | Feels good! ## BEN | yahoo (7|11) | Found cache file - Got 100% of valid prices | Well done! ## WY | yahoo (8|11) | Found cache file - Got 100% of valid prices | Well done! ## ROL | yahoo (9|11) | Found cache file - Got 100% of valid prices | Looking good! ## VTR | yahoo (10|11) | Found cache file - Got 100% of valid prices | Nice! ## ^GSPC | yahoo (11|11) | Found cache file - Got 100% of valid prices | Got it! df.stocks \u0026lt;- l.out$df.tickers Now, lets check if everything went well with the import process.\nprint(l.out$df.control) ## # A tibble: 11 x 6 ## ticker src download.status total.obs perc.benchmark.dates threshold.decisi… ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 FTV yahoo OK 628 0.624 OUT ## 2 ZBH yahoo OK 1006 1 KEEP ## 3 OXY yahoo OK 1006 1 KEEP ## 4 C yahoo OK 1006 1 KEEP ## 5 XLNX yahoo OK 1006 1 KEEP ## 6 VZ yahoo OK 1006 1 KEEP ## 7 BEN yahoo OK 1006 1 KEEP ## 8 WY yahoo OK 1006 1 KEEP ## 9 ROL yahoo OK 1006 1 KEEP ## 10 VTR yahoo OK 1006 1 KEEP ## 11 ^GSPC yahoo OK 1006 1 KEEP It seems that everything is Ok. All stocks have column perc.benchmark.dates equal to one (100%), meaning that they have the exact same dates as the benchmark ticker.\nNow, lets plot the time series of prices and look for any problem:\nlibrary(ggplot2) p \u0026lt;- ggplot(df.stocks, aes(x=ref.date, y=price.adjusted)) + geom_line() + facet_wrap(~ticker, scales = \u0026#39;free\u0026#39;) print(p) Again, we see that all prices seems to be Ok. This is one of the advantages of working with adjusted (and not closing) prices from yahoo finance. Artificial effects in the dataset such as ex-dividend prices, splits and inplits are already taken into account and the result is a smooth series without any breaks.\nThe final step in preparing the data is to add a column with the returns of the market index. This is not strictly necessary but I really like to keep things organized in a tabular way. Since we will match each vector of returns of the stocks to a vector of returns of the market index, it makes sense to synchronize the rows in the data.frame. First, we isolate the data for the market index in object df.MktIdx and use function match to make an index that matches the dates between the assets and the market index. We later use this index to build a new column in df.stocks. See the next code:\ndf.MktIdx \u0026lt;- df.stocks[df.stocks$ticker==ticker.MktIdx, ] idx \u0026lt;- match(df.stocks$ref.date, df.MktIdx$ref.date) df.stocks$ret.MktIdx \u0026lt;- df.MktIdx$ret.adjusted.prices[idx] Now that we have the data in the correct format and structure, let’s start to calculate some betas. Here is where the different approaches will differ in syntax. Let’s start with the first case, using loops.\n Estimating betas Using loops Loops are great and (almost) everyone loves then. While they can be a bit more verbose than fancy on-liners, the structure of a loop is very flexible and this can help solve complex problems. Let use it in our problem.\nThe first step in using loops is the understand the vector that will be used as iterator in the loop. In our problem we are processing each stock, so the number of iterations in the loop is simply the number of stocks in the sample. We can find the unique stocks with the command unique.\n# Check unique tickers unique.tickers \u0026lt;- unique(df.stocks$ticker) # create a empty vector to store betas beta.vec \u0026lt;- c() for (i.ticker in unique.tickers){ # message to prompt cat(\u0026#39;\\nRunning ols for\u0026#39;,i.ticker) # filter the data.frame for stock i.ticker df.temp \u0026lt;- df.stocks[df.stocks$ticker==i.ticker, ] # calculate beta with lm my.ols \u0026lt;- lm(data = df.temp, formula = ret.adjusted.prices ~ ret.MktIdx) # save beta my.beta \u0026lt;- coef(my.ols)[2] # store beta em beta.vec beta.vec \u0026lt;- c(beta.vec, my.beta) } ## ## Running ols for ZBH ## Running ols for OXY ## Running ols for C ## Running ols for XLNX ## Running ols for VZ ## Running ols for BEN ## Running ols for WY ## Running ols for ROL ## Running ols for VTR ## Running ols for ^GSPC # print result print(data.frame(unique.tickers,beta.vec)) ## unique.tickers beta.vec ## 1 ZBH 0.9130036 ## 2 OXY 1.0046153 ## 3 C 1.3468402 ## 4 XLNX 1.2272195 ## 5 VZ 0.5864176 ## 6 BEN 1.2555433 ## 7 WY 0.9400250 ## 8 ROL 0.8057286 ## 9 VTR 0.4923402 ## 10 ^GSPC 1.0000000 As you can see, the result is a lengthy code, but it works quite well. The final result is a dataframe with the tickers and their betas. Notice that, as expected, the betas are all positive and ^GSPC has a beta equal to 1.\n Using function by Another way of solving the problem is to calculate the betas using one of the functions from the apply family. In this case, we will use function by. Be aware that you can also solve the problem using tapply and lapply. The code, however, will increase in complexity.\nThe function by works similarly to tapply. The difference is that it is oriented to dataframes. That is, given a grouping variable, the original dataframe is broken into smaller dataframes and each piece is passed to a function. This helps a lot our problem since we need to work with two columns, the vector of returns of the asset and the vector of returns of the market index.\nGiven the functional form of by, will need to encapsulate a procedure that takes a dataframe as input and returns a coefficient beta, calculated from columns ret and ret.MktIdx. The next code does that.\nget.beta \u0026lt;- function(df.temp){ # estimate model my.ols \u0026lt;- lm(data=df.temp, formula = ret.adjusted.prices ~ ret.MktIdx) # isolate beta my.beta \u0026lt;- coef(my.ols)[2] # return beta return(my.beta) } The previous function accepts a single dataframe called df.temp, uses it to calculate a linear model with function lm and then returns the resulting beta, which is the second coefficient in coef(my.ols). Now, lets use it with function by.\n# get betas with by my.l \u0026lt;- by(data = df.stocks, INDICES = df.stocks$ticker, FUN = get.beta) # my.l is an objetct of class by. To get only its elements, we can unclass it betas \u0026lt;- unclass(my.l) # print result print(data.frame(betas)) ## betas ## ^GSPC 1.0000000 ## BEN 1.2555433 ## C 1.3468402 ## OXY 1.0046153 ## ROL 0.8057286 ## VTR 0.4923402 ## VZ 0.5864176 ## WY 0.9400250 ## XLNX 1.2272195 ## ZBH 0.9130036 Again, it worked well. Needless to say that the results are identical to the previous case.\n Using dplyr Now, let’s solve our problem using package dplyr. If you are not familiar with the tidyverse and the work of Hadley Wickham, you will be a happier person after reading the rest of this post, trust me.\nPackage dplyr is one of my favorites and most used packages. It allows for the representation of data processing procedures in a simpler and more intuitive way. It really helps to tackle computational problems if you can fit it within a flexible structure. This is what, in my opinion, dplyr does best. It combines clever functions with dataframes in the long (tidy) format.\nHave a look in the next set of code.\nlibrary(dplyr) beta.tab \u0026lt;- df.stocks %\u0026gt;% group_by(ticker) %\u0026gt;% # group by column ticker do(ols.model = lm(data = ., formula = ret.adjusted.prices ~ret.MktIdx)) %\u0026gt;% # estimate model mutate(beta = coef(ols.model)[2]) # get coefficients print(beta.tab) ## Source: local data frame [10 x 3] ## Groups: \u0026lt;by row\u0026gt; ## ## # A tibble: 10 x 3 ## ticker ols.model beta ## \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ^GSPC \u0026lt;lm\u0026gt; 1 ## 2 BEN \u0026lt;lm\u0026gt; 1.26 ## 3 C \u0026lt;lm\u0026gt; 1.35 ## 4 OXY \u0026lt;lm\u0026gt; 1.00 ## 5 ROL \u0026lt;lm\u0026gt; 0.806 ## 6 VTR \u0026lt;lm\u0026gt; 0.492 ## 7 VZ \u0026lt;lm\u0026gt; 0.586 ## 8 WY \u0026lt;lm\u0026gt; 0.940 ## 9 XLNX \u0026lt;lm\u0026gt; 1.23 ## 10 ZBH \u0026lt;lm\u0026gt; 0.913 After loading dplyr, we use the pipeline operator %\u0026gt;% to streamline all calculations. This means that we don’t need to keep a copy of intermediate calculations. Also, It looks pretty, don’t you agree?\nThe line beta.tab \u0026lt;- df.stocks %\u0026gt;% passes the dataframe df.stocks for the next line, group_by(ticker) %\u0026gt;%, which will group the dataframe according the the values of column ticker and pass the result for the next step. The line do(ols.model = lm(data = ., formula = ret ~ret.MktIdx)) estimates the model by passing a temporary dataframe and saves it in a column called ols.model. Notice that the model is a S3 object and not a single value. The dataframe alternative tibble is flexible with its content. The final line, mutate(beta = coef(ols.model)[2]) retrieves the beta from each element of the column ols.model.\nWhat I really like about dplyr is that it makes it easy to extend the original code. As an example, if I wanted to use a second grouping variable, I can just add it in the second line as group_by(ticker, newgroupingvariable). This becomes handy if you need, lets say, to estimated the model in different time periods.\nAs an example, let’s assume that I want to split the sample for each stock in half and see if the betas change significantly from time period to the other. This robustness check is a very common procedure in scientific research. First, let’s build a new column in df.stocks that sets the time periods as Sample 1 and Sample 2. We can use tapply for that;\nset.sample \u0026lt;- function(ref.dates){ my.n \u0026lt;- length(ref.dates) # get number of dates my.n.half \u0026lt;- floor(my.n/2) # get aproximate half of observations # create grouping variable samples.vec \u0026lt;- c(rep(\u0026#39;Sample 1\u0026#39;, my.n.half ), rep(\u0026#39;Sample 2\u0026#39;, my.n-my.n.half)) # return return(samples.vec) } # build group my.l \u0026lt;- tapply(X = df.stocks$ref.date, INDEX = df.stocks$ticker, FUN = set.sample ) # unsort it my.l \u0026lt;- my.l[my.tickers] # save it in dataframe df.stocks$my.sample \u0026lt;- unlist(my.l) We proceed by calling the same functions as before, but using an additional grouping variable.\nbeta.tab \u0026lt;- df.stocks %\u0026gt;% group_by(ticker,my.sample) %\u0026gt;% # group by column ticker do(ols.model = lm(data = ., formula = ret.adjusted.prices ~ret.MktIdx)) %\u0026gt;% # estimate model mutate(beta = coef(ols.model)[2]) # get coefficients print(beta.tab) ## Source: local data frame [20 x 4] ## Groups: \u0026lt;by row\u0026gt; ## ## # A tibble: 20 x 4 ## ticker my.sample ols.model beta ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ^GSPC Sample 1 \u0026lt;lm\u0026gt; 1.00 ## 2 ^GSPC Sample 2 \u0026lt;lm\u0026gt; 1. ## 3 BEN Sample 1 \u0026lt;lm\u0026gt; 1.40 ## 4 BEN Sample 2 \u0026lt;lm\u0026gt; 1.08 ## 5 C Sample 1 \u0026lt;lm\u0026gt; 1.54 ## 6 C Sample 2 \u0026lt;lm\u0026gt; 1.11 ## 7 OXY Sample 1 \u0026lt;lm\u0026gt; 1.10 ## 8 OXY Sample 2 \u0026lt;lm\u0026gt; 0.882 ## 9 ROL Sample 1 \u0026lt;lm\u0026gt; 0.781 ## 10 ROL Sample 2 \u0026lt;lm\u0026gt; 0.836 ## 11 VTR Sample 1 \u0026lt;lm\u0026gt; 0.653 ## 12 VTR Sample 2 \u0026lt;lm\u0026gt; 0.295 ## 13 VZ Sample 1 \u0026lt;lm\u0026gt; 0.669 ## 14 VZ Sample 2 \u0026lt;lm\u0026gt; 0.485 ## 15 WY Sample 1 \u0026lt;lm\u0026gt; 1.08 ## 16 WY Sample 2 \u0026lt;lm\u0026gt; 0.772 ## 17 XLNX Sample 1 \u0026lt;lm\u0026gt; 1.07 ## 18 XLNX Sample 2 \u0026lt;lm\u0026gt; 1.42 ## 19 ZBH Sample 1 \u0026lt;lm\u0026gt; 0.888 ## 20 ZBH Sample 2 \u0026lt;lm\u0026gt; 0.944 As we can see, the output now shows the beta for all combinations between ticker and my.sample. It seems that the betas tend to be higher for Sample 2, meaning that the overall systematic risk in the market has increased over time, at least for the majority of the ten chosen stocks. Given the small sample of stocks, It might be interesting to test for this property in a larger dataset.\nBack to the model, if you want more information about it, you can just write new lines in the last call to %\u0026gt;%. Let’s say, for example, that you want to get the value of alpha and the corresponding t-statistic of both coefficients. We can use the following code for that:\nlibrary(dplyr) beta.tab \u0026lt;- df.stocks %\u0026gt;% group_by(ticker) %\u0026gt;% # group by column ticker do(ols.model = lm(data = ., formula = ret.adjusted.prices ~ret.MktIdx)) %\u0026gt;% # estimate model mutate(beta = coef(ols.model)[2], beta.tstat = summary(ols.model)[[4]][2,3], alpha = coef(ols.model)[1], alpha.tstat = summary(ols.model)[[4]][1,3]) # get coefficients ## Warning in summary.lm(ols.model): essentially perfect fit: summary may be ## unreliable ## Warning in summary.lm(ols.model): essentially perfect fit: summary may be ## unreliable print(beta.tab) ## Source: local data frame [10 x 6] ## Groups: \u0026lt;by row\u0026gt; ## ## # A tibble: 10 x 6 ## ticker ols.model beta beta.tstat alpha alpha.tstat ## \u0026lt;chr\u0026gt; \u0026lt;list\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 ^GSPC \u0026lt;lm\u0026gt; 1 3.49e17 0 0 ## 2 BEN \u0026lt;lm\u0026gt; 1.26 2.99e 1 -0.000708 -1.96 ## 3 C \u0026lt;lm\u0026gt; 1.35 3.46e 1 -0.000184 -0.548 ## 4 OXY \u0026lt;lm\u0026gt; 1.00 2.19e 1 -0.000217 -0.551 ## 5 ROL \u0026lt;lm\u0026gt; 0.806 2.11e 1 0.000879 2.67 ## 6 VTR \u0026lt;lm\u0026gt; 0.492 8.94e 0 -0.000144 -0.304 ## 7 VZ \u0026lt;lm\u0026gt; 0.586 1.65e 1 0.000287 0.935 ## 8 WY \u0026lt;lm\u0026gt; 0.940 2.24e 1 -0.000463 -1.28 ## 9 XLNX \u0026lt;lm\u0026gt; 1.23 2.46e 1 0.000623 1.45 ## 10 ZBH \u0026lt;lm\u0026gt; 0.913 2.17e 1 -0.000164 -0.451 In the previous code, I added line beta.tstat = summary(ols.model)[[4]][2,3] that returns the t-statistic of the beta coefficient. The location of this parameter is found by investigating the elements of an object of type lm. After calling summary, the t-statistic is available in the fourth element of the lm object, which is a matrix with several information from the estimation. The t-statistic for the alpha parameter is found in a similar way.\nAs you can see, the syntax of dplyr make it easy to extend the model and quickly try new things. It is possible to do the same using other R functions and a loop, but using dplyr is really handy.\n  Conclusions As you can probably suspect from the text, I’m a big fan of dplyr and I’m always teaching its use to my students. While loops are ok and I personally use then a lot in more complex problems, the functions in dplyr allow for an intuitive syntax in data processing, making it easy to understand and extend code.\nDo notice that the code in this example is self contained and reproducible. If you want to try it for more stocks, just change input n.chosen.stocks to a higher value.\n ","date":1484438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1484438400,"objectID":"eaa46698471195768e19879f01fc6e4f","permalink":"http://www.msperlin.com/blog/post/2017-01-15-calculatingbetas/","publishdate":"2017-01-15T00:00:00Z","relpermalink":"/blog/post/2017-01-15-calculatingbetas/","section":"post","summary":"One of the first examples about using linear regression models in finance is the calculation of betas, the so called market model. Coefficient beta is a measure of systematic risk and it is calculated by estimating a linear model where the dependent variable is the return vector of a stock and the explanatory variable is the return vector of a diversified local market index, such as SP500 (US), FTSE (UK), Ibovespa (Brazil), or any other.\nFrom the academic side, the calculation of beta is part of a famous asset pricing model, CAPM - Capital Asset Pricing Model, that relates expected return and systematic risk.","tags":["R","stock market","beta","linear regression"],"title":"How to calculate betas (systematic risk) for a large number of stocks","type":"post"},{"authors":null,"categories":["R","GetHFData","B3","market microstructure","high frequency"],"content":" Recently, Bovespa, the Brazilian financial exchange company, allowed external access to its ftp site. In this address one can find several information regarding the Brazilian financial system, including datasets with high frequency (tick by tick) trading data for three different markets: equity, options and BMF.\nDownloading and processing these files, however, can be exausting. The dataset is composed of zip files with the whole trading data, separated by day and market. These files are huge in size and processing or aggregating them in a usefull manner requires specific knowledge for the structure of the dataset.\nThe package GetHFData make is easy to access this dataset directly by allowing the easy importation and aggregations of it. Based on this package the user can:\n Access the contents of the Bovespa ftp using function function ghfd_get_ftp_contents Get the list of available ticker in the trading data using ghfd_get_available_tickers_from_ftp Download individual files using ghfd_download_file Download and process a batch of dates and assets codes with ghfd_get_HF_data  More details can be found in my SSRN paper.\n","date":1483315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483315200,"objectID":"758ae112f7269bdfff89587f60a2e218","permalink":"http://www.msperlin.com/blog/post/2017-01-02-gethfdata/","publishdate":"2017-01-02T00:00:00Z","relpermalink":"/blog/post/2017-01-02-gethfdata/","section":"post","summary":"Recently, Bovespa, the Brazilian financial exchange company, allowed external access to its ftp site. In this address one can find several information regarding the Brazilian financial system, including datasets with high frequency (tick by tick) trading data for three different markets: equity, options and BMF.\nDownloading and processing these files, however, can be exausting. The dataset is composed of zip files with the whole trading data, separated by day and market. These files are huge in size and processing or aggregating them in a usefull manner requires specific knowledge for the structure of the dataset.","tags":["R","GetHFData","B3","market microstructure","high frequency"],"title":"Using R to download high frequency trade data diretcly from Bovespa","type":"post"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"4b16c5b9edad3c30fb33fd87c89b01c9","permalink":"http://www.msperlin.com/blog/talk/2018-02-01-unisinos-getdfpdata/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/blog/talk/2018-02-01-unisinos-getdfpdata/","section":"talk","summary":"Palestra sobre pacote GetDFPData, realizada no Unisinos-Poa.","tags":[],"title":"Accessing Financial Reports and Corporate Events Data using GetDFPData","type":"talk"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"1e268ebd91b0d2e1a4498d846a5860df","permalink":"http://www.msperlin.com/blog/talk/2018-06-01-puc-rj-getdfpdata/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/blog/talk/2018-06-01-puc-rj-getdfpdata/","section":"talk","summary":"Palestra realizada na PUC-RJ sobre pacote GetDFPData.","tags":[],"title":"Accessing Financial Reports and Corporate Events Data using GetDFPData","type":"talk"},{"authors":null,"categories":["about me"],"content":" Hello everyone! This is the first (ever) post for a new blog about Finance and R. My name is Marcelo Perlin and my day job is assistant professor of Finance in Federal University of Rio Grande do Sul, in the city of south of Brazil (Porto Alegre). I received my PhD in Finance from Reading University (ICMA Centre) in 2010 and I have a long experience in doing applied research in Finance.\nIn the computational side, I fell in love with R in the moment I learned to use it. I had a long standing working relationship with Matlab and I was impressed in observing how much easier it was to manipulate and process data with R, once you understand its sintax. I translated all of my finance libraries from Matlab to R, realizing during the process that I was writing R code much faster and the result was much simpler to mantain than I would do in Matlab. After that, I was hooked. I’m currently writing a book about R and Finance that I hope to publish soon. The portuguese version should be up in amazon very soon. The english version is due in a couple of monts.\nIn this blog I will write about my CRAN packages and any other ideas about finance and R or research papers that I’m currently working on. The content will be mostly about ways to make the life of a financial data analyst more enjoyable.\nOh, and when I’m not researching and teaching, I also love playing tennis and swimming. Fell free to drop me an email at marceloperlin@gmail.com.\nBest,\nMarcelo.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"262fa34c13f8b287b2bb04d6b8dfa8ba","permalink":"http://www.msperlin.com/blog/post/2017-01-01-first-post/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/blog/post/2017-01-01-first-post/","section":"post","summary":"Hello everyone! This is the first (ever) post for a new blog about Finance and R. My name is Marcelo Perlin and my day job is assistant professor of Finance in Federal University of Rio Grande do Sul, in the city of south of Brazil (Porto Alegre). I received my PhD in Finance from Reading University (ICMA Centre) in 2010 and I have a long experience in doing applied research in Finance.\nIn the computational side, I fell in love with R in the moment I learned to use it.","tags":["about me"],"title":"My first post!","type":"post"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"ac46f44af217383096de9f916b0b2e7d","permalink":"http://www.msperlin.com/blog/talk/2018-11-07-ufpr-predatory/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/blog/talk/2018-11-07-ufpr-predatory/","section":"talk","summary":"Palestra realizada para o PPG da UFPR sobre a inserção de revistas predatórias no sistema acadêmico Brasileiro.","tags":[],"title":"O Perigo das Revistas Predatórias","type":"talk"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"a5984819ff2c6ae5c8eff9bd5c06e7e7","permalink":"http://www.msperlin.com/blog/publication/2016_economiaaplicada-microestruturatd/","publishdate":"2016-07-01T00:00:00Z","relpermalink":"/blog/publication/2016_economiaaplicada-microestruturatd/","section":"publication","summary":"O objetivo deste artigo é analisar a microestrutura do Tesouro Direto em dois pontos: a dinâmica do fluxo de ordens e a formação dos spreads. Primeiro observa-se um padrão temporal da demanda por esses títulos na forma de uma sazonalidade em duas dimensões, dia do mês e mês do ano. No artigo descreve-se um modelo estatístico para a previsão da demanda com base nesta sazonalidade e analisa-se sua capacidade de previsão fora da amostra. Os resultados mostram que o modelo consegue capturar as propriedades das séries temporais, cuja previsibilidade pode auxiliar na administração do estoque de títulos por parte do governo. O segundo ponto de análise no estudo foi na formação do spread (preço de compra menos preço de venda) dentro do tesouro direto. No trabalho mostra-se que o Tesouro define o spread dos títulos negociados na plataforma do tesouro direto de acordo com regras simples sobre a maturidade de cada um dos instrumentos, formando um padrão do tipo escada. No desenvolvimento do artigo se discute os fatores negativos dentro desse método e apresenta-se também um método alternativo através de um modelo de ajustamento, o qual define o spread de forma mais parcimoniosa.","tags":[],"title":"A Microestrutura do Tesouro Direto: Sazonalidade do Fluxo de Ordens e a Formação de Spreads","type":"publication"},{"authors":["Alan Delgado de Oliveira","Tiago Pascoal Filomena","Marcelo Scherer Perlin","Miguel Lejeune","Guilherme Ribeiro de Macedo"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"4f3399f44b07e4ed6d96352188a59277","permalink":"http://www.msperlin.com/blog/publication/2016_oe-multistagestochasticprogramming/","publishdate":"2016-07-01T00:00:00Z","relpermalink":"/blog/publication/2016_oe-multistagestochasticprogramming/","section":"publication","summary":"This paper proposes a multistage stochastic programming approach for the asset-liability management of Brazilian pension funds. We generate asset price scenarios with stochastic differential equations—Geometric Brownian Motion model for stocks and Cox–Ingersoll–Ross model for fixed income securities. Intertemporal solvency regulatory rules for Brazilian pension funds are considered endogenously in the model and enforced with a combinatorial constraint. A VaR probabilistic constraint is incorporated to obtain a positive funding ratio at each time period with high probability. Our approach uses multiple trees to provide a representative characterization of the uncertainty and is not computationally prohibitive. We evaluate the insolvency probability under different initial funding ratios through extensive simulations. The study reveals that the likely decrease of interest rate premiums in the next years will force pension fund managers to significantly change their portfolio strategies. They will have to take more risk in order to deliver the cash flows required to cover the liabilities and satisfy the regulatory constraints.","tags":[],"title":"A Multistage Stochastic Programming Asset-Liability Management Model - An Application to the Brazilian Pension Fund Industry","type":"publication"},{"authors":["Marcelo S. Perlin","Henrique P. Ramos"],"categories":null,"content":"","date":1467331200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467331200,"objectID":"e984d6eca129d99284d4f62308c9d31d","permalink":"http://www.msperlin.com/blog/publication/2016_rbfin-gethfdata/","publishdate":"2016-07-01T00:00:00Z","relpermalink":"/blog/publication/2016_rbfin-gethfdata/","section":"publication","summary":"This paper introduces GetHFData, a R package for downloading, importing and aggregating high frequency trading data from the Brazilian financial market. Based on a set of user choices, the package GetHFData will download the required files directly from Bovespa’s ftp site and aggregate the financial data. The main objective of the publication of this software is to facilitate the computational effort related to research based on this large financial dataset and also to increase the reproducibility of studies by setting a replicable standard for data acquisition and processing. In this paper we present the available functions of the software, a brief description of the Brazilian market and several reproducible examples of usage.","tags":[],"title":"GetHFData: A R package for downloading and aggregating high frequency trading data from Bovespa","type":"publication"},{"authors":null,"categories":null,"content":"A major part of my work as an academic researcher is finding ways to access and organize financial and economic datasets using code. In this section I\u0026rsquo;ll provide a download link for the full datasets I use for research or class material. All files represent the whole database, iterating by available time periods and cases. These files are raw and come with no warranty. Be aware that you\u0026rsquo;ll probably need to clean it up before any good use. As such, please make sure to cite the original source for reproducibility (see citation format from dataverse).\nAll data is distributed using the Harvard Dataverse Project. The Dataverse platform is stable, provides data persistency, versioning and the automatic generation of citation text. You can even use the dataverse package for reading the files from an R session.\nAll hosted data will be updated at least once a year.\n Available Datasets    R Package Source of Data Description Direct Link Last Update     GetTDData Tesouro Nacional Prices and yields of brazilian sovereign bonds Link 2022-04-06   GetFREData CVM Corporate dataset from FRE systems Link 2022-04-06   GetDFPData2 CVM Annual Financial Reports from DFP system Link 2022-04-06     How to Support I often get asked about how to support the release of the parsed data files. First, buying one of my books and donating a copy to your local library is a great way of contributing. Alternatively, you can make a single or continuous donation to project GetDFPData2 using Paypal (instructions at link).\n ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"494b4c66d4128fe395a8c9607fab3629","permalink":"http://www.msperlin.com/blog/data/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/blog/data/","section":"","summary":"A major part of my work as an academic researcher is finding ways to access and organize financial and economic datasets using code. In this section I\u0026rsquo;ll provide a download link for the full datasets I use for research or class material. All files represent the whole database, iterating by available time periods and cases. These files are raw and come with no warranty. Be aware that you\u0026rsquo;ll probably need to clean it up before any good use. As such, please make sure to cite the original source for reproducibility (see citation format from dataverse).","tags":null,"title":"Compiled Datasets","type":"page"},{"authors":null,"categories":null,"content":"2018    University World News - The alarming rise of predatory journals\n   Revista Pesquisa Fapesp - A sombra das revistas predatórias no Brasil\n   LSE Impact Blog - Predatory publishers threaten to consume public research funds and undermine national academic systems – the case of Brazil\n   Superintendência de Comunicação Social (UFPR) - Revistas predatórias ameaçam sistema acadêmico brasileiro, alertam pesquisadores\n  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"781ca44c9922fba544993025bf75ae21","permalink":"http://www.msperlin.com/blog/research/on-the-news/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/blog/research/on-the-news/","section":"research","summary":"2018    University World News - The alarming rise of predatory journals\n   Revista Pesquisa Fapesp - A sombra das revistas predatórias no Brasil\n   LSE Impact Blog - Predatory publishers threaten to consume public research funds and undermine national academic systems – the case of Brazil\n   Superintendência de Comunicação Social (UFPR) - Revistas predatórias ameaçam sistema acadêmico brasileiro, alertam pesquisadores\n  ","tags":null,"title":"Coverage by News","type":"research"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"http://www.msperlin.com/blog/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/blog/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"http://www.msperlin.com/blog/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/blog/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":null,"categories":null,"content":"2018  Article \u0026ldquo;Is predatory publishing a real threat? Evidence from a large database study\u0026rdquo; in the top 5% of all research outputs scored by Altmetric. [link] Top 10% of authors on SSRN by all-time downloads. [link] Top 10% of authors on SSRN by total new downloads within the last 12 months. [link]  2016  RBFIN best paper of 2015 (Honorary mention) - Award from the Brazilian Finance Society for best paper published in the Brazilian Review of Finance for the year of 2015. Title of paper: The researchers, the publications and the journals of Finance in Brazil: An analysis based on resumes from the Lattes platform. [Link]  2014   APIMEC Award for research paper in Capital Markets (3º place) - Award from the Brazilian Association of Financial Analysts (APIMEC) for the best conference papers in the subject of Capital Markets Title of paper: Estimating the Intensity of News based on Trade Data. [Link]\n  RBFIN best paper of 2013 (1º place) - Award from the Brazilian Finance Society for best paper published in the Brazilian Review of Finance for the year of 2013. Title of paper: The effects of the introduction of market makers in the Brazilian equity market. [Link]\n  2011   ANBIMA award (1º place) - Award from ANBIMA (Brazilian Association of Capital Markets) for the best paper in the area of fixed income markets. Title of paper: The Determinants of a Cross Market Arbitrage Opportunity: Theory and Evidence for the European Bond Market. [Link]\n  Award from IBEF SP/KPMG (Honorary mention) - Award from the Institute of Finance Executives. Title of paper: On the Performance of the Tick Test. [Link]\n  File Exchange Pick of the Week - Award from the Matlab community website. Matlab package: MS_Regress - A Package for Markov Regime Switching Models in Matlab. [Link]\n  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"641d2389a93293010c11d519828807c7","permalink":"http://www.msperlin.com/blog/research/awards/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/blog/research/awards/","section":"research","summary":"2018  Article \u0026ldquo;Is predatory publishing a real threat? Evidence from a large database study\u0026rdquo; in the top 5% of all research outputs scored by Altmetric. [link] Top 10% of authors on SSRN by all-time downloads. [link] Top 10% of authors on SSRN by total new downloads within the last 12 months. [link]  2016  RBFIN best paper of 2015 (Honorary mention) - Award from the Brazilian Finance Society for best paper published in the Brazilian Review of Finance for the year of 2015. Title of paper: The researchers, the publications and the journals of Finance in Brazil: An analysis based on resumes from the Lattes platform.","tags":["R","finance"],"title":"Research Awards","type":"research"},{"authors":null,"categories":null,"content":"Since 2022, I\u0026rsquo;ve been active in Linkedin, sharing all major work-related updates. Just follow me there and you\u0026rsquo;ll receive all updates.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"098a47a3e6687de4f237bccc8e18561c","permalink":"http://www.msperlin.com/blog/subscribe_email/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/blog/subscribe_email/","section":"","summary":"Since 2022, I\u0026rsquo;ve been active in Linkedin, sharing all major work-related updates. Just follow me there and you\u0026rsquo;ll receive all updates.","tags":null,"title":"Subscribe","type":"page"},{"authors":null,"categories":null,"content":"You can subscribe to my blog posts here:\n\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"6534cd16b6ea0789ed6866cc1135c244","permalink":"http://www.msperlin.com/blog/subscribe_rss/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/blog/subscribe_rss/","section":"","summary":"You can subscribe to my blog posts here:","tags":null,"title":"Subscribe to RSS","type":"page"},{"authors":["admin","吳恩達"],"categories":["Demo","教程"],"content":"Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\n Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n 👉 Get Started 📚 View the documentation 💬 Ask a question on the forum 👥 Chat with the community 🐦 Twitter: @source_themes @GeorgeCushen #MadeWithAcademic 💡 Request a feature or report a bug ⬆️ Updating? View the Update Guide and Release Notes ❤️ Support development of Academic:  ☕️ Donate a coffee 💵 Become a backer on Patreon 🖼️ Decorate your laptop or journal with an Academic sticker 👕 Wear the T-shirt 👩‍💻 Contribute      Academic is mobile first with a responsive design to ensure that your site looks stunning on every device.   Key features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Academic comes with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the sun/moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\n Choose a stunning theme and font for your site. Themes are fully customizable.\nEcosystem   Academic Admin: An admin tool to import publications from BibTeX or import assets for an offline site  Academic Scripts: Scripts to help migrate content to new versions of Academic  Install You can choose from one of the following four methods to install:\n  one-click install using your web browser (recommended)  install on your computer using Git with the Command Prompt/Terminal app  install on your computer by downloading the ZIP files  install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating  View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"http://www.msperlin.com/blog/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/blog/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["Academic","开源"],"title":"Academic: the website builder for Hugo","type":"post"},{"authors":["Martin Pontuschka","Marcelo S. Perlin"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"c42de70484adba3fc2eca2f0e30fabd0","permalink":"http://www.msperlin.com/blog/publication/2015_ram-estrategiapares/","publishdate":"2015-07-01T00:00:00Z","relpermalink":"/blog/publication/2015_ram-estrategiapares/","section":"publication","summary":"A estratégia de pares é um popular método de negociação de ativos financeiros. Um dos motivos para isto se deve ao fato de que o resultado deste tipo de operação procede somente da relação entre os preços de dois ativos e não da direção do mercado. Somente a possibilidade de capturar ineficiências na precificação dos ativos é o que permite a obtenção de lucros sistemáticos através de um método de negociação de ativos. Com base na abertura de uma posição comprada (long), e no mesmo instante, uma posição vendida (short), esta estratégia de arbitragem estatística busca obter lucros por conta da convergência dos preços dos ativos negociados. Desta forma, o objetivo deste trabalho é analisar o desempenho da estratégia de pares em diferentes frequências de dados no mercado acionário brasileiro. Esta pesquisa baseou-se em Perlin (2007) onde foi encontrado o resultado de que as ineficiências de mercado aparecem em maior quantidade nos dados de maiores frequências, neste caso diário, semanal e mensal. O presente trabalho estende o leque de frequências entrando no universo intradiário com frequências de amostragem em 1 minuto, 5 minutos, 15 minutos, 30 minutos, 60 minutos e diários. O período de tempo da base de dados utilizada nesta pesquisa se estende de 01 de janeiro de 2008 até 31 de dezembro de 2011. Para compor a base de dados foram utilizados os vinte ativos com maior número de contratos negociados no período. A metodologia empregada nesta pesquisa utiliza a técnica de períodos de treinamento e períodos de negociações. Nos períodos de treinamento, os pares de ativos são selecionados de acordo com os menores desvios quadráticos nos seus preços normalizados. Nos períodos de negociação a estratégia verifica as operações realizadas em cada par de ativo previamente selecionado. Os resultados da pesquisa confirmam a hipótese primária de que quanto maior a frequência de amostragem, maiores evidências de ineficiência de mercado. Para chegar a esta conclusão foram comparados os índices de Sharpe da estratégia de pares nas diferentes frequências de dados.","tags":[],"title":"A Estratégia de pares no Mercado Acionário Brasileiro: O Impacto da Frequência de Dados","type":"publication"},{"authors":["Marcelo S. Perlin","André Portela Santos"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"c103db6333926e55381710597c4a27ae","permalink":"http://www.msperlin.com/blog/publication/2015_rbfin-pesquisadoresfinancasbr/","publishdate":"2015-07-01T00:00:00Z","relpermalink":"/blog/publication/2015_rbfin-pesquisadoresfinancasbr/","section":"publication","summary":"Este artigo analisa a produção científica da área de Finanças no Brasil. Utilizando um software proprietário para obter informações diretamente da plataforma de currículos Lattes foi possível verificar o perfil e as tendências da pesquisa em Finanças no território nacional. Os principais resultados da pesquisa mostram que a maioria dos pesquisadores de Finanças são relativamente jovens em termos de tempo de carreira, com doutorado finalizado entre os anos de 2005 e 2014 e situados no sudeste do país. Observa-se também que a produção científica nacional em periódicos internacionais de impacto é pequena em comparação com o total de publicações encontradas. O número de publicações por ano tem crescido exponencialmente, porém a qualidade das produções, medida pelo Qualis, deteriorou-se. Uma análise da produtividade dos autores mostra que os autores mais produtivos possuem duas características em comum: doutorado fora do Brasil e bolsa de produtividade do CNPQ.","tags":[],"title":"Os pesquisadores, as publicações e os periódicos da área de Finanças no Brasil: Uma análise com base em currículos da plataforma Lattes.","type":"publication"},{"authors":["Marcelo S. Perlin","Chris Brooks","Alfonso Dufour"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"80c7d577a002ed69c5fd6a86ebbc820f","permalink":"http://www.msperlin.com/blog/publication/2014_qref-ticktest/","publishdate":"2014-07-01T00:00:00Z","relpermalink":"/blog/publication/2014_qref-ticktest/","section":"publication","summary":"In financial research, the sign of a trade (or identity of trade aggressor) is not always available in the transaction dataset and it can be estimated using a simple set of rules called the tick test. In this paper we investigate the accuracy of the tick test from an analytical perspective by providing a closed formula for the performance of the prediction algorithm. By analyzing the derived equation, we provide formal arguments for the use of the tick test by proving that it is bounded to perform better than chance (50/50) and that the set of rules from the tick test provides an unbiased estimator of the trade signs. On the empirical side of the research, we compare the values from the analytical formula against the empirical performance of the tick test for fifteen heavily traded stocks in the Brazilian equity market. The results show that the formula is quite realistic in assessing the accuracy of the prediction algorithm in a real data situation.","tags":[],"title":"On the Performance of the Tick Test","type":"publication"},{"authors":["Fernanda Gomes Victor","Marcelo S. Perlin","Mauro Mastella"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"0a1cbeed25c639b3b2d0ea6327b2c961","permalink":"http://www.msperlin.com/blog/publication/2013_rbfin-comunalidadeliquidez/","publishdate":"2013-07-01T00:00:00Z","relpermalink":"/blog/publication/2013_rbfin-comunalidadeliquidez/","section":"publication","summary":"O objetivo deste artigo é estudar a dinâmica da liquidez intradiária na Bolsa de Valores Brasileira, sob a ótica de co-movimentos (ou comunalidades). No trabalho argumenta-se que este fator comum na liquidez das ações é afetado pelos efeitos intradiários particulares da microestrutura do mercado. Utilizando uma base de dados de alta frequência e o volume negociado como proxy para liquidez, tal hipótese é investigada para os dados brasileiros, sendo encontrada forte evidência de que o efeito de comunalidade da liquidez muda de forma significativa ao longo de diferentes intervalos do dia. Durante as primeiras e últimas horas de negociação este efeito é mais intenso, justificando-se tal resultado como um produto da chegada de novas informações ao mercado e uma consequência do risco overnight.","tags":[],"title":"Comunalidades na Liquidez – Evidências e Comportamento Intradiário para o Mercado Brasileiro","type":"publication"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"2cefa6ba871fcf2006fdf669ebe0079a","permalink":"http://www.msperlin.com/blog/publication/2013_rbfin-formadoresmercado/","publishdate":"2013-07-01T00:00:00Z","relpermalink":"/blog/publication/2013_rbfin-formadoresmercado/","section":"publication","summary":"O objetivo principal deste artigo é analisar empiricamente o efeito da introdução de formadores de mercado no processo de compra e venda de ações no mercado acionário brasileiro. Agregando informações sobre as datas de início de contrato de formadores de mercado a uma privilegiada base de dados de alta frequência, foi possível efetivar um estudo de evento para verificar os efeitos da introdução dos agentes de liquidez. Conforme esperado, observou-se que o formador de mercado aumentou significativamente a liquidez dos papéis. Foi possível verificar um aumento médio de 31% no número de negócios entre o período anterior e posterior ao início das atividades do formador de mercado. Reporta-se também que a entrada do agente de liquidez muda significativamente a autocorrelação dos sinais de transação em aproximadamente 10%, sendo este resultado mais forte para as ações menos líquidas. Além disso, a pesquisa também indica resultados heterogêneos para a performance do formador de mercado quando os mesmos são segmentados pela instituição financeira de origem. Tal informação pode servir de base para uma escolha inteligente do formador de mercado por parte das empresas negociadas em bolsa.","tags":[],"title":"Os Efeitos da Introdução de Agentes de Liquidez no Mercado Acionário Brasileiro","type":"publication"},{"authors":["Marcelo S. Perlin","Jochen Schanz"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1309478400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1309478400,"objectID":"c97200636ab191ec47fde53d78d64b3d","permalink":"http://www.msperlin.com/blog/publication/2011_bewps-chaps/","publishdate":"2011-07-01T00:00:00Z","relpermalink":"/blog/publication/2011_bewps-chaps/","section":"publication","summary":"When settling their own liabilities and those of their clients, settlement banks rely on incoming payments to fund a part of their outgoing payments. We investigate their behaviour in CHAPS, the United Kingdom’s large-value payment system. Our estimates suggest that in normal times, banks increase their payment outflows when their liquidity is above target and immediately following the receipt of payments. We use these estimates to determine the robustness of this payment system to two hypothetical behavioural changes. In the first, a single bank stops sending payments, perhaps because of an operational problem. In the second, it pays out exactly what it previously received, relying exclusively on the liquidity provided by other system members. Using the observed uncertainty around our estimated behavioural equations, we derive probabilistic statements about the time at which the bank’s counterparties would run out of liquidity if they followed their estimated normal-time behaviour.","tags":[],"title":"System-Wide liquidity risk in the UK’s large-value payment system: an empirical analysis","type":"publication"},{"authors":["Marcelo S. Perlin"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1246406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1246406400,"objectID":"c6ae6bdb4e4f595b9f9f28074d58b13f","permalink":"http://www.msperlin.com/blog/publication/2009_jdhf-pairstrading/","publishdate":"2009-07-01T00:00:00Z","relpermalink":"/blog/publication/2009_jdhf-pairstrading/","section":"publication","summary":"Pairs-trading is a popular trading strategy that tries to take advantage of market inefficiencies in order to obtain profit. The idea is simple: find two stocks that move together and take long/short positions when they diverge abnormally, hoping that the prices will converge in the future. From the academic point of view of weak market efficiency theory, pairs-trading strategy should not present positive performance, as, according to it, the actual price of a stock reflects its past trading data, including historical prices. This leaves us with a question: does pairs-trading strategy present positive performance for the Brazilian market? The main objective of this research is to verify the performance and risk of pairs-trading in the Brazilian financial market for different frequencies of the database: daily, weekly and monthly prices for the same time period. The main conclusion of this simulation is that pairs-trading strategy was a profitable and market-neutral strategy at the Brazilian market. Such profitability was consistent over a region of the strategy's parameters. The best results were found for the highest frequency (daily), which is an intuitive result.","tags":[],"title":"Evaluation of pairs-trading strategy at the Brazilian financial market","type":"publication"},{"authors":["Marcelo S. Perlin","Paulo Sérgio Ceretta"],"categories":null,"content":"","date":1183248000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1183248000,"objectID":"06721ece3bed4888e50b7c0d312e9f07","permalink":"http://www.msperlin.com/blog/publication/2007_read-nn/","publishdate":"2007-07-01T00:00:00Z","relpermalink":"/blog/publication/2007_read-nn/","section":"publication","summary":"The predictability of stock market’s behavior is a topic studied by different academic circles for long time. A popular tool to make predictions about the stock market behavior on short term is the technical analysis. Such tool is based on the analysis of quantitative indicators and also chart patterns in order to identify the time to entry (buy) or exit the market (sell). A quantitative approach that is related to charting is the use of the non-parametric approach of nearest neighbor algorithm in order to produce forecasts of the time series on t+1. The main objective of this paper is to study the forecasting performance of the nearest neighbor method for the Brazilian Equity data in two versions, the univariate and also the multivariate case, which is also called simultaneous nearest neighbor. The main conclusion of the paper is that the ability of the algorithm in forecasting the values of the stock prices is mixed. A comparative analysis with the random walk model showed that this naïve approach has more explicative power in numerical accuracy. For the case of directional forecasts, the NN presented better results, resulting in correct directional forecasts moderately higher than 50% for most of the assets and with a maximum of approximately 60% correct market direction forecasts, which indicates that the method may add value in quantitative trading strategies. Comparing the results for both versions of the algorithm, its clear that both presented very similar results, but the univariate case was slightly better.","tags":[],"title":"Non Linear Modelling in the Brazilian Market: Evaluating the Forecasting Performance of the NN (Univariate Nearest Neighbour) and SNN (Simultaneous Nearest Neighbour) Forecasting Algorithm","type":"publication"},{"authors":["Marcelo S. Perlin","Paulo Sérgio Ceretta"],"categories":null,"content":"","date":1151712000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1151712000,"objectID":"3414ee8bb7950dd5e0344029538b72ce","permalink":"http://www.msperlin.com/blog/publication/2006_read-caos/","publishdate":"2006-07-01T00:00:00Z","relpermalink":"/blog/publication/2006_read-caos/","section":"publication","summary":"Dentro da teoria financeira moderna do mercado de capitais, o estudo da existência da previsibilidade no comportamento do mercado é objeto de pesquisa de inúmeros artigos. Tal tipo de pesquisa é indicada como um teste da eficiência do mercado. Uma grande parte desses estudos baseiam-se no encontro de relações entre o comportamento dos ativos em relações a outras variáveis (Fama e French (1992)) ou no teste de modelos matemáticos de previsão (Rodríguez, Rivero e Artiles (2001) ). Uma abordagem alternativa para o tipo de pesquisa destacada anteriormente encontra-se na teoria de sistemas não lineares, mais precisamente na teoria do caos. O presenta artigo possui como objetivo verificar a existência de comportamento caótico para os contratos de café arábica entre 11/03/2002 até 30/12/2002 (todos com vencimento até janeiro de 2003). Para atingir tal objetivo, foi utilizada a proposta qualitativa e quantitativa do teste descrito em, respectivamente, Gilmore (1998) e Ceretta (2003). A principal conclusão da pesquisa é o encontro de evidências favoráveis a existência de dinâmicas não lineares nos contratos de café arábica, sugerindo que o comportamento do ativo não é previsível no longo prazo.","tags":[],"title":"Teoria do Caos aplicada aos Contratos de Café no Mercado de Derivativos","type":"publication"}]